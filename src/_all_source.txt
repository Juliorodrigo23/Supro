================================================
FILE: README.md
================================================
# Supro Arm Tracker

An Arm Rotation Tracking System by Julio Contreras ‚Äî Under Dr. Ortiz's Research Lab 

![Supro Logo](SuproLogo.gif)

Supro Arm Tracker is a sophisticated motion tracking application built in Rust for analyzing forearm rotation patterns (supination and pronation) in real-time from a webcam or pre-recorded video files. It uses Google's MediaPipe for robust skeleton and hand tracking, combined with custom algorithms and Kalman filters for smooth and accurate gesture analysis.

---

## Core Features

* **Multi-Mode Operation**:
    * üé• **Live Camera Mode**: Analyze arm movements directly from a webcam feed[cite: 3, 39].
    * üìÅ **Video File Mode**: Process and analyze pre-recorded video files[cite: 3, 41, 65].
    * üìä **Analysis Mode**: Review, chart, and export data from previous sessions[cite: 57, 102].
* **Advanced Tracking & Visualization**:
    * Real-time skeleton tracking overlay showing shoulders, elbows, and wrists[cite: 145, 150].
    * Detailed hand and finger landmark tracking[cite: 154, 158].
    * **Dual View** UI to compare raw video feed against the tracking overlay side-by-side[cite: 59, 82].
* **Gesture Recognition**:
    * Detects and classifies forearm movements into **Supination** or **Pronation**[cite: 319, 412].
    * Displays real-time gesture classification, confidence levels, and rotation angles[cite: 111, 117, 131, 132].
* **Data Recording & Export**:
    * Record tracking sessions for later analysis[cite: 45, 171, 175].
    * Export detailed, frame-by-frame joint and gesture data to a **CSV file**[cite: 105, 228].
    * Generate a summary **HTML report** with session statistics[cite: 106, 246].
* **Configurable Settings**:
    * Adjust tracking parameters like confidence threshold and smoothing factor[cite: 186, 187].
    * Toggle tracking for the left arm, right arm, and fingers independently[cite: 180].

---

## Technology Stack

* **Primary Language**: **Rust**
* **GUI Framework**: **`eframe` / `egui`** for the immediate-mode graphical user interface[cite: 528].
* **Computer Vision Backend**: **Google MediaPipe** (Pose and Hand models) running in a separate Python process[cite: 551, 552, 564].
* **Camera & Video Input**: The `nokhwa` crate for camera capture in Rust[cite: 502].
* **Mathematical Filtering**: A custom **Kalman Filter** implementation for smoothing joint positions, written using the `nalgebra` crate[cite: 318, 325, 441].
* **Inter-Process Communication**: A lightweight bridge between Rust and Python using standard input/output (stdin/stdout) to exchange JSON data[cite: 286, 293, 307, 309].

---

## How It Works

The application's architecture is designed to leverage the strengths of both Rust and Python:

1.  **UI & Main Logic (Rust)**: The core application is a native `eframe` GUI that manages the user interface, state, and controls[cite: 197].
2.  **Python Subprocess**: On starting the camera, the Rust application spawns a Python script (`python/mediapipe_service.py`) as a background process[cite: 293].
3.  **Data Exchange**:
    * Rust captures a frame from the camera[cite: 511].
    * The frame is converted to RGB data, serialized into a JSON object, and sent to the Python script's `stdin`[cite: 305, 307].
    * The Python script reads the JSON, processes the frame using the MediaPipe library, and extracts pose and hand landmarks[cite: 554, 561].
    * The resulting landmark data is serialized back into a JSON string and printed to `stdout`[cite: 561].
4.  **Analysis & Rendering (Rust)**:
    * The Rust application reads the JSON response from the Python process's `stdout`[cite: 309].
    * The raw landmark data is smoothed using Kalman filters[cite: 442].
    * Custom algorithms in `tracking.rs` use the smoothed joint and hand positions to calculate forearm rotation angles and classify gestures[cite: 391].
    * The final tracking data and gestures are rendered as overlays on the video feed in the `egui` UI[cite: 144].

---

## Building and Running

This project is configured for macOS, including a build script for creating a `.app` bundle.

### Prerequisites

1.  **Rust Toolchain**: Install via `rustup`.
2.  **Python 3**: Ensure `python3` is available in your PATH.
3.  **Python Dependencies**: Install the required computer vision libraries.
    ```bash
    pip3 install mediapipe opencv-python numpy
    ```
4.  **macOS Build Tools**: `Xcode Command Line Tools` are needed for the bundling script.

### Instructions

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd Supro-Rewritten
    ```

2.  **Run in Debug Mode:**
    ```bash
    cargo run
    ```

3.  **Build a Release `.app` Bundle (macOS):**
    The `build.sh` script automates the process of bundling the application, copying the Python scripts, setting the correct `Info.plist` values, and moving the final app to the `/Applications` folder[cite: 530, 540, 549, 550].
    ```bash
    ./build.sh
    ```

---

## Project Structure

````

.
‚îú‚îÄ‚îÄ Cargo.toml              \# Rust project manifest and dependencies
‚îú‚îÄ‚îÄ build.sh                \# macOS application bundling script
‚îú‚îÄ‚îÄ assets/                 \# Icons and other static assets
‚îú‚îÄ‚îÄ python/
‚îÇ   ‚îî‚îÄ‚îÄ mediapipe\_service.py  \# The Python backend for MediaPipe processing
‚îî‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ main.rs               \# Application entry point and setup
‚îú‚îÄ‚îÄ app.rs                \# Core application struct, UI layout, and event handling
‚îú‚îÄ‚îÄ tracking.rs           \# All tracking logic, gesture algorithms, and Kalman filters
‚îú‚îÄ‚îÄ mediapipe\_bridge.rs   \# Manages communication with the Python subprocess
‚îú‚îÄ‚îÄ data.rs               \# Handles CSV and HTML data exporting
‚îú‚îÄ‚îÄ video.rs              \# Manages camera and video file sources via `nokhwa`
‚îî‚îÄ‚îÄ ui.rs                 \# Theming and custom UI components

```

---

## License

```


================================================
FILE: build.rs
================================================
use std::env;
use std::fs;
use std::path::Path;

fn main() {
    let target_dir = env::var("OUT_DIR").unwrap();
    let manifest_dir = env::var("CARGO_MANIFEST_DIR").unwrap();
    
    // Copy Info.plist to target directory
    let info_plist_src = Path::new(&manifest_dir).join("Info.plist");
    let info_plist_dst = Path::new(&target_dir).join("../../Info.plist");
    
    if info_plist_src.exists() {
        fs::copy(info_plist_src, info_plist_dst).expect("Failed to copy Info.plist");
    }
}


================================================
FILE: build.sh
================================================
#!/bin/bash
set -euo pipefail

echo "Building bundle..."
cargo bundle --release

APP="target/release/bundle/osx/Arm Tracker.app"
PLIST="$APP/Contents/Info.plist"
RES="$APP/Contents/Resources"

# Your .icns built from PNG beforehand and tracked by Cargo.toml:
# [package.metadata.bundle]
# icon = ["assets/AppIcon.icns"]
ASSETS_ICNS="assets/AppIcon.icns"

echo "Ensuring AppIcon.icns exists inside the bundle..."
if [[ ! -f "$RES/AppIcon.icns" ]]; then
  echo "  - AppIcon.icns not found in bundle Resources. Attempting to copy from assets/..."
  mkdir -p "$RES"
  if [[ -f "$ASSETS_ICNS" ]]; then
    cp "$ASSETS_ICNS" "$RES/AppIcon.icns"
    echo "  - Copied: $ASSETS_ICNS -> $RES/AppIcon.icns"
  else
    echo "ERROR: $ASSETS_ICNS not found. Build AppIcon.icns from your 1024x1024 PNG first."
    echo "Hint:"
    echo "  mkdir -p assets/AppIcon.iconset"
    echo "  sips -z 16 16     appicon_1024.png --out assets/AppIcon.iconset/icon_16x16.png"
    echo "  sips -z 32 32     appicon_1024.png --out assets/AppIcon.iconset/icon_16x16@2x.png"
    echo "  sips -z 32 32     appicon_1024.png --out assets/AppIcon.iconset/icon_32x32.png"
    echo "  sips -z 64 64     appicon_1024.png --out assets/AppIcon.iconset/icon_32x32@2x.png"
    echo "  sips -z 128 128   appicon_1024.png --out assets/AppIcon.iconset/icon_128x128.png"
    echo "  sips -z 256 256   appicon_1024.png --out assets/AppIcon.iconset/icon_128x128@2x.png"
    echo "  sips -z 256 256   appicon_1024.png --out assets/AppIcon.iconset/icon_256x256.png"
    echo "  sips -z 512 512   appicon_1024.png --out assets/AppIcon.iconset/icon_256x256@2x.png"
    echo "  sips -z 512 512   appicon_1024.png --out assets/AppIcon.iconset/icon_512x512.png"
    echo "  cp appicon_1024.png assets/AppIcon.iconset/icon_512x512@2x.png"
    echo "  iconutil -c icns assets/AppIcon.iconset -o assets/AppIcon.icns"
    exit 1
  fi
fi

echo "Setting icon and permissions keys in Info.plist..."

# Ensure CFBundleIconName = AppIcon (no extension)
# If the key exists, set; otherwise, add.
if /usr/libexec/PlistBuddy -c 'Print :CFBundleIconName' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :CFBundleIconName AppIcon' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :CFBundleIconName string AppIcon' "$PLIST"
fi

# If legacy CFBundleIconFile exists and has an extension, fix it (macOS prefers name w/o extension)
if /usr/libexec/PlistBuddy -c 'Print :CFBundleIconFile' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :CFBundleIconFile AppIcon' "$PLIST" || true
fi

# (Optional) Set App Store category string to Developer Tools
if /usr/libexec/PlistBuddy -c 'Print :LSApplicationCategoryType' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :LSApplicationCategoryType public.app-category.developer-tools' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :LSApplicationCategoryType string public.app-category.developer-tools' "$PLIST"
fi

# Add/Update privacy usage descriptions
if /usr/libexec/PlistBuddy -c 'Print :NSCameraUsageDescription' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSCameraUsageDescription This app requires camera access to track arm rotation movements for analysis.' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSCameraUsageDescription string This app requires camera access to track arm rotation movements for analysis.' "$PLIST"
fi

if /usr/libexec/PlistBuddy -c 'Print :NSMicrophoneUsageDescription' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSMicrophoneUsageDescription Arm Tracker may record audio while capturing video.' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSMicrophoneUsageDescription string Arm Tracker may record audio while capturing video.' "$PLIST"
fi

# (Nice to have) Mark high-DPI capable
if /usr/libexec/PlistBuddy -c 'Print :NSHighResolutionCapable' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSHighResolutionCapable true' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSHighResolutionCapable bool true' "$PLIST"
fi

# (Strongly recommended) bump CFBundleVersion to avoid icon caching issues
# If CFBundleVersion is numeric, increment; else set a fresh numeric build
if /usr/libexec/PlistBuddy -c 'Print :CFBundleVersion' "$PLIST" >/dev/null 2>&1; then
  CUR_VER="$(/usr/libexec/PlistBuddy -c 'Print :CFBundleVersion' "$PLIST" || echo 0)"
  if [[ "$CUR_VER" =~ ^[0-9]+$ ]]; then
    NEW_VER=$((CUR_VER + 1))
  else
    NEW_VER="$(date +%s)"
  fi
  /usr/libexec/PlistBuddy -c "Set :CFBundleVersion $NEW_VER" "$PLIST"
else
  /usr/libexec/PlistBuddy -c "Add :CFBundleVersion string $(date +%s)" "$PLIST"
fi

echo ""
echo "‚úì Bundle created successfully"
echo "Verifying keys..."
/usr/libexec/PlistBuddy -c 'Print :CFBundleIdentifier' "$PLIST" || echo "CFBundleIdentifier missing (check Cargo.toml identifier)"
/usr/libexec/PlistBuddy -c 'Print :CFBundleIconName' "$PLIST" || echo "CFBundleIconName missing"
ls -l "$RES/AppIcon.icns" || true
echo ""

# After creating the bundle, copy Python files
PYTHON_DIR="$APP/Contents/Resources/python"
mkdir -p "$PYTHON_DIR"
cp -r python/*.py "$PYTHON_DIR/"
echo "Copied Python scripts to bundle"

# Replace old app in /Applications to avoid duplicate cache entries
DEST="/Applications/Arm Tracker.app"
if [[ -d "$DEST" ]]; then
  echo "Removing old /Applications bundle..."
  rm -rf "$DEST"
fi
echo "Copying new bundle to /Applications..."
cp -R "$APP" "$DEST"

echo "Refreshing Dock/Finder to clear icon caches..."
killall Dock || true
killall Finder || true

echo "Opening app..."
open "$DEST"

echo "Done."



================================================
FILE: Cargo.toml
================================================
[package]
name = "arm_tracker"
version = "0.1.0"
edition = "2021"

[package.metadata.bundle]
name = "Arm Tracker"
identifier = "com.armtracker.app"
icon = ["assets/AppIcon.icns"]
version = "1.0.0"
copyright = "Copyright (c) 2025"
category = "DeveloperTool" 
short_description = "Arm rotation tracking system"
long_description = "A sophisticated motion tracking application for analyzing arm rotation patterns"
osx_minimum_system_version = "10.13"

[package.metadata.bundle.osx]
info_plist_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/Info.plist"

[dependencies]
# GUI Framework
eframe = "0.24"
egui = "0.24"
egui_extras = "0.24"
image = { version = "0.24", features = ["jpeg", "png"] }
cocoa = "0.25"
objc = "0.2"

# File Dialog
rfd = "0.12"

# Camera support
nokhwa = { version = "0.10", features = ["input-native", "output-threaded"] }

# SVG support for logo
resvg = "0.35"
usvg = "0.35"

# Math and Linear Algebra
nalgebra = "0.32"
nalgebra-glm = "0.18"

# Async Runtime
tokio = { version = "1.35", features = ["full"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
csv = "1.3"

# Time and Date
chrono = "0.4"

# File System
directories = "5.0"

# UUID for temp file naming
uuid = { version = "1.6", features = ["v4", "fast-rng"] }

# Error Handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = "0.3"

# Basic utilities
once_cell = "1.19"

[[bin]]
name = "arm_tracker"
path = "src/main.rs"


================================================
FILE: combine.py
================================================
#!/usr/bin/env python3
"""
Combine all file contents under a source tree into a single text file,
excluding a specific subdirectory.

Usage:
    python3 combine_src_to_text.py
    # or customize paths by editing DEFAULT_* constants or passing CLI args:
    python3 combine_src_to_text.py "/path/to/src" "/path/to/src/bin" "/path/to/output.txt"
"""

from __future__ import annotations
import sys
import os
from pathlib import Path
from typing import Iterable

# --- Default locations (edit these if you want to run without CLI args) ---
DEFAULT_SRC_DIR = Path("/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/src")
DEFAULT_EXCLUDE_DIR = DEFAULT_SRC_DIR / "bin"
DEFAULT_OUTPUT_FILE = DEFAULT_SRC_DIR / "_all_source.txt"
DEFAULT_EXTRA_PATHS = [
    Path("/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/Cargo.toml"),
    Path("/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/build.sh"),
    Path("/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/python/mediapipe_service.py"),
    Path("/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/python/mediapipe_processor.py"),
]
# -------------------------------------------------------------------------


def iter_files(root: Path, exclude_dir: Path) -> Iterable[Path]:
    """
    Yield all files under `root`, skipping anything inside `exclude_dir`.
    """
    root = root.resolve()
    exclude_dir = exclude_dir.resolve()
    for dirpath, dirnames, filenames in os.walk(root):
        current = Path(dirpath).resolve()

        # If we're at/inside the excluded folder, prune it from traversal.
        # (Modify dirnames in-place so os.walk won't descend.)
        # Compare using .resolve() to be robust to symlinks.
        dirnames[:] = [
            d for d in dirnames
            if (Path(dirpath) / d).resolve() != exclude_dir
            and not d.startswith(".git")   # nice to skip VCS internals if present
        ]

        # Skip everything if this subtree is the excluded path
        if current == exclude_dir or exclude_dir in current.parents:
            continue

        for fn in filenames:
            p = current / fn
            # Skip typical junk files
            if fn == ".DS_Store":
                continue
            yield p


def read_file_as_text(path: Path) -> str:
    """
    Read a file as UTF-8 text, falling back to replacement characters on decode errors.
    If the file is truly binary, you'll still get a readable best-effort dump.
    """
    try:
        # Try straightforward UTF-8 first
        return path.read_text(encoding="utf-8")
    except UnicodeDecodeError:
        # Fallback: decode with replacement to avoid crashing
        with path.open("rb") as f:
            data = f.read()
        return data.decode("utf-8", errors="replace")
    except Exception as e:
        return f"<<ERROR READING FILE: {e}>>"


def write_combined_output(
    src_dir: Path,
    exclude_dir: Path,
    out_file: Path,
    extra_paths: Iterable[Path] | None = None,
) -> None:
    """
    Create (or overwrite) `out_file` containing the concatenated contents of all files
    under `src_dir` except those inside `exclude_dir`. Each file is prefixed by a header
    showing its relative path. If `extra_paths` are provided, append them at the end
    with absolute-path headers.
    """
    src_dir = src_dir.resolve()
    exclude_dir = exclude_dir.resolve()
    out_file = out_file.resolve()

    out_file.parent.mkdir(parents=True, exist_ok=True)

    files = sorted(iter_files(src_dir, exclude_dir))
    count = 0

    with out_file.open("w", encoding="utf-8") as out:
        out.write(f"# Combined dump of {src_dir}\n")
        out.write(f"# Excluding: {exclude_dir}\n\n")

        for p in files:
            rel = p.relative_to(src_dir)
            out.write("\n")
            out.write("=" * 80 + "\n")
            out.write(f"FILE: {rel}\n")
            out.write("=" * 80 + "\n\n")
            out.write(read_file_as_text(p))
            out.write("\n")
            count += 1

        # --- append extras at the very end ---
        if extra_paths:
            out.write("\n")
            out.write("#" * 80 + "\n")
            out.write("# EXTRA FILES (appended after source tree)\n")
            out.write("#" * 80 + "\n\n")

            for xp in extra_paths:
                xp = Path(xp).resolve()
                out.write("\n")
                out.write("=" * 80 + "\n")
                out.write(f"EXTRA FILE: {xp}\n")
                out.write("=" * 80 + "\n\n")
                out.write(read_file_as_text(xp))
                out.write("\n")

    print(f"Wrote {count} in-tree files + {len(list(extra_paths or []))} extras into: {out_file}")


def main() -> None:
    """
    Entry point. You can:
      - Run with no args to use the DEFAULT_* constants at the top, or
      - Provide 1‚ÄìN args:
            arg1 = src_dir
            arg2 = exclude_dir (defaults to <src_dir>/bin if omitted)
            arg3 = output_file (defaults to <src_dir>/_all_source.txt if omitted)
            arg4..N = extra file paths to append at end
    """
    argv = sys.argv[1:]

    if len(argv) == 0:
        src_dir = DEFAULT_SRC_DIR
        exclude_dir = DEFAULT_EXCLUDE_DIR
        out_file = DEFAULT_OUTPUT_FILE
        extra_paths = DEFAULT_EXTRA_PATHS
    else:
        src_dir = Path(argv[0])
        exclude_dir = Path(argv[1]) if len(argv) >= 2 else (src_dir / "bin")
        out_file = Path(argv[2]) if len(argv) >= 3 else (src_dir / "_all_source.txt")
        extra_paths = [Path(p) for p in argv[3:]] if len(argv) >= 4 else DEFAULT_EXTRA_PATHS

    write_combined_output(src_dir, exclude_dir, out_file, extra_paths)


if __name__ == "__main__":
    main()



================================================
FILE: Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>CFBundleDevelopmentRegion</key>
    <string>en</string>
    <key>CFBundleExecutable</key>
    <string>arm_tracker</string>
    <key>CFBundleIdentifier</key>
    <string>com.armtracker.app</string>
    <key>CFBundleInfoDictionaryVersion</key>
    <string>6.0</string>
    <key>CFBundleName</key>
    <string>Arm Tracker</string>
    <key>CFBundlePackageType</key>
    <string>APPL</string>
    <key>CFBundleShortVersionString</key>
    <string>1.0.0</string>
    <key>CFBundleVersion</key>
    <string>1</string>
    <key>LSMinimumSystemVersion</key>
    <string>10.13</string>
    <key>NSHighResolutionCapable</key>
    <true/>
    <key>NSCameraUsageDescription</key>
    <string>This app requires camera access to track arm rotation movements for analysis.</string>
    <key>NSMicrophoneUsageDescription</key>
    <string>Arm Tracker may record audio while capturing video.</string>
</dict>
</plist>


================================================
FILE: python/mediapipe_processor.py
================================================
#mediapipe_processor.py
import mediapipe as mp
import numpy as np
import json
import cv2

class MediaPipeProcessor:
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,
            smooth_landmarks=True,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
    
    def process_frame(self, frame_data):
        """Process a frame and return landmarks"""
        # Convert frame data to numpy array
        frame = np.frombuffer(frame_data, dtype=np.uint8)
        frame = frame.reshape((720, 1280, 3))  # Adjust dimensions as needed
        
        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Process with MediaPipe
        pose_results = self.pose.process(rgb_frame)
        hands_results = self.hands.process(rgb_frame)
        
        result = {
            'pose_landmarks': [],
            'hand_landmarks': []
        }
        
        if pose_results.pose_landmarks:
            result['pose_landmarks'] = [
                [lm.x, lm.y, lm.z] 
                for lm in pose_results.pose_landmarks.landmark
            ]
        
        if hands_results.multi_hand_landmarks:
            for hand_landmarks in hands_results.multi_hand_landmarks:
                hand_data = [
                    [lm.x, lm.y, lm.z] 
                    for lm in hand_landmarks.landmark
                ]
                result['hand_landmarks'].append(hand_data)
        
        return json.dumps(result)
    
    def cleanup(self):
        self.pose.close()
        self.hands.close()


================================================
FILE: python/mediapipe_service.py
================================================
#!/usr/bin/env python3
import sys
import json
import numpy as np
import traceback

try:
    import mediapipe as mp
    import cv2
except ImportError as e:
    print(f"Error: Missing required packages: {e}", file=sys.stderr)
    print("Install with: pip3 install mediapipe opencv-python numpy", file=sys.stderr)
    sys.exit(1)

class MediaPipeService:
    def __init__(self):
        # Initialize pose tracking
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,  # Changed from 0 to 1 for better accuracy
            smooth_landmarks=True,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5  # Increased from 0.3
        )
        
        # Initialize hand tracking - ENABLED
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,  # Changed from 0 to 1
            min_detection_confidence=0.4,  # Lowered from 0.5
            min_tracking_confidence=0.4
        )
        
        print("MediaPipe service initialized with hands enabled", file=sys.stderr)
    
    def process_frame(self, frame_data):
        try:
            width = frame_data['width']
            height = frame_data['height']
            data = np.array(frame_data['data'], dtype=np.uint8)
            
            # Reshape frame
            frame = data.reshape((height, width, 3))
            
            # Process with MediaPipe
            pose_results = self.pose.process(frame)
            hands_results = self.hands.process(frame)
            
            result = {
                'pose_landmarks': [],
                'hand_landmarks': []
            }
            
            if pose_results.pose_landmarks:
                result['pose_landmarks'] = [
                    [lm.x, lm.y, lm.z] 
                    for lm in pose_results.pose_landmarks.landmark
                ]
            
            # PROCESS HANDS - ENABLED
            if hands_results.multi_hand_landmarks:
                for hand_landmarks in hands_results.multi_hand_landmarks:
                    hand_data = [
                        [lm.x, lm.y, lm.z] 
                        for lm in hand_landmarks.landmark
                    ]
                    result['hand_landmarks'].append(hand_data)
            
            return result
            
        except Exception as e:
            print(f"Error processing frame: {e}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return {'pose_landmarks': [], 'hand_landmarks': []}
    
    def run(self):
        print("READY", file=sys.stdout)
        sys.stdout.flush()
        print("MediaPipe service ready with hands tracking", file=sys.stderr)
        
        while True:
            try:
                line = sys.stdin.readline()
                if not line:
                    print("End of input stream", file=sys.stderr)
                    break
                
                frame_data = json.loads(line)
                result = self.process_frame(frame_data)
                print(json.dumps(result))
                sys.stdout.flush()
                
            except json.JSONDecodeError as e:
                print(f"JSON decode error: {e}", file=sys.stderr)
                print(json.dumps({'pose_landmarks': [], 'hand_landmarks': []}))
                sys.stdout.flush()
            except Exception as e:
                print(f"Service error: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                print(json.dumps({'pose_landmarks': [], 'hand_landmarks': []}))
                sys.stdout.flush()

if __name__ == '__main__':
    try:
        service = MediaPipeService()
        service.run()
    except Exception as e:
        print(f"Failed to start service: {e}", file=sys.stderr)
        sys.exit(1)


================================================
FILE: python/requirements.txt
================================================
mediapipe>=0.10.0
opencv-python>=4.8.0
numpy>=1.24.0


================================================
FILE: src/data.rs
================================================
// src/data.rs
use crate::tracking::{TrackingResult, GestureType};
use csv::Writer;
use std::path::{Path, PathBuf};
use std::fs::File;
use anyhow::{Result, Context};
use chrono::{DateTime, Local};
use serde::Serialize;

#[derive(Debug, Serialize)]
struct TrackingRecord {
    timestamp: f64,
    frame: i32,
    tracking_lost: bool,
    
    // Joint positions
    left_shoulder_x: Option<f64>,
    left_shoulder_y: Option<f64>,
    left_shoulder_z: Option<f64>,
    left_shoulder_confidence: Option<f64>,
    
    right_shoulder_x: Option<f64>,
    right_shoulder_y: Option<f64>,
    right_shoulder_z: Option<f64>,
    right_shoulder_confidence: Option<f64>,
    
    left_elbow_x: Option<f64>,
    left_elbow_y: Option<f64>,
    left_elbow_z: Option<f64>,
    left_elbow_confidence: Option<f64>,
    
    right_elbow_x: Option<f64>,
    right_elbow_y: Option<f64>,
    right_elbow_z: Option<f64>,
    right_elbow_confidence: Option<f64>,
    
    left_wrist_x: Option<f64>,
    left_wrist_y: Option<f64>,
    left_wrist_z: Option<f64>,
    left_wrist_confidence: Option<f64>,
    
    right_wrist_x: Option<f64>,
    right_wrist_y: Option<f64>,
    right_wrist_z: Option<f64>,
    right_wrist_confidence: Option<f64>,
    
    // Gestures
    left_gesture: Option<String>,
    left_gesture_confidence: Option<f64>,
    left_gesture_angle: Option<f64>,
    
    right_gesture: Option<String>,
    right_gesture_confidence: Option<f64>,
    right_gesture_angle: Option<f64>,
}

pub struct DataExporter {
    output_dir: PathBuf,
    session_name: String,
    tracking_data: Vec<TrackingResult>,
    timestamps: Vec<f64>,
}

impl DataExporter {
    pub fn new(output_dir: impl AsRef<Path>, session_name: Option<String>) -> Self {
        let session_name = session_name.unwrap_or_else(|| {
            format!("session_{}", Local::now().format("%Y%m%d_%H%M%S"))
        });
        
        Self {
            output_dir: output_dir.as_ref().to_path_buf(),
            session_name,
            tracking_data: Vec::new(),
            timestamps: Vec::new(),
        }
    }
    
    pub fn add_frame(&mut self, result: TrackingResult, timestamp: f64) {
        self.tracking_data.push(result);
        self.timestamps.push(timestamp);
    }
    
    pub fn export_csv(&self) -> Result<PathBuf> {
        let csv_path = self.output_dir
            .join(&self.session_name)
            .join("tracking_data.csv");
        
        // Create directory if it doesn't exist
        if let Some(parent) = csv_path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let file = File::create(&csv_path)?;
        let mut writer = Writer::from_writer(file);
        
        for (i, (result, timestamp)) in self.tracking_data.iter()
            .zip(self.timestamps.iter())
            .enumerate() 
        {
            let record = self.create_record(i as i32, *timestamp, result);
            writer.serialize(record)?;
        }
        
        writer.flush()?;
        Ok(csv_path)
    }
    
    fn create_record(&self, frame: i32, timestamp: f64, result: &TrackingResult) -> TrackingRecord {
        let mut record = TrackingRecord {
            timestamp,
            frame,
            tracking_lost: result.tracking_lost,
            left_shoulder_x: None,
            left_shoulder_y: None,
            left_shoulder_z: None,
            left_shoulder_confidence: None,
            right_shoulder_x: None,
            right_shoulder_y: None,
            right_shoulder_z: None,
            right_shoulder_confidence: None,
            left_elbow_x: None,
            left_elbow_y: None,
            left_elbow_z: None,
            left_elbow_confidence: None,
            right_elbow_x: None,
            right_elbow_y: None,
            right_elbow_z: None,
            right_elbow_confidence: None,
            left_wrist_x: None,
            left_wrist_y: None,
            left_wrist_z: None,
            left_wrist_confidence: None,
            right_wrist_x: None,
            right_wrist_y: None,
            right_wrist_z: None,
            right_wrist_confidence: None,
            left_gesture: None,
            left_gesture_confidence: None,
            left_gesture_angle: None,
            right_gesture: None,
            right_gesture_confidence: None,
            right_gesture_angle: None,
        };
        
        // Fill in joint data
        for (name, joint) in &result.joints {
            match name.as_str() {
                "left_shoulder" => {
                    record.left_shoulder_x = Some(joint.position.x);
                    record.left_shoulder_y = Some(joint.position.y);
                    record.left_shoulder_z = Some(joint.position.z);
                    record.left_shoulder_confidence = Some(joint.confidence);
                }
                "right_shoulder" => {
                    record.right_shoulder_x = Some(joint.position.x);
                    record.right_shoulder_y = Some(joint.position.y);
                    record.right_shoulder_z = Some(joint.position.z);
                    record.right_shoulder_confidence = Some(joint.confidence);
                }
                "left_elbow" => {
                    record.left_elbow_x = Some(joint.position.x);
                    record.left_elbow_y = Some(joint.position.y);
                    record.left_elbow_z = Some(joint.position.z);
                    record.left_elbow_confidence = Some(joint.confidence);
                }
                "right_elbow" => {
                    record.right_elbow_x = Some(joint.position.x);
                    record.right_elbow_y = Some(joint.position.y);
                    record.right_elbow_z = Some(joint.position.z);
                    record.right_elbow_confidence = Some(joint.confidence);
                }
                "left_wrist" => {
                    record.left_wrist_x = Some(joint.position.x);
                    record.left_wrist_y = Some(joint.position.y);
                    record.left_wrist_z = Some(joint.position.z);
                    record.left_wrist_confidence = Some(joint.confidence);
                }
                "right_wrist" => {
                    record.right_wrist_x = Some(joint.position.x);
                    record.right_wrist_y = Some(joint.position.y);
                    record.right_wrist_z = Some(joint.position.z);
                    record.right_wrist_confidence = Some(joint.confidence);
                }
                _ => {}
            }
        }
        
        // Fill in gesture data
        if let Some(left_gesture) = &result.left_gesture {
            record.left_gesture = Some(format!("{:?}", left_gesture.gesture_type));
            record.left_gesture_confidence = Some(left_gesture.confidence);
            record.left_gesture_angle = Some(left_gesture.angle);
        }
        
        if let Some(right_gesture) = &result.right_gesture {
            record.right_gesture = Some(format!("{:?}", right_gesture.gesture_type));
            record.right_gesture_confidence = Some(right_gesture.confidence);
            record.right_gesture_angle = Some(right_gesture.angle);
        }
        
        record
    }
    
    pub fn generate_report(&self) -> Result<PathBuf> {
        let report_path = self.output_dir
            .join(&self.session_name)
            .join("report.html");
        
        // Create directory if it doesn't exist
        if let Some(parent) = report_path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let html_content = self.create_html_report()?;
        std::fs::write(&report_path, html_content)?;
        
        Ok(report_path)
    }
    
    fn create_html_report(&self) -> Result<String> {
        let total_frames = self.tracking_data.len();
        let tracking_lost_count = self.tracking_data.iter()
            .filter(|r| r.tracking_lost)
            .count();
        
        let left_supination_count = self.tracking_data.iter()
            .filter(|r| r.left_gesture.as_ref()
                .map(|g| g.gesture_type == GestureType::Supination)
                .unwrap_or(false))
            .count();
        
        let left_pronation_count = self.tracking_data.iter()
            .filter(|r| r.left_gesture.as_ref()
                .map(|g| g.gesture_type == GestureType::Pronation)
                .unwrap_or(false))
            .count();
        
        let html = format!(r#"
<!DOCTYPE html>
<html>
<head>
    <title>Arm Tracking Report - {}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background: #f5f5f5; }}
        h1 {{ color: #333; }}
        .stats {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .stat-item {{ margin: 10px 0; }}
        .stat-label {{ font-weight: bold; color: #666; }}
        .stat-value {{ color: #4682EA; font-size: 1.2em; }}
    </style>
</head>
<body>
    <h1>Arm Tracking Session Report</h1>
    <div class="stats">
        <h2>Session: {}</h2>
        <div class="stat-item">
            <span class="stat-label">Total Frames:</span>
            <span class="stat-value">{}</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Tracking Success Rate:</span>
            <span class="stat-value">{:.1}%</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Left Arm Supination:</span>
            <span class="stat-value">{} frames</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Left Arm Pronation:</span>
            <span class="stat-value">{} frames</span>
        </div>
    </div>
</body>
</html>
        "#,
            self.session_name,
            self.session_name,
            total_frames,
            (1.0 - tracking_lost_count as f64 / total_frames as f64) * 100.0,
            left_supination_count,
            left_pronation_count
        );
        
        Ok(html)
    }
}


================================================
FILE: src/main.rs
================================================
// src/main.rs
mod app;
mod tracking;
mod ui;
mod video;
mod data;
mod mediapipe_bridge;

use eframe::egui;
use usvg::TreeParsing;

#[cfg(target_os = "macos")]
pub(crate) fn set_macos_dock_icon_from_bundle() {
    use cocoa::{
        appkit::{NSApp, NSImage},
        base::{id, nil},
        foundation::NSString,
    };
    use objc::{class, msg_send, sel, sel_impl};

    unsafe {
        // NSBundle *bundle = [NSBundle mainBundle];
        let bundle: id = msg_send![class!(NSBundle), mainBundle];

        // NSString *name = @"AppIcon"; NSString *typ = @"icns";
        let name = NSString::alloc(nil).init_str("AppIcon");
        let typ = NSString::alloc(nil).init_str("icns");

        // NSString *path = [bundle pathForResource:name ofType:typ];
        let path: id = msg_send![bundle, pathForResource: name ofType: typ];
        if path != nil {
            // NSImage *img = [[NSImage alloc] initWithContentsOfFile:path];
            let img: id = msg_send![NSImage::alloc(nil), initWithContentsOfFile: path];
            if img != nil {
                // [NSApp setApplicationIconImage:img];
                let app = NSApp();
                let _: () = msg_send![app, setApplicationIconImage: img];
            }
        }
    }
}

#[cfg(not(target_os = "macos"))]
fn set_macos_dock_icon_from_bundle() {
    // no-op on non-macOS
}

fn main() {
    // Initialize logging
    tracing_subscriber::fmt::init();
    set_macos_dock_icon_from_bundle();

    if let Ok(p) = std::env::current_exe() {
        eprintln!("Running from: {}", p.display());
    }

    // DEBUG: List available cameras
    println!("=== Camera Detection Debug ===");
    match nokhwa::query(nokhwa::utils::ApiBackend::Auto) {
        Ok(cameras) => {
            println!("Found {} camera(s):", cameras.len());
            for (i, camera) in cameras.iter().enumerate() {
                println!("  [{}] {}", i, camera.human_name());
            }
        }
        Err(e) => {
            println!("Failed to query cameras: {}", e);
        }
    }
    println!("============================\n");

    // Set up GUI options
    let options = eframe::NativeOptions {
        viewport: egui::ViewportBuilder::default()
            .with_inner_size([1400.0, 900.0])
            .with_min_inner_size([1200.0, 800.0]),
        centered: true,
        ..Default::default()
    };

    // Run the application (updated title)
    let result = eframe::run_native(
        "Supro Arm Tracker",
        options,
        Box::new(|cc| {
            // Configure fonts and visuals
            configure_fonts(&cc.egui_ctx);
            cc.egui_ctx.set_visuals(create_visuals());

            Box::new(app::ArmTrackerApp::new(cc))
        }),
    );

    if let Err(e) = result {
        eprintln!("Error running application: {:?}", e);
    }
}



fn load_svg_as_rgba(path: &str, size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    let svg_data = std::fs::read_to_string(path)?;
    let opt = usvg::Options::default();
    let tree = usvg::Tree::from_str(&svg_data, &opt)?;
    
    // Use resvg's re-exported tiny_skia types
    let pixmap_size = tree.size.to_int_size();
    let mut pixmap = resvg::tiny_skia::Pixmap::new(size, size).unwrap();
    
    let scale = size as f32 / pixmap_size.width().max(pixmap_size.height()) as f32;
    let transform = resvg::tiny_skia::Transform::from_scale(scale, scale);
    
    // Use the Tree's render method directly with consistent types
    resvg::Tree::from_usvg(&tree).render(transform, &mut pixmap.as_mut());
    
    Ok(pixmap.data().to_vec())
}

fn configure_fonts(ctx: &egui::Context) {
    let mut fonts = egui::FontDefinitions::default();
    
    // Load Montserrat font
    let font_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/fonts/Montserrat-VariableFont_wght.ttf";
    if let Ok(font_data) = std::fs::read(font_path) {
        fonts.font_data.insert(
            "Montserrat".to_owned(),
            egui::FontData::from_owned(font_data),
        );
        
        // Set Montserrat as the primary font
        fonts.families.entry(egui::FontFamily::Proportional)
            .or_default()
            .insert(0, "Montserrat".to_owned());
            
        fonts.families.entry(egui::FontFamily::Monospace)
            .or_default()
            .push("Montserrat".to_owned());
    }
    
    ctx.set_fonts(fonts);
}

fn create_visuals() -> egui::Visuals {
    let mut visuals = egui::Visuals::dark();
    
    // Customize colors for a modern, professional look
    visuals.widgets.noninteractive.bg_fill = egui::Color32::from_rgb(30, 30, 35);
    visuals.widgets.inactive.bg_fill = egui::Color32::from_rgb(45, 45, 52);
    visuals.widgets.hovered.bg_fill = egui::Color32::from_rgb(55, 55, 65);
    visuals.widgets.active.bg_fill = egui::Color32::from_rgb(70, 130, 240);
    
    // Adjust rounding for modern appearance
    visuals.widgets.noninteractive.rounding = egui::Rounding::same(8.0);
    visuals.widgets.inactive.rounding = egui::Rounding::same(8.0);
    visuals.widgets.hovered.rounding = egui::Rounding::same(8.0);
    visuals.widgets.active.rounding = egui::Rounding::same(8.0);
    
    visuals.window_rounding = egui::Rounding::same(12.0);
    visuals.menu_rounding = egui::Rounding::same(8.0);
    
    visuals
}


================================================
FILE: src/mediapipe_bridge.rs
================================================
// src/mediapipe_bridge.rs
use anyhow::{Result, Context};
use nalgebra::Vector3;
use std::process::{Command, Stdio, Child};
use std::io::{Write, BufRead, BufReader};
use serde::{Deserialize, Serialize};
use image::DynamicImage;
use std::time::{Duration, Instant};

#[derive(Debug, Serialize, Deserialize)]
struct MediaPipeFrame {
    width: u32,
    height: u32,
    data: Vec<u8>,
}

#[derive(Debug, Deserialize)]
pub struct MediaPipeResult {
    pub pose_landmarks: Vec<[f64; 3]>,    // Make public
    pub hand_landmarks: Vec<Vec<[f64; 3]>>, // Make public
}

pub struct MediaPipeWrapper {
    python_process: Child,
    stdin: std::process::ChildStdin,
    stdout: BufReader<std::process::ChildStdout>,
}

impl MediaPipeWrapper {
    pub fn new() -> Result<Self> {
        eprintln!("=== MediaPipe Initialization ===");
        
        // Try multiple paths to find the Python script
        let possible_paths = vec![
            // For bundled app
            std::env::current_exe()
                .ok()
                .and_then(|exe| exe.parent().map(|p| p.join("../Resources/python/mediapipe_service.py"))),
            // For development
            std::env::current_dir().ok().map(|d| d.join("python/mediapipe_service.py")),
        ];
        
        let script_path = possible_paths
            .into_iter()
            .flatten()
            .find(|p| p.exists())
            .ok_or_else(|| anyhow::anyhow!("Could not find mediapipe_service.py"))?;
        
        eprintln!("Found Python script at: {}", script_path.display());
        
        let mut child = Command::new("python3")
            .arg(&script_path)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::inherit())
            .spawn()
            .context("Failed to spawn Python MediaPipe process - is Python3 installed?")?;
        
        eprintln!("Python process spawned with PID: {:?}", child.id());
        
        let stdin = child.stdin.take()
            .ok_or_else(|| anyhow::anyhow!("Failed to get stdin"))?;
        let mut stdout = BufReader::new(
            child.stdout.take()
                .ok_or_else(|| anyhow::anyhow!("Failed to get stdout"))?
        );
        
        // Wait for ready signal with timeout
        eprintln!("Waiting for MediaPipe service to be ready...");
        let mut ready_line = String::new();
        let start = Instant::now();
        let timeout = Duration::from_secs(15); // Increased timeout
        
        loop {
            if start.elapsed() > timeout {
                eprintln!("Timeout after {:?}", start.elapsed());
                return Err(anyhow::anyhow!("Timeout waiting for MediaPipe service"));
            }
            
            match stdout.read_line(&mut ready_line) {
                Ok(0) => {
                    eprintln!("Python process closed unexpectedly");
                    return Err(anyhow::anyhow!("Python process terminated"));
                }
                Ok(_) => {
                    eprintln!("Received from Python: {}", ready_line.trim());
                    if ready_line.trim() == "READY" {
                        eprintln!("‚úì MediaPipe service is ready!");
                        break;
                    }
                }
                Err(e) => {
                    eprintln!("Error reading from Python: {}", e);
                    return Err(anyhow::anyhow!("Failed to read from Python process: {}", e));
                }
            }
        }
        
        eprintln!("=== MediaPipe Initialized Successfully ===");
        
        Ok(Self {
            python_process: child,
            stdin,
            stdout,
        })
    }
    
    pub fn process_image(&mut self, image: &DynamicImage) -> Result<MediaPipeResult> {
        // Convert image to RGB bytes
        let rgb = image.to_rgb8();
        let frame_data = MediaPipeFrame {
            width: rgb.width(),
            height: rgb.height(),
            data: rgb.into_raw(),
        };
        
        eprintln!("Sending frame: {}x{} ({} bytes)", 
                 frame_data.width, frame_data.height, frame_data.data.len());
        
        // Send frame to Python
        let json_data = serde_json::to_string(&frame_data)?;
        writeln!(self.stdin, "{}", json_data)?;
        self.stdin.flush()?;
        
        // Read response
        let mut response = String::new();
        self.stdout.read_line(&mut response)
            .context("Failed to read response from MediaPipe")?;
        
        if response.trim().is_empty() {
            return Err(anyhow::anyhow!("Empty response from MediaPipe"));
        }
        
        // Parse result
        let result: MediaPipeResult = serde_json::from_str(&response)
            .context("Failed to parse MediaPipe response")?;
        
        if !result.pose_landmarks.is_empty() {
            eprintln!("‚úì Received {} pose landmarks", result.pose_landmarks.len());
        } else {
            eprintln!("‚úó No pose landmarks detected");
        }
        
        Ok(result)
    }
    
    pub fn get_pose_landmarks(&mut self, image: &DynamicImage) -> Result<Vec<Vector3<f64>>> {
        let result = self.process_image(image)?;
        Ok(result.pose_landmarks.into_iter()
            .map(|[x, y, z]| Vector3::new(x, y, z))
            .collect())
    }
    
    pub fn get_hand_landmarks(&mut self, image: &DynamicImage) -> Result<Vec<Vec<Vector3<f64>>>> {
        let result = self.process_image(image)?;
        Ok(result.hand_landmarks.into_iter()
            .map(|hand| hand.into_iter()
                .map(|[x, y, z]| Vector3::new(x, y, z))
                .collect())
            .collect())
    }
}

impl Drop for MediaPipeWrapper {
    fn drop(&mut self) {
        eprintln!("Shutting down MediaPipe service");
        let _ = self.python_process.kill();
    }
}


================================================
FILE: src/tracking.rs
================================================
// src/tracking.rs - Fixed version with lazy MediaPipe initialization
use nalgebra::{Vector3, Vector6, Matrix3, Matrix6, Matrix3x6};
use std::collections::{HashMap, VecDeque};
use anyhow::Result;
use image::DynamicImage;
use crate::mediapipe_bridge::MediaPipeWrapper;
use std::time::Instant;

#[derive(Clone)]
pub struct PerformanceMetrics {
    pub avg_fps: f32,
    pub avg_processing_time: f32,
    pub tracking_confidence: f32,
    frame_times: VecDeque<f32>,
}

pub struct KalmanFilter {
    state: Vector6<f64>,  // [x, y, z, vx, vy, vz]
    covariance: Matrix6<f64>,
    process_noise: Matrix6<f64>,
    measurement_noise: Matrix3<f64>,    
    dt: f64,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum GestureType {
    Pronation,
    Supination,
    None,
}

#[derive(Debug, Clone)]
pub struct GestureState {
    pub gesture_type: GestureType,
    pub confidence: f64,
    pub angle: f64,
}

#[derive(Debug, Clone)]
pub struct JointState {
    pub position: Vector3<f64>,
    pub velocity: Vector3<f64>,
    pub confidence: f64,
    pub pixel_pos: (i32, i32),
}

#[derive(Debug, Clone)]
pub struct HandState {
    pub landmarks: Vec<Vector3<f64>>,
    pub confidences: Vec<f64>,
    pub is_tracked: bool,
}

#[derive(Debug, Clone, Default)]
pub struct TrackingResult {
    pub tracking_lost: bool,
    pub joints: HashMap<String, JointState>,
    pub hands: HashMap<String, HandState>,
    pub left_gesture: Option<GestureState>,
    pub right_gesture: Option<GestureState>,
    pub timestamp: f64,
}

pub struct ArmTracker {
    active_arms: HashMap<String, bool>,
    active_fingers: HashMap<String, bool>,
    palm_history: HashMap<String, VecDeque<Vector3<f64>>>,
    rotation_history: HashMap<String, VecDeque<f64>>,
    last_valid_gestures: HashMap<String, GestureState>,
    config: TrackerConfig,
    // Simulation state for demo
    sim_time: f64,
    mediapipe: Option<MediaPipeWrapper>,
    mediapipe_initialized: bool,
    init_attempts: u32,
    metrics: PerformanceMetrics,
    frame_counter: u32,
    adaptive_skip_rate: usize,
    last_confidence: f64,
    joint_filters: HashMap<String, KalmanFilter>,
    hand_state_cache: HashMap<String, (HandState, u32)>,
    hand_filters: HashMap<String, Vec<KalmanFilter>>,
}

#[derive(Debug, Clone)]
pub struct TrackerConfig {
    pub history_size: usize,
    pub confidence_threshold: f64,
    pub gesture_angle_threshold: f64,
    pub min_rotation_threshold: f64,
    pub rotation_smoothing_factor: f64,
    pub min_stable_frames: usize,
    pub enable_kalman: bool,          // Add this
    pub downsample_width: u32,        // Add this
    pub adaptive_frame_skip: bool,    // Add this
    pub max_frame_skip: usize,        // Add this
}

impl Default for TrackerConfig {
    fn default() -> Self {
        Self {
            history_size: 10,
            confidence_threshold: 0.6,
            gesture_angle_threshold: 0.05,  // Lowered from 0.1
            min_rotation_threshold: 0.03,   // Lowered from 0.05
            rotation_smoothing_factor: 0.5,  // Lowered from 0.6 for faster response
            min_stable_frames: 2,
            enable_kalman: true,
            downsample_width: 640,
            adaptive_frame_skip: false,  // Disable adaptive skipping
            max_frame_skip: 1,
        }
    }
}
impl PerformanceMetrics {
    pub fn new() -> Self {
        Self {
            avg_fps: 0.0,
            avg_processing_time: 0.0,
            tracking_confidence: 0.0,
            frame_times: VecDeque::with_capacity(30),
        }
    }
}


impl KalmanFilter {
    pub fn new() -> Self {
        let mut process_noise = Matrix6::identity() * 0.1;
        process_noise.fixed_view_mut::<3, 3>(3, 3).fill_diagonal(0.2);
        
        Self {
            state: Vector6::zeros(),
            covariance: Matrix6::identity(),
            process_noise,
            measurement_noise: Matrix3::identity() * 0.1,
            dt: 1.0 / 30.0,
        }
    }
    
    pub fn predict(&mut self) {
        let mut f = Matrix6::identity();
        f.fixed_view_mut::<3, 3>(0, 3).fill_diagonal(self.dt);
        
        self.state = f * self.state;
        self.covariance = f * self.covariance * f.transpose() + self.process_noise;
    }
    
    pub fn update(&mut self, measurement: Vector3<f64>) {
        // H is 3x6 matrix (observes position, not velocity)
        let mut h = Matrix3x6::<f64>::zeros();
        h[(0, 0)] = 1.0;
        h[(1, 1)] = 1.0;
        h[(2, 2)] = 1.0;
        
        // Innovation
        let y = measurement - (h * self.state);
        
        // Innovation covariance
        let s = h * self.covariance * h.transpose() + self.measurement_noise;
        
        // Kalman gain
        let k = self.covariance * h.transpose() * s.try_inverse().unwrap();
        
        // Update state and covariance
        self.state = self.state + k * y;
        let i = Matrix6::identity();
        self.covariance = (i - k * h) * self.covariance;
    }
    
    pub fn position(&self) -> Vector3<f64> {
        Vector3::new(self.state[0], self.state[1], self.state[2])
    }
}

impl ArmTracker {
    pub fn new() -> Result<Self> {
        let mut tracker = Self {
            active_arms: HashMap::new(),
            active_fingers: HashMap::new(),
            palm_history: HashMap::new(),
            rotation_history: HashMap::new(),
            last_valid_gestures: HashMap::new(),
            config: TrackerConfig::default(),
            sim_time: 0.0,
            mediapipe: None,
            mediapipe_initialized: false,
            init_attempts: 0,
            metrics: PerformanceMetrics::new(),
            frame_counter: 0,
            adaptive_skip_rate: 1,
            last_confidence: 0.0,
            joint_filters: HashMap::new(),
            hand_state_cache: HashMap::new(),
            hand_filters: HashMap::new(),
        };
        
        // Initialize tracking flags
        tracker.active_arms.insert("left".to_string(), true);
        tracker.active_arms.insert("right".to_string(), true);
        tracker.active_fingers.insert("left".to_string(), true);
        tracker.active_fingers.insert("right".to_string(), true);
        
        // Initialize history buffers
        for side in &["left", "right"] {
            tracker.palm_history.insert(
                side.to_string(),
                VecDeque::with_capacity(tracker.config.history_size)
            );
            tracker.rotation_history.insert(
                side.to_string(),
                VecDeque::with_capacity(tracker.config.history_size)
            );
            tracker.last_valid_gestures.insert(
                side.to_string(),
                GestureState {
                    gesture_type: GestureType::None,
                    confidence: 0.0,
                    angle: 0.0,
                }
            );
        }
        
        Ok(tracker)
    }

     pub fn initialize_mediapipe(&mut self) {
        if self.mediapipe_initialized {
            eprintln!("MediaPipe already initialized");
            return;
        }
        
        eprintln!("Initializing MediaPipe for camera tracking...");
        std::thread::sleep(std::time::Duration::from_millis(500));
        
        match MediaPipeWrapper::new() {
            Ok(mp) => {
                eprintln!("‚úì MediaPipe initialized successfully");
                self.mediapipe = Some(mp);
                self.mediapipe_initialized = true;
                self.init_attempts = 0;
            }
            Err(e) => {
                eprintln!("‚úó MediaPipe initialization failed: {}", e);
                eprintln!("  Will use simulation mode for tracking");
            }
        }
    }
    
    pub fn shutdown_mediapipe(&mut self) {
        if self.mediapipe.is_some() {
            eprintln!("Shutting down MediaPipe...");
            self.mediapipe = None;
            self.mediapipe_initialized = false;
            self.init_attempts = 0;
            eprintln!("‚úì MediaPipe shutdown complete");
        }
    }
    
    fn generate_simulation_data(&mut self, result: &mut TrackingResult) {
        let t = self.sim_time;
        
        if *self.active_arms.get("left").unwrap_or(&false) {
            result.joints.insert("left_shoulder".to_string(), JointState {
                position: Vector3::new(0.3, 0.4, 0.0),
                velocity: Vector3::zeros(),
                confidence: 0.95,
                pixel_pos: (300, 200),
            });
            
            result.joints.insert("left_elbow".to_string(), JointState {
                position: Vector3::new(0.35, 0.5 + 0.05 * t.sin(), 0.0),
                velocity: Vector3::new(0.0, 0.05 * t.cos(), 0.0),
                confidence: 0.9,
                pixel_pos: (350, 300),
            });
            
            result.joints.insert("left_wrist".to_string(), JointState {
                position: Vector3::new(0.4 + 0.1 * (t * 0.5).cos(), 0.6 + 0.1 * t.sin(), 0.0),
                velocity: Vector3::new(-0.05 * (t * 0.5).sin(), 0.1 * t.cos(), 0.0),
                confidence: 0.85,
                pixel_pos: (400, 400),
            });
            
            let gesture_type = if (t * 0.3).sin() > 0.3 {
                GestureType::Supination
            } else if (t * 0.3).sin() < -0.3 {
                GestureType::Pronation
            } else {
                GestureType::None
            };
            
            if gesture_type != GestureType::None {
                result.left_gesture = Some(GestureState {
                    gesture_type,
                    confidence: 0.7 + 0.2 * (t * 2.0).sin().abs(),
                    angle: 45.0_f64.to_radians() * (t * 0.3).sin(),
                });
            }
        }
        
        if *self.active_arms.get("right").unwrap_or(&false) {
            result.joints.insert("right_shoulder".to_string(), JointState {
                position: Vector3::new(0.7, 0.4, 0.0),
                velocity: Vector3::zeros(),
                confidence: 0.95,
                pixel_pos: (700, 200),
            });
            
            result.joints.insert("right_elbow".to_string(), JointState {
                position: Vector3::new(0.65, 0.5 + 0.05 * (t + 1.5).sin(), 0.0),
                velocity: Vector3::new(0.0, 0.05 * (t + 1.5).cos(), 0.0),
                confidence: 0.9,
                pixel_pos: (650, 300),
            });
            
            result.joints.insert("right_wrist".to_string(), JointState {
                position: Vector3::new(0.6 - 0.1 * (t * 0.5 + 1.0).cos(), 0.6 + 0.1 * (t + 1.5).sin(), 0.0),
                velocity: Vector3::new(0.05 * (t * 0.5 + 1.0).sin(), 0.1 * (t + 1.5).cos(), 0.0),
                confidence: 0.85,
                pixel_pos: (600, 400),
            });
            
            let gesture_type = if (t * 0.25 + 1.0).sin() > 0.3 {
                GestureType::Pronation
            } else if (t * 0.25 + 1.0).sin() < -0.3 {
                GestureType::Supination
            } else {
                GestureType::None
            };
            
            if gesture_type != GestureType::None {
                result.right_gesture = Some(GestureState {
                    gesture_type,
                    confidence: 0.65 + 0.25 * (t * 1.5).cos().abs(),
                    angle: 50.0_f64.to_radians() * (t * 0.25 + 1.0).sin(),
                });
            }
        }
    }
    
    pub fn toggle_arm(&mut self, side: &str) {
        if let Some(active) = self.active_arms.get_mut(side) {
            *active = !*active;
        }
    }
    
    pub fn toggle_fingers(&mut self, side: &str) {
        if let Some(active) = self.active_fingers.get_mut(side) {
            *active = !*active;
        }
    }
    
    pub fn is_using_mediapipe(&self) -> bool {
        self.mediapipe.is_some() && self.mediapipe_initialized
    }
    
    pub fn is_initializing(&self) -> bool {
        false
    }
    
    pub fn reset_mediapipe(&mut self) {
        self.shutdown_mediapipe();
        eprintln!("MediaPipe reset - call initialize_mediapipe() to retry");
    }

    // Add the missing process_hand_landmarks method
    // Add the missing process_hand_landmarks method
fn process_hand_landmarks(&mut self, hand_landmarks: &[[f64; 3]], hand_index: usize, result: &mut TrackingResult) {
    if hand_landmarks.len() < 21 {
        return;
    }
    
    let landmarks: Vec<Vector3<f64>> = hand_landmarks.iter()
        .map(|lm| Vector3::new(lm[0], lm[1], lm[2]))
        .collect();
    
    let wrist_pos = landmarks[0];
    
    // Try to match to wrist joints first (strictest)
    let side = if result.joints.contains_key("left_wrist") && 
                result.joints.contains_key("right_wrist") {
        let left_wrist = &result.joints["left_wrist"].position;
        let right_wrist = &result.joints["right_wrist"].position;
        
        let dist_left = (wrist_pos - left_wrist).norm();
        let dist_right = (wrist_pos - right_wrist).norm();
        
        eprintln!("Hand {} distances - left: {:.3}, right: {:.3}", hand_index, dist_left, dist_right);
        
        // Increased threshold - MediaPipe coordinates are 0-1 range
        const MAX_HAND_ARM_DISTANCE: f64 = 0.3; // DOUBLED from 0.15
        
        if dist_left.min(dist_right) > MAX_HAND_ARM_DISTANCE {
            eprintln!("Hand {} too far from wrists (min dist: {:.3}), trying position fallback", 
                     hand_index, dist_left.min(dist_right));
            
            // FALLBACK: Use x-position if distances too large
            if wrist_pos.x > 0.5 { "right" } else { "left" }
        } else {
            if dist_left < dist_right { "left" } else { "right" }
        }
    } else {
        eprintln!("Missing wrist joints - left: {}, right: {}", 
                 result.joints.contains_key("left_wrist"),
                 result.joints.contains_key("right_wrist"));
        
        // FALLBACK: Use x-position
        if wrist_pos.x > 0.5 { "right" } else { "left" }
    };
    
    eprintln!("Hand {} assigned to {} side", hand_index, side);
    
    // Rest of your code unchanged...
    let filters = self.get_or_create_hand_filters(side);
    let mut smoothed_landmarks = Vec::new();
    
    for (i, lm) in hand_landmarks.iter().enumerate() {
        let measurement = Vector3::new(lm[0], lm[1], lm[2]);
        filters[i].predict();
        filters[i].update(measurement);
        smoothed_landmarks.push(filters[i].position());
    }

    let hand_state = HandState {
        landmarks: smoothed_landmarks.clone(),
        confidences: vec![1.0; smoothed_landmarks.len()],
        is_tracked: true,
    };
    
    self.hand_state_cache.insert(side.to_string(), (hand_state.clone(), 0));
    result.hands.insert(side.to_string(), hand_state);
    
    // Calculate gesture if we have arm joints
    if result.joints.contains_key(&format!("{}_shoulder", side)) &&
       result.joints.contains_key(&format!("{}_elbow", side)) &&
       result.joints.contains_key(&format!("{}_wrist", side)) {
        
        let shoulder = &result.joints[&format!("{}_shoulder", side)].position;
        let elbow = &result.joints[&format!("{}_elbow", side)].position;
        let wrist = &result.joints[&format!("{}_wrist", side)].position;
        
        if let Some(gesture) = self.calculate_arm_rotation_enhanced(
            side,
            shoulder,
            elbow,
            wrist,
            Some(&smoothed_landmarks)
        ) {
            if side == "left" {
                result.left_gesture = Some(gesture);
            } else {
                result.right_gesture = Some(gesture);
            }
        }
    }
}

    pub fn process_frame_with_metrics(&mut self, frame: &DynamicImage) -> Result<(TrackingResult, PerformanceMetrics)> {
        let start = Instant::now();
        let result = self.process_frame(frame)?;
        let elapsed = start.elapsed().as_secs_f32();
        
        self.metrics.frame_times.push_front(elapsed);
        if self.metrics.frame_times.len() > 30 {
            self.metrics.frame_times.pop_back();
        }
        
        self.metrics.avg_processing_time = self.metrics.frame_times.iter().sum::<f32>() 
            / self.metrics.frame_times.len() as f32;
        self.metrics.avg_fps = 1.0 / self.metrics.avg_processing_time;
        
        // Fix: Convert f64 to f32
        self.metrics.tracking_confidence = if result.joints.is_empty() {
            0.0
        } else {
            (result.joints.values()
                .map(|j| j.confidence)
                .sum::<f64>() / result.joints.len() as f64) as f32
        };
        
        Ok((result, self.metrics.clone()))
    }

    fn get_or_create_hand_filters(&mut self, side: &str) -> &mut Vec<KalmanFilter> {
        self.hand_filters.entry(side.to_string())
            .or_insert_with(|| {
                (0..21).map(|_| KalmanFilter::new()).collect()
            })
    }

    fn calculate_arm_rotation_enhanced(
        &mut self, 
        side: &str,
        shoulder: &Vector3<f64>, 
        elbow: &Vector3<f64>, 
        wrist: &Vector3<f64>,
        hand_landmarks: Option<&Vec<Vector3<f64>>>
    ) -> Option<GestureState> {
        // Calculate forearm vector
        let forearm = (wrist - elbow).normalize();
        
        // Get palm normal if hand landmarks available
        let palm_normal = hand_landmarks.and_then(|landmarks| {
            if landmarks.len() >= 21 {
                Some(self.calculate_palm_normal(landmarks))
            } else {
                None
            }
        })?;  // Early return if no palm normal
        
        // Calculate rotation axis and angle relative to anatomical reference
        let rotation_axis = palm_normal.cross(&forearm);
        let _rotation_angle = palm_normal.dot(&forearm).clamp(-1.0, 1.0).acos();
        
        // Update palm history with anatomically aware normal
        let history = self.palm_history.get_mut(side).unwrap();
        history.push_front(palm_normal);
        if history.len() > self.config.history_size {
            history.pop_back();
        }
        
        // Need at least MIN_STABLE_FRAMES for stable detection
        if history.len() < self.config.min_stable_frames {
            return None;
        }

        // Calculate smoothed rotation angle from palm history - MATCHING C++ LOGIC
        let mut cumulative_angle = 0.0;
        let mut cumulative_axis = Vector3::zeros();
        let mut valid_samples = 0;

        for i in 1..history.len() {
            let curr_normal = history[i-1];
            let prev_normal = history[i];
            
            // Calculate rotation angle between consecutive frames
            let angle = curr_normal.dot(&prev_normal).clamp(-1.0, 1.0).acos();
            
            // Only count significant rotations - MATCHING C++
            if angle > self.config.min_rotation_threshold {
                cumulative_angle += angle;
                cumulative_axis += curr_normal.cross(&prev_normal);
                valid_samples += 1;
            }
        }

        // If we don't have enough valid samples, no significant rotation
        if valid_samples < (self.config.min_stable_frames - 1) {
            return None;
        }

        let avg_angle = cumulative_angle / valid_samples as f64;
        let _avg_axis = cumulative_axis.normalize();

        // Apply exponential smoothing to rotation history
        let rotation_history = self.rotation_history.get_mut(side).unwrap();
        rotation_history.push_front(avg_angle);
        if rotation_history.len() > self.config.history_size {
            rotation_history.pop_back();
        }

        // Calculate smoothed rotation with exponential moving average - MATCHING C++
        let mut smoothed_rotation = 0.0;
        let mut weight_sum = 0.0;
        let mut weight = 1.0;

        for rot in rotation_history.iter() {
            smoothed_rotation += rot * weight;
            weight_sum += weight;
            weight *= self.config.rotation_smoothing_factor;
        }
        smoothed_rotation /= weight_sum;

        // Only detect rotation if it's significant
        if smoothed_rotation > self.config.gesture_angle_threshold {
            // Determine rotation direction - MATCHING C++ LOGIC
            let is_supination = if side == "left" {
                // For left arm, positive rotation around forearm axis is supination
                rotation_axis.dot(&Vector3::y()) < 0.0
            } else {
                // For right arm, negative rotation around forearm axis is supination
                rotation_axis.dot(&Vector3::y()) < 0.0
            };
            
            Some(GestureState {
                gesture_type: if is_supination { 
                    GestureType::Supination 
                } else { 
                    GestureType::Pronation 
                },
                confidence: (smoothed_rotation / (self.config.gesture_angle_threshold * 2.0)).min(1.0),
                angle: smoothed_rotation,
            })
        } else {
            None
        }
    }

    fn calculate_palm_normal(&self, landmarks: &[Vector3<f64>]) -> Vector3<f64> {
        // MediaPipe hand landmark indices - matching C++ exactly
        const WRIST: usize = 0;
        const THUMB_CMC: usize = 1;
        const INDEX_MCP: usize = 5;
        const MIDDLE_MCP: usize = 9;
        const RING_MCP: usize = 13;
        const PINKY_MCP: usize = 17;
        const MIDDLE_PIP: usize = 10;
        const MIDDLE_TIP: usize = 12;

        // Get key points
        let wrist = landmarks[WRIST];
        let thumb_cmc = landmarks[THUMB_CMC];
        let index_mcp = landmarks[INDEX_MCP];
        let middle_mcp = landmarks[MIDDLE_MCP];
        let ring_mcp = landmarks[RING_MCP];
        let pinky_mcp = landmarks[PINKY_MCP];
        let middle_pip = landmarks[MIDDLE_PIP];
        let middle_tip = landmarks[MIDDLE_TIP];

        // Calculate robust palm direction vectors
        let palm_center = (index_mcp + middle_mcp + ring_mcp + pinky_mcp) / 4.0;
        let palm_direction = (palm_center - wrist).normalize();
        
        // Calculate palm width vector (perpendicular to thumb-pinky line)
        let thumb_pinky = (pinky_mcp - thumb_cmc).normalize();
        
        // Calculate finger direction (using middle finger as reference)
        let finger_direction = (middle_tip - middle_mcp).normalize();
        
        // Calculate palm normal using multiple reference vectors - THIS IS THE KEY DIFFERENCE
        let normal1 = thumb_pinky.cross(&palm_direction);
        let normal2 = thumb_pinky.cross(&finger_direction);
        
        // Combine normals with weights (equal weighting like C++)
        let weighted_normal = (normal1 + normal2).normalize();
        
        weighted_normal
    }


// In tracking.rs, update the process_frame method around line 500:
pub fn process_frame(&mut self, frame: &DynamicImage) -> Result<TrackingResult> {
    let mut result = TrackingResult::default();
    result.timestamp = self.sim_time;
    self.sim_time += 0.033;
    self.frame_counter += 1;
    
    if let Some(ref mut mp) = self.mediapipe {
        match mp.process_image(frame) {
            Ok(mp_result) => {
                if mp_result.pose_landmarks.len() > 16 {
                    self.process_pose_with_kalman(&mp_result.pose_landmarks, &mut result);
                    
                    for (i, hand_lms) in mp_result.hand_landmarks.iter().enumerate() {
                        self.process_hand_landmarks(hand_lms, i, &mut result);
                    }
                    
                    // Keep gestures from last_valid_gestures if not detected this frame
                    if result.left_gesture.is_none() {
                        if let Some(last_gesture) = self.last_valid_gestures.get("left") {
                            if last_gesture.gesture_type != GestureType::None {
                                result.left_gesture = Some(last_gesture.clone());
                            }
                        }
                    } else if let Some(gesture) = &result.left_gesture {
                        self.last_valid_gestures.insert("left".to_string(), gesture.clone());
                    }
                    
                    if result.right_gesture.is_none() {
                        if let Some(last_gesture) = self.last_valid_gestures.get("right") {
                            if last_gesture.gesture_type != GestureType::None {
                                result.right_gesture = Some(last_gesture.clone());
                            }
                        }
                    } else if let Some(gesture) = &result.right_gesture {
                        self.last_valid_gestures.insert("right".to_string(), gesture.clone());
                    }
                    
                    result.tracking_lost = false;
                }
            }
            Err(e) => {
                eprintln!("MediaPipe error: {}", e);
                result.tracking_lost = true;
            }
        }
    } else {
        self.generate_simulation_data(&mut result);
    }
    
    Ok(result)
}

    fn process_pose_with_kalman(&mut self, landmarks: &[[f64; 3]], result: &mut TrackingResult) {
        const LEFT_SHOULDER: usize = 11;
        const RIGHT_SHOULDER: usize = 12;
        const LEFT_ELBOW: usize = 13;
        const RIGHT_ELBOW: usize = 14;
        const LEFT_WRIST: usize = 15;
        const RIGHT_WRIST: usize = 16;
        
        let joint_indices = [
            ("left_shoulder", LEFT_SHOULDER),
            ("right_shoulder", RIGHT_SHOULDER),
            ("left_elbow", LEFT_ELBOW),
            ("right_elbow", RIGHT_ELBOW),
            ("left_wrist", LEFT_WRIST),
            ("right_wrist", RIGHT_WRIST),
        ];
        
        for (name, idx) in joint_indices.iter() {
            if *idx < landmarks.len() {
                let measurement = Vector3::new(
                    landmarks[*idx][0],
                    landmarks[*idx][1],
                    landmarks[*idx][2],
                );
                
                // Use or create Kalman filter for this joint
                let kalman = self.joint_filters
                    .entry(name.to_string())
                    .or_insert_with(KalmanFilter::new);
                
                kalman.predict();
                kalman.update(measurement);
                
                let smoothed_pos = kalman.position();
                
                result.joints.insert(name.to_string(), JointState {
                    position: smoothed_pos,
                    velocity: Vector3::zeros(), // Could calculate from Kalman state
                    confidence: 0.9,
                    pixel_pos: (
                        (smoothed_pos.x * 640.0) as i32,
                        (smoothed_pos.y * 480.0) as i32
                    ),
                });
            }
        }
    }



}



================================================
FILE: src/ui.rs
================================================
// src/ui.rs - Fixed to use resvg's re-exported tiny_skia
use eframe::egui::{self, Color32, Pos2, Rect, Stroke, Vec2};
use image::DynamicImage;
use usvg::TreeParsing;

#[derive(Debug, Clone)]
pub struct Theme {
    pub primary: Color32,
    pub secondary: Color32,
    pub background: Color32,
    pub surface: Color32,
    pub error: Color32,
    pub warning: Color32,
    pub success: Color32,
    pub text_primary: Color32,
    pub text_secondary: Color32,
}

impl Default for Theme {
    fn default() -> Self {
        Self {
            primary: Color32::from_rgb(70, 130, 240),
            secondary: Color32::from_rgb(255, 152, 0),
            background: Color32::from_rgb(20, 20, 25),
            surface: Color32::from_rgb(30, 30, 35),
            error: Color32::from_rgb(244, 67, 54),
            warning: Color32::from_rgb(255, 152, 0),
            success: Color32::from_rgb(76, 175, 80),
            text_primary: Color32::WHITE,
            text_secondary: Color32::from_rgb(200, 200, 200),
        }
    }
}

pub struct UIComponents {
    pub logo_texture: Option<egui::TextureHandle>,
    pub theme: Theme,
    animations: AnimationState,
}

#[derive(Default)]
struct AnimationState {
    record_pulse: f32,
    gesture_transitions: std::collections::HashMap<String, f32>,
}

impl UIComponents {
    pub fn new(ctx: &egui::Context) -> Self {
        let mut components = Self {
            logo_texture: None,
            theme: Theme::default(),
            animations: AnimationState::default(),
        };
        
        // Try to load SVG logo
        let logo_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/assets/supro.svg";
        if let Ok(logo_rgba) = load_svg_as_rgba(logo_path, 256) {
            let size = [256, 256];
            let color_image = egui::ColorImage::from_rgba_unmultiplied(
                size,
                &logo_rgba,
            );
            
            components.logo_texture = Some(ctx.load_texture(
                "logo",
                color_image,
                Default::default(),
            ));
        }
        
        components
    }
    
    pub fn draw_gesture_indicator(
        &mut self,
        ui: &mut egui::Ui,
        gesture_type: &str,
        confidence: f32,
        angle: f32,
    ) {
        let available_size = ui.available_size();
        let center = Pos2::new(available_size.x / 2.0, available_size.y / 2.0);
        let radius = available_size.x.min(available_size.y) * 0.4;
        
        // Background circle
        let painter = ui.painter();
        painter.circle_filled(center, radius, self.theme.surface);
        
        // Confidence arc
        let color = match gesture_type {
            "supination" => self.theme.success,
            "pronation" => self.theme.warning,
            _ => self.theme.text_secondary,
        };
        
        let arc_angle = confidence * std::f32::consts::PI * 2.0;
        draw_arc(painter, center, radius * 0.9, 0.0, arc_angle, color, 5.0);
        
        // Center text
        painter.text(
            center,
            egui::Align2::CENTER_CENTER,
            gesture_type.to_uppercase(),
            egui::FontId::proportional(24.0),
            self.theme.text_primary,
        );
        
        // Angle indicator
        let angle_text = format!("{:.1}¬∞", angle.to_degrees());
        painter.text(
            Pos2::new(center.x, center.y + radius * 0.5),
            egui::Align2::CENTER_CENTER,
            angle_text,
            egui::FontId::proportional(16.0),
            self.theme.text_secondary,
        );
    }
    
    pub fn draw_joint_skeleton(
        &mut self,
        ui: &mut egui::Ui,
        joints: &[(String, (f32, f32))],
    ) {
        let painter = ui.painter();
        let rect = ui.available_rect_before_wrap();
        
        // Define skeleton connections
        let connections = vec![
            ("left_shoulder", "left_elbow"),
            ("left_elbow", "left_wrist"),
            ("right_shoulder", "right_elbow"),
            ("right_elbow", "right_wrist"),
            ("left_shoulder", "right_shoulder"),
        ];
        
        // Draw connections
        for (from, to) in connections {
            if let (Some(from_joint), Some(to_joint)) = (
                joints.iter().find(|(name, _)| name == from),
                joints.iter().find(|(name, _)| name == to),
            ) {
                let from_pos = Pos2::new(
                    rect.left() + from_joint.1.0 * rect.width(),
                    rect.top() + from_joint.1.1 * rect.height(),
                );
                let to_pos = Pos2::new(
                    rect.left() + to_joint.1.0 * rect.width(),
                    rect.top() + to_joint.1.1 * rect.height(),
                );
                
                painter.line_segment(
                    [from_pos, to_pos],
                    Stroke::new(2.0, self.theme.primary),
                );
            }
        }
        
        // Draw joints
        for (name, (x, y)) in joints {
            let pos = Pos2::new(
                rect.left() + x * rect.width(),
                rect.top() + y * rect.height(),
            );
            
            let color = if name.contains("left") {
                self.theme.primary
            } else {
                self.theme.secondary
            };
            
            painter.circle_filled(pos, 5.0, color);
            painter.circle_stroke(pos, 7.0, Stroke::new(2.0, self.theme.text_primary));
        }
    }
    
    pub fn draw_recording_indicator(&mut self, ui: &mut egui::Ui, is_recording: bool) {
        if !is_recording {
            return;
        }
        
        // Animate pulse effect
        self.animations.record_pulse += ui.input(|i| i.unstable_dt) * 2.0;
        let pulse = (self.animations.record_pulse.sin() + 1.0) * 0.5;
        
        let size = 20.0 + pulse * 5.0;
        let color = Color32::from_rgb(
            244,
            (67.0 + pulse * 30.0) as u8,
            54,
        );
        
        let painter = ui.painter();
        let pos = Pos2::new(ui.available_width() - 30.0, 30.0);
        
        painter.circle_filled(pos, size, color);
        painter.text(
            Pos2::new(pos.x - 50.0, pos.y),
            egui::Align2::RIGHT_CENTER,
            "REC",
            egui::FontId::proportional(14.0),
            color,
        );
    }
    
    pub fn draw_confidence_bar(
        &self,
        ui: &mut egui::Ui,
        label: &str,
        value: f32,
    ) {
        ui.horizontal(|ui| {
            ui.label(label);
            
            let bar_width = 200.0;
            let bar_height = 20.0;
            let rect = ui.allocate_space(Vec2::new(bar_width, bar_height)).1;
            
            let painter = ui.painter();
            
            // Background
            painter.rect_filled(
                rect,
                egui::Rounding::same(4.0),
                self.theme.surface,
            );
            
            // Fill
            let fill_width = bar_width * value;
            let fill_rect = Rect::from_min_size(
                rect.min,
                Vec2::new(fill_width, bar_height),
            );
            
            let color = if value > 0.7 {
                self.theme.success
            } else if value > 0.4 {
                self.theme.warning
            } else {
                self.theme.error
            };
            
            painter.rect_filled(
                fill_rect,
                egui::Rounding::same(4.0),
                color,
            );
            
            // Text
            painter.text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                format!("{:.0}%", value * 100.0),
                egui::FontId::proportional(12.0),
                self.theme.text_primary,
            );
        });
    }
}

fn draw_arc(
    painter: &egui::Painter,
    center: Pos2,
    radius: f32,
    start_angle: f32,
    end_angle: f32,
    color: Color32,
    thickness: f32,
) {
    let points_count = ((end_angle - start_angle).abs() * 50.0) as usize;
    let mut points = Vec::with_capacity(points_count);
    
    for i in 0..=points_count {
        let t = i as f32 / points_count as f32;
        let angle = start_angle + (end_angle - start_angle) * t;
        let x = center.x + radius * angle.cos();
        let y = center.y + radius * angle.sin();
        points.push(Pos2::new(x, y));
    }
    
    for i in 1..points.len() {
        painter.line_segment(
            [points[i - 1], points[i]],
            Stroke::new(thickness, color),
        );
    }
}

fn load_svg_as_rgba(path: &str, size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    let svg_data = std::fs::read_to_string(path)?;
    let opt = usvg::Options::default();
    let tree = usvg::Tree::from_str(&svg_data, &opt)?;
    
    // Use resvg's re-exported tiny_skia types
    let pixmap_size = tree.size.to_int_size();
    let mut pixmap = resvg::tiny_skia::Pixmap::new(size, size).unwrap();
    
    let scale = size as f32 / pixmap_size.width().max(pixmap_size.height()) as f32;
    let transform = resvg::tiny_skia::Transform::from_scale(scale, scale);
    
    // Use the Tree's render method directly with consistent types
    resvg::Tree::from_usvg(&tree).render(transform, &mut pixmap.as_mut());
    
    Ok(pixmap.data().to_vec())
}

fn load_logo_image() -> Result<DynamicImage, image::ImageError> {
    // This is now a fallback
    Ok(DynamicImage::new_rgba8(128, 128))
}

// Custom widget for video display
pub struct VideoWidget {
    texture_id: Option<egui::TextureId>,
    aspect_ratio: f32,
}

impl VideoWidget {
    pub fn new() -> Self {
        Self {
            texture_id: None,
            aspect_ratio: 16.0 / 9.0,
        }
    }
    
    pub fn update_frame(&mut self, ctx: &egui::Context, frame: &DynamicImage) {
        // Convert image to egui texture
        let size = [frame.width() as _, frame.height() as _];
        let rgba = frame.to_rgba8();
        let pixels = rgba.as_flat_samples();
        
        let color_image = egui::ColorImage::from_rgba_unmultiplied(
            size,
            pixels.as_slice(),
        );
        
        self.texture_id = Some(ctx.load_texture(
            "video_frame",
            color_image,
            Default::default(),
        ).id());
    }
    
    pub fn show(&self, ui: &mut egui::Ui) {
        let available_size = ui.available_size();
        let widget_width = available_size.x;
        let widget_height = widget_width / self.aspect_ratio;
        
        let size = Vec2::new(widget_width, widget_height);
        let (rect, _response) = ui.allocate_exact_size(size, egui::Sense::hover());
        
        if let Some(texture_id) = self.texture_id {
            ui.painter().image(
                texture_id,
                rect,
                Rect::from_min_max(Pos2::ZERO, Pos2::new(1.0, 1.0)),
                Color32::WHITE,
            );
        } else {
            ui.painter().rect_filled(
                rect,
                egui::Rounding::same(4.0),
                Color32::from_rgb(50, 50, 55),
            );
            ui.painter().text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                "No Video Signal",
                egui::FontId::proportional(16.0),
                Color32::from_rgb(150, 150, 155),
            );
        }
    }
}


================================================
FILE: src/video.rs
================================================
// src/video.rs - Enhanced with video file processing and overlay capabilities
use std::path::{Path, PathBuf};
use anyhow::{Result, Context};
use image::{DynamicImage, ImageBuffer};
use nokhwa::pixel_format::RgbFormat;
use nokhwa::utils::{CameraIndex, RequestedFormat, RequestedFormatType};
use nokhwa::Camera;
use std::sync::{Arc, Mutex};
use std::process::{Command};
use std::fs;
use std::io::Write;

pub enum VideoSource {
    Camera(Arc<Mutex<Camera>>),
    File(VideoFileReader),
}

pub struct VideoFileReader {
    path: PathBuf,
    current_frame: usize,
    total_frames: usize,
    width: u32,
    height: u32,
    fps: f32,
    frames_cache: Vec<DynamicImage>,
    is_loaded: bool,
}

impl VideoFileReader {
    pub fn new(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref().to_path_buf();
        
        // Get video info using ffprobe
        let output = Command::new("ffprobe")
            .args(&[
                "-v", "error",
                "-select_streams", "v:0",
                "-count_frames",
                "-show_entries", "stream=width,height,r_frame_rate,nb_frames",
                "-of", "csv=p=0",
                path.to_str().unwrap(),
            ])
            .output()
            .context("Failed to run ffprobe. Is FFmpeg installed?")?;
        
        let info = String::from_utf8_lossy(&output.stdout);
        let parts: Vec<&str> = info.trim().split(',').collect();
        
        if parts.len() < 4 {
            return Err(anyhow::anyhow!("Failed to parse video info"));
        }
        
        let width = parts[0].parse().unwrap_or(1280);
        let height = parts[1].parse().unwrap_or(720);
        let fps_str = parts[2];
        let fps = if fps_str.contains('/') {
            let fps_parts: Vec<&str> = fps_str.split('/').collect();
            fps_parts[0].parse::<f32>().unwrap_or(30.0) / fps_parts[1].parse::<f32>().unwrap_or(1.0)
        } else {
            fps_str.parse().unwrap_or(30.0)
        };
        let total_frames = parts[3].parse().unwrap_or(0);
        
        Ok(Self {
            path,
            current_frame: 0,
            total_frames,
            width,
            height,
            fps,
            frames_cache: Vec::new(),
            is_loaded: false,
        })
    }
    
    pub fn load_all_frames(&mut self) -> Result<()> {
        if self.is_loaded {
            return Ok(());
        }
        
        eprintln!("Loading video frames from: {}", self.path.display());
        
        // Extract frames using ffmpeg
        let temp_dir = std::env::temp_dir().join(format!("arm_tracker_{}", uuid::Uuid::new_v4()));
        fs::create_dir_all(&temp_dir)?;
        
        // Extract frames as images
        let status = Command::new("ffmpeg")
            .args(&[
                "-i", self.path.to_str().unwrap(),
                "-vf", "scale=640:480",
                &format!("{}/frame_%04d.png", temp_dir.display()),
            ])
            .status()
            .context("Failed to extract frames with ffmpeg")?;
        
        if !status.success() {
            return Err(anyhow::anyhow!("FFmpeg frame extraction failed"));
        }
        
        // Load extracted frames
        self.frames_cache.clear();
        for i in 1..=self.total_frames {
            let frame_path = temp_dir.join(format!("frame_{:04}.png", i));
            if frame_path.exists() {
                let img = image::open(&frame_path)?;
                self.frames_cache.push(img);
            }
        }
        
        // Clean up temp files
        let _ = fs::remove_dir_all(&temp_dir);
        
        self.is_loaded = true;
        eprintln!("Loaded {} frames", self.frames_cache.len());
        Ok(())
    }
    
    pub fn get_frame(&mut self, index: usize) -> Option<DynamicImage> {
        if !self.is_loaded {
            let _ = self.load_all_frames();
        }
        self.frames_cache.get(index).cloned()
    }
    
    pub fn next_frame(&mut self) -> Option<DynamicImage> {
        let frame = self.get_frame(self.current_frame);
        if frame.is_some() {
            self.current_frame = (self.current_frame + 1) % self.total_frames;
        }
        frame
    }
    
    pub fn seek(&mut self, frame_index: usize) {
        self.current_frame = frame_index.min(self.total_frames - 1);
    }
    
    pub fn get_progress(&self) -> f32 {
        if self.total_frames == 0 {
            0.0
        } else {
            self.current_frame as f32 / self.total_frames as f32
        }
    }
}

#[derive(Debug, Clone)]
pub struct VideoInfo {
    pub path: PathBuf,
    pub fps: f64,
    pub frame_count: i32,
    pub width: i32,
    pub height: i32,
    pub current_frame: i32,
}

impl VideoSource {
    pub fn new_camera(index: i32) -> Result<Self> {
        eprintln!("DEBUG: Attempting to open camera index {}", index);
        
        let camera_index = CameraIndex::Index(index as u32);
        
        use nokhwa::utils::{CameraFormat, FrameFormat, Resolution};
        
        let format = CameraFormat::new(
            Resolution::new(640, 480),
            FrameFormat::MJPEG,
            30,
        );
        
        let requested = RequestedFormat::new::<RgbFormat>(RequestedFormatType::Exact(format));
        
        eprintln!("DEBUG: Creating camera object...");
        let camera = Camera::new(camera_index, requested)
            .map_err(|e| {
                eprintln!("DEBUG: Failed to create camera: {}", e);
                anyhow::anyhow!("Failed to open camera: {}", e)
            })?;
        
        eprintln!("DEBUG: Camera created successfully");
        Ok(VideoSource::Camera(Arc::new(Mutex::new(camera))))
    }
    
    pub fn new_file(path: impl AsRef<Path>) -> Result<Self> {
        let reader = VideoFileReader::new(path)?;
        Ok(VideoSource::File(reader))
    }
    
    pub fn read_frame(&mut self) -> Result<DynamicImage> {
        match self {
            VideoSource::Camera(camera) => {
                let mut cam = camera.lock().unwrap();
                
                if !cam.is_stream_open() {
                    cam.open_stream()
                        .map_err(|e| anyhow::anyhow!("Failed to open camera stream: {}", e))?;
                }
                
                let frame = cam.frame()
                    .map_err(|e| anyhow::anyhow!("Failed to capture frame: {}", e))?;
                
                let decoded = frame.decode_image::<RgbFormat>()
                    .map_err(|e| anyhow::anyhow!("Failed to decode frame: {}", e))?;
                
                let width = decoded.width();
                let height = decoded.height();
                let rgb_data = decoded.into_vec();
                
                let mut rgba_data = Vec::with_capacity((width * height * 4) as usize);
                for chunk in rgb_data.chunks(3) {
                    rgba_data.push(chunk[0]);
                    rgba_data.push(chunk[1]);
                    rgba_data.push(chunk[2]);
                    rgba_data.push(255);
                }
                
                let img = ImageBuffer::from_raw(width, height, rgba_data)
                    .ok_or_else(|| anyhow::anyhow!("Failed to create image buffer"))?;
                
                let flipped = image::imageops::flip_horizontal(&img);
                Ok(DynamicImage::ImageRgba8(flipped))
            }
            VideoSource::File(reader) => {
                reader.next_frame()
                    .ok_or_else(|| anyhow::anyhow!("No more frames in video"))
            }
        }
    }
    
    pub fn get_info(&self) -> Option<VideoInfo> {
        match self {
            VideoSource::Camera(camera) => {
                let cam = camera.lock().unwrap();
                let resolution = cam.resolution();
                Some(VideoInfo {
                    path: PathBuf::from("camera://0"),
                    fps: cam.frame_rate() as f64,
                    frame_count: -1,
                    width: resolution.width() as i32,
                    height: resolution.height() as i32,
                    current_frame: 0,
                })
            }
            VideoSource::File(reader) => Some(VideoInfo {
                path: reader.path.clone(),
                fps: reader.fps as f64,
                frame_count: reader.total_frames as i32,
                width: reader.width as i32,
                height: reader.height as i32,
                current_frame: reader.current_frame as i32,
            }),
        }
    }
    
    pub fn seek(&mut self, frame_number: i32) -> Result<()> {
        if let VideoSource::File(reader) = self {
            reader.seek(frame_number as usize);
        }
        Ok(())
    }
    
    pub fn get_progress(&self) -> f32 {
        match self {
            VideoSource::Camera(_) => 0.0,
            VideoSource::File(reader) => reader.get_progress(),
        }
    }
}

impl Drop for VideoSource {
    fn drop(&mut self) {
        if let VideoSource::Camera(camera) = self {
            if let Ok(mut cam) = camera.lock() {
                let _ = cam.stop_stream();
            }
        }
    }
}

pub struct VideoRecorder {
    output_dir: PathBuf,
    session_id: String,
    fps: f64,
    frame_count: i32,
    frames: Vec<DynamicImage>,
    overlay_frames: Vec<DynamicImage>,
    width: u32,
    height: u32,
}

impl VideoRecorder {
    pub fn new(
        output_dir: impl AsRef<Path>,
        width: u32,
        height: u32,
        fps: f64,
    ) -> Result<Self> {
        let session_id = format!("recording_{}", chrono::Local::now().format("%Y%m%d_%H%M%S"));
        let output_dir = output_dir.as_ref().join(&session_id);
        
        // Create output directory
        std::fs::create_dir_all(&output_dir)?;
        
        Ok(Self {
            output_dir,
            session_id,
            fps,
            frame_count: 0,
            frames: Vec::new(),
            overlay_frames: Vec::new(),
            width,
            height,
        })
    }
    
    pub fn add_frame(&mut self, frame: &DynamicImage, overlay_frame: Option<&DynamicImage>) {
        self.frames.push(frame.clone());
        if let Some(overlay) = overlay_frame {
            self.overlay_frames.push(overlay.clone());
        } else {
            self.overlay_frames.push(frame.clone());
        }
        self.frame_count += 1;
    }
    
    pub fn save_videos(&self) -> Result<(PathBuf, PathBuf)> {
        let raw_video_path = self.output_dir.join("raw_video.mp4");
        let overlay_video_path = self.output_dir.join("overlay_video.mp4");
        
        // Save raw video
        self.save_video_from_frames(&self.frames, &raw_video_path)?;
        
        // Save overlay video
        self.save_video_from_frames(&self.overlay_frames, &overlay_video_path)?;
        
        Ok((raw_video_path, overlay_video_path))
    }
    
    fn save_video_from_frames(&self, frames: &[DynamicImage], output_path: &Path) -> Result<()> {
        // Create temp directory for frames
        let temp_dir = self.output_dir.join("temp_frames");
        std::fs::create_dir_all(&temp_dir)?;
        
        // Save frames as images
        for (i, frame) in frames.iter().enumerate() {
            let frame_path = temp_dir.join(format!("frame_{:05}.png", i));
            frame.save(&frame_path)?;
        }
        
        // Use ffmpeg to create video
        let status = Command::new("ffmpeg")
            .args(&[
                "-y",
                "-r", &self.fps.to_string(),
                "-i", &format!("{}/frame_%05d.png", temp_dir.display()),
                "-c:v", "libx264",
                "-preset", "medium",
                "-crf", "23",
                "-pix_fmt", "yuv420p",
                output_path.to_str().unwrap(),
            ])
            .status()
            .context("Failed to run ffmpeg")?;
        
        // Clean up temp frames
        let _ = std::fs::remove_dir_all(&temp_dir);
        
        if !status.success() {
            return Err(anyhow::anyhow!("FFmpeg video encoding failed"));
        }
        
        Ok(())
    }
    
    pub fn get_output_dir(&self) -> &Path {
        &self.output_dir
    }
}

// Video gallery management
pub struct VideoGallery {
    videos_dir: PathBuf,
    videos: Vec<VideoEntry>,
}

#[derive(Clone)]
pub struct VideoEntry {
    pub path: PathBuf,
    pub thumbnail: Option<DynamicImage>,
    pub name: String,
    pub date: chrono::DateTime<chrono::Local>,
    pub has_overlay: bool,
    pub has_csv: bool,
}

impl VideoGallery {
    pub fn new(videos_dir: impl AsRef<Path>) -> Self {
        Self {
            videos_dir: videos_dir.as_ref().to_path_buf(),
            videos: Vec::new(),
        }
    }
    
    pub fn scan_videos(&mut self) -> Result<()> {
        self.videos.clear();
        
        if !self.videos_dir.exists() {
            std::fs::create_dir_all(&self.videos_dir)?;
        }
        
        // Scan for video directories
        for entry in std::fs::read_dir(&self.videos_dir)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.is_dir() {
                // Check for raw video
                let raw_video = path.join("raw_video.mp4");
                if raw_video.exists() {
                    let overlay_exists = path.join("overlay_video.mp4").exists();
                    let csv_exists = path.join("tracking_data.csv").exists();
                    
                    // Generate thumbnail from first frame
                    let thumbnail = self.extract_thumbnail(&raw_video).ok();
                    
                    let metadata = std::fs::metadata(&raw_video)?;
                    let modified = metadata.modified()?;
                    let datetime = chrono::DateTime::<chrono::Local>::from(modified);
                    
                    self.videos.push(VideoEntry {
                        path: raw_video,
                        thumbnail,
                        name: path.file_name().unwrap().to_string_lossy().to_string(),
                        date: datetime,
                        has_overlay: overlay_exists,
                        has_csv: csv_exists,
                    });
                }
            }
        }
        
        // Sort by date (newest first)
        self.videos.sort_by(|a, b| b.date.cmp(&a.date));
        
        Ok(())
    }
    
    fn extract_thumbnail(&self, video_path: &Path) -> Result<DynamicImage> {
        // Extract first frame as thumbnail
        let temp_thumb = std::env::temp_dir().join("thumb.png");
        
        let status = Command::new("ffmpeg")
            .args(&[
                "-i", video_path.to_str().unwrap(),
                "-vf", "scale=320:240",
                "-vframes", "1",
                "-y",
                temp_thumb.to_str().unwrap(),
            ])
            .status()?;
        
        if !status.success() {
            return Err(anyhow::anyhow!("Failed to extract thumbnail"));
        }
        
        let thumb = image::open(&temp_thumb)?;
        let _ = std::fs::remove_file(&temp_thumb);
        
        Ok(thumb)
    }
    
    pub fn get_videos(&self) -> &[VideoEntry] {
        &self.videos
    }
}

Directory structure:
‚îî‚îÄ‚îÄ juliorodrigo23-supro/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ build.rs
    ‚îú‚îÄ‚îÄ build.sh
    ‚îú‚îÄ‚îÄ Cargo.toml
    ‚îú‚îÄ‚îÄ combine.py
    ‚îú‚îÄ‚îÄ Info.plist
    ‚îú‚îÄ‚îÄ python/
    ‚îÇ   ‚îú‚îÄ‚îÄ mediapipe_processor.py
    ‚îÇ   ‚îú‚îÄ‚îÄ mediapipe_service.py
    ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
    ‚îî‚îÄ‚îÄ src/
        ‚îú‚îÄ‚îÄ data.rs
        ‚îú‚îÄ‚îÄ main.rs
        ‚îú‚îÄ‚îÄ mediapipe_bridge.rs
        ‚îú‚îÄ‚îÄ tracking.rs
        ‚îú‚îÄ‚îÄ ui.rs
        ‚îî‚îÄ‚îÄ video.rs
