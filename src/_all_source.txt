# Combined dump of /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/src
# Excluding: /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/src/bin


================================================================================
FILE: _all_source.txt
================================================================================



================================================================================
FILE: app.rs
================================================================================

// src/app.rs - Enhanced with video upload, gallery, and streamlined UI
use crate::tracking::{ArmTracker, TrackingResult, GestureType};
use crate::ui::{Theme, UIComponents};
use crate::video::{VideoSource, VideoRecorder, VideoGallery, VideoEntry};
use crate::data::DataExporter;

use eframe::egui;
use std::sync::{Arc, Mutex};
use std::path::PathBuf;
use chrono::{DateTime, Local};
use rfd::FileDialog;

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum AppMode {
    Live,
    VideoFile,
    Gallery,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ViewMode {
    SingleCamera,
    DualView,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MediaPipeStatus {
    NotInitialized,
    Initializing,
    Ready,
    Failed,
    SimulationMode,
}

pub struct ArmTrackerApp {
    // Core components
    tracker: Arc<Mutex<ArmTracker>>,
    video_source: Option<VideoSource>,
    recorder: Option<VideoRecorder>,
    data_exporter: Option<DataExporter>,
    mediapipe_status: MediaPipeStatus,
    
    // UI State
    mode: AppMode,
    view_mode: ViewMode,
    theme: Theme,
    show_settings: bool,
    show_about: bool,
    show_save_message: bool,
    save_message_timer: f32,
    
    // Recording state
    is_recording: bool,
    recording_start: Option<DateTime<Local>>,
    recording_duration: std::time::Duration,
    
    // Tracking data
    current_result: TrackingResult,
    tracking_history: Vec<TrackingResult>,
    last_valid_result: Option<TrackingResult>,
    show_overlay: bool,
    
    // Video processing
    selected_video: Option<PathBuf>,
    video_progress: f32,
    is_playing: bool,
    is_processing: bool,
    processing_complete: bool,
    processing_message: String,
    current_video_frame: usize,
    video_playback_speed: f32,
    
    // Gallery
    video_gallery: VideoGallery,
    selected_gallery_video: Option<VideoEntry>,
    
    // UI Components
    ui_components: UIComponents,
    
    // Settings - Simplified to just directories
    settings: AppSettings,
    
    current_frame_texture: Option<egui::TextureHandle>,
    overlay_frame_texture: Option<egui::TextureHandle>,
    
    // Time tracking for frame processing
    sim_time: f64,
    
    #[cfg(target_os = "macos")]
    pub(crate) macos_icon_set: bool,
}

#[derive(Debug, Clone)]
pub struct AppSettings {
    pub working_directory: PathBuf,  // For processing videos
    pub output_directory: PathBuf,   // For saving recordings
}

impl Default for AppSettings {
    fn default() -> Self {
        let base_dir = directories::UserDirs::new()
            .and_then(|dirs| dirs.document_dir().map(|p| p.join("SuproTracker")))
            .unwrap_or_else(|| PathBuf::from("./SuproTracker"));
        
        Self {
            working_directory: base_dir.join("working"),
            output_directory: base_dir.join("recordings"),
        }
    }
}

impl ArmTrackerApp {
    pub fn new(cc: &eframe::CreationContext<'_>) -> Self {
        let tracker = Arc::new(Mutex::new(
            ArmTracker::new().expect("Failed to initialize tracker")
        ));
        
        let settings = AppSettings::default();
        
        // Ensure directories exist
        let _ = std::fs::create_dir_all(&settings.working_directory);
        let _ = std::fs::create_dir_all(&settings.output_directory);
        
        let mut gallery = VideoGallery::new(&settings.output_directory);
        let _ = gallery.scan_videos();

        Self {
            tracker,
            video_source: None,
            recorder: None,
            data_exporter: None,
            mediapipe_status: MediaPipeStatus::NotInitialized,
            mode: AppMode::Live,
            view_mode: ViewMode::DualView,
            theme: Theme::default(),
            show_settings: false,
            show_about: false,
            show_save_message: false,
            save_message_timer: 0.0,
            is_recording: false,
            recording_start: None,
            recording_duration: std::time::Duration::ZERO,
            current_result: TrackingResult::default(),
            tracking_history: Vec::new(),
            last_valid_result: None,
            show_overlay: true,
            selected_video: None,
            video_progress: 0.0,
            is_playing: false,
            is_processing: false,
            processing_complete: false,
            processing_message: String::new(),
            current_video_frame: 0,
            video_playback_speed: 1.0,
            video_gallery: gallery,
            selected_gallery_video: None,
            ui_components: UIComponents::new(&cc.egui_ctx),
            settings,
            current_frame_texture: None,
            overlay_frame_texture: None,
            sim_time: 0.0,
            #[cfg(target_os = "macos")]
            macos_icon_set: false,
        }
    }
    
    fn update_mediapipe_status(&mut self) {
        if let Ok(tracker) = self.tracker.lock() {
            if tracker.is_using_mediapipe() {
                self.mediapipe_status = MediaPipeStatus::Ready;
            } else if tracker.is_initializing() {
                self.mediapipe_status = MediaPipeStatus::Initializing;
            } else if self.video_source.is_none() {
                self.mediapipe_status = MediaPipeStatus::NotInitialized;
            } else {
                self.mediapipe_status = MediaPipeStatus::SimulationMode;
            }
        }
    }
    
    fn render_tracking_status(&self, ui: &mut egui::Ui) {
        ui.horizontal(|ui| {
            let (status_text, color) = match self.mediapipe_status {
                MediaPipeStatus::NotInitialized => ("Not Initialized", egui::Color32::GRAY),
                MediaPipeStatus::Initializing => ("Initializing...", egui::Color32::YELLOW),
                MediaPipeStatus::Ready => ("MediaPipe Ready", egui::Color32::GREEN),
                MediaPipeStatus::Failed => ("Failed (Simulation Mode)", egui::Color32::from_rgb(255, 100, 0)),
                MediaPipeStatus::SimulationMode => ("Simulation Mode", egui::Color32::from_rgb(100, 150, 255)),
            };
            
            let radius = 6.0;
            let rect = ui.allocate_space(egui::vec2(radius * 2.0, radius * 2.0)).1;
            ui.painter().circle_filled(rect.center(), radius, color);
            
            ui.add_space(5.0);
            ui.label(egui::RichText::new(status_text).color(color));
            
            if self.mediapipe_status == MediaPipeStatus::Initializing {
                ui.add(egui::Spinner::new());
            }
        });
    }
    
    fn stop_camera(&mut self) {
        self.video_source = None;
        self.current_frame_texture = None;
        self.current_result = TrackingResult::default();
        self.last_valid_result = None;
        
        if let Ok(mut tracker) = self.tracker.lock() {
            tracker.shutdown_mediapipe();
        }
        
        self.mediapipe_status = MediaPipeStatus::NotInitialized;
        eprintln!("Camera and MediaPipe stopped");
    }
    
    fn start_camera(&mut self) {
        if let Some(src) = self.video_source.as_mut() {
            if let Err(e) = src.read_frame() {
                eprintln!("Camera already open but failed to read frame: {e}");
            } else {
                eprintln!("Camera already running.");
            }
            return;
        }
        
        match VideoSource::new_camera(0) {
            Ok(mut src) => {
                match src.read_frame() {
                    Ok(frame) => {
                        eprintln!("Camera started: {}x{}", frame.width(), frame.height());
                        self.video_source = Some(src);
                        self.mediapipe_status = MediaPipeStatus::Initializing;
                        
                        let tracker = Arc::clone(&self.tracker);
                        std::thread::spawn(move || {
                            std::thread::sleep(std::time::Duration::from_millis(500));
                            eprintln!("Starting MediaPipe initialization...");
                            if let Ok(mut t) = tracker.lock() {
                                t.initialize_mediapipe();
                            }
                        });
                    }
                    Err(e) => {
                        eprintln!("Camera opened but failed to read first frame: {e}");
                    }
                }
            }
            Err(e) => {
                eprintln!("Failed to open camera: {e}");
            }
        }
    }
    
    fn open_video_file(&mut self) {
        if let Some(path) = FileDialog::new()
            .add_filter("Video", &["mp4", "avi", "mov", "mkv"])
            .pick_file() 
        {
            self.selected_video = Some(path.clone());
            self.load_selected_video();
        }
    }
    
    fn load_selected_video(&mut self) {
        if let Some(path) = &self.selected_video {
            match VideoSource::new_file(path) {
                Ok(source) => {
                    self.video_source = Some(source);
                    self.is_playing = true;
                    self.is_processing = true;
                    self.processing_complete = false;
                    self.processing_message = "Initializing video processing...".to_string();
                    self.video_progress = 0.0;
                    
                    // Initialize MediaPipe for video processing
                    if let Ok(mut tracker) = self.tracker.lock() {
                        tracker.initialize_mediapipe();
                    }
                    
                    // Initialize recorder for saving processed video
                    if let Some(info) = self.video_source.as_ref().and_then(|s| s.get_info()) {
                        match VideoRecorder::new(
                            &self.settings.working_directory,
                            info.width as u32,
                            info.height as u32,
                            info.fps,
                        ) {
                            Ok(recorder) => {
                                let output_dir = recorder.get_output_dir().to_path_buf();
                                self.recorder = Some(recorder);
                                
                                // Initialize data exporter
                                self.data_exporter = Some(DataExporter::new(
                                    output_dir,
                                    Some(format!("processed_{}", Local::now().format("%Y%m%d_%H%M%S")))
                                ));
                            }
                            Err(e) => {
                                eprintln!("Failed to initialize recorder: {}", e);
                            }
                        }
                    }
                }
                Err(e) => {
                    eprintln!("Failed to open video file: {}", e);
                    self.processing_message = format!("Error: {}", e);
                    self.is_processing = false;
                    self.processing_complete = false;
                }
            }
        }
    }

    fn get_video_loading_info(&self) -> (f32, String) {
        if let Some(VideoSource::File(reader)) = &self.video_source {
            (reader.get_loading_progress(), reader.get_loading_message().to_string())
        } else {
            (0.0, String::new())
        }
    }
    
    fn save_processed_video(&mut self) {
        if let Some(recorder) = self.recorder.take() {
            self.processing_message = "Saving videos...".to_string();
            
            match recorder.save_videos() {
                Ok((raw_path, overlay_path)) => {
                    // Save CSV data
                    if let Some(exporter) = self.data_exporter.take() {
                        match exporter.export_csv() {
                            Ok(csv_path) => {
                                self.processing_message = format!(
                                    "Saved:\n- Raw: {}\n- Overlay: {}\n- CSV: {}",
                                    raw_path.display(),
                                    overlay_path.display(),
                                    csv_path.display()
                                );
                                self.show_save_message = true;
                                self.save_message_timer = 5.0;
                                
                                // Refresh gallery
                                let _ = self.video_gallery.scan_videos();
                            }
                            Err(e) => {
                                self.processing_message = format!("CSV save error: {}", e);
                            }
                        }
                    }
                }
                Err(e) => {
                    self.processing_message = format!("Video save error: {}", e);
                }
            }
            
            self.processing_complete = true;
        }
    }
    
    fn toggle_recording(&mut self) {
        self.is_recording = !self.is_recording;
        
        if self.is_recording {
            self.recording_start = Some(Local::now());
            
            // Initialize recorder and data exporter
            if let Some(info) = self.video_source.as_ref().and_then(|s| s.get_info()) {
                match VideoRecorder::new(
                    &self.settings.output_directory,
                    info.width as u32,
                    info.height as u32,
                    info.fps,
                ) {
                    Ok(recorder) => {
                        let output_dir = recorder.get_output_dir().to_path_buf();
                        self.recorder = Some(recorder);
                        
                        // Initialize data exporter
                        self.data_exporter = Some(DataExporter::new(
                            output_dir,
                            Some(format!("session_{}", Local::now().format("%Y%m%d_%H%M%S")))
                        ));
                    }
                    Err(e) => {
                        eprintln!("Failed to start recording: {}", e);
                        self.is_recording = false;
                    }
                }
            }
            
            // Ensure MediaPipe is initialized if recording from camera
            if self.mode == AppMode::Live && self.video_source.is_some() {
                if let Ok(mut tracker) = self.tracker.lock() {
                    tracker.initialize_mediapipe();
                }
            }
        } else {
            // Stop recording and save
            self.recording_start = None;
            self.recording_duration = std::time::Duration::ZERO;
            
            if self.recorder.is_some() {
                self.save_processed_video();
            }
        }
    }
    
    fn on_mode_changed(&mut self, _old_mode: AppMode, new_mode: AppMode) {
        match new_mode {
            AppMode::Live => {
                eprintln!("Switched to Live Camera mode");
            }
            AppMode::VideoFile => {
                if self.video_source.is_some() && self.mode == AppMode::Live {
                    self.stop_camera();
                }
                eprintln!("Switched to Video File mode");
            }
            AppMode::Gallery => {
                // Refresh gallery when entering gallery mode
                let _ = self.video_gallery.scan_videos();
                eprintln!("Switched to Gallery mode");
            }
        }
    }
    
    fn render_header(&mut self, ctx: &egui::Context) {
        egui::TopBottomPanel::top("header").show(ctx, |ui| {
            ui.add_space(8.0);
            egui::menu::bar(ui, |ui| {
                ui.horizontal(|ui| {
                    if let Some(logo) = self.ui_components.logo_texture.as_ref() {
                        ui.image((logo.id(), egui::vec2(64.0, 64.0)));
                    }
                    
                    ui.vertical(|ui| {
                        ui.heading("SuPro");
                        ui.add_space(2.0);
                        ui.label(
                            egui::RichText::new("Arm Rotation Tracking System")
                                .italics()
                                .size(14.0)
                                .color(egui::Color32::LIGHT_GRAY),
                        );
                        ui.add_space(2.0);
                        ui.label(
                            egui::RichText::new("By Julio Contreras â€” Under Dr. Ortiz's Research Lab")
                                .size(13.0)
                                .color(egui::Color32::WHITE),
                        );
                    });
                });
                
                ui.separator();
                
                // Mode selection
                ui.horizontal(|ui| {
                    let old_mode = self.mode;
                    
                    ui.selectable_value(&mut self.mode, AppMode::Live, "ðŸŽ¥ Live Camera");
                    ui.selectable_value(&mut self.mode, AppMode::VideoFile, "ðŸ“ Upload Video");
                    ui.selectable_value(&mut self.mode, AppMode::Gallery, "ðŸ–¼ Gallery");
                    
                    if self.mode != old_mode {
                        self.on_mode_changed(old_mode, self.mode);
                    }
                });
                
                ui.separator();
                
                // View mode buttons (only for Live and VideoFile modes)
                if self.mode != AppMode::Gallery {
                    ui.horizontal(|ui| {
                        if ui.selectable_label(self.view_mode == ViewMode::SingleCamera, "Single View").clicked() {
                            self.view_mode = ViewMode::SingleCamera;
                        }
                        if ui.selectable_label(self.view_mode == ViewMode::DualView, "Dual View").clicked() {
                            self.view_mode = ViewMode::DualView;
                        }
                    });
                }
                
                ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                    if ui.button("âš™ Settings").clicked() {
                        self.show_settings = !self.show_settings;
                    }
                    if ui.button("â„¹ About").clicked() {
                        self.show_about = !self.show_about;
                    }
                });
            });
            ui.add_space(6.0);
        });
    }
    
    fn render_main_content(&mut self, ctx: &egui::Context) {
        egui::CentralPanel::default().show(ctx, |ui| {
            match self.mode {
                AppMode::Live => {
                    match self.view_mode {
                        ViewMode::SingleCamera => self.render_single_view_streamlined(ui),
                        ViewMode::DualView => self.render_dual_view_streamlined(ui),
                    }
                }
                AppMode::VideoFile => {
                    self.render_video_file_mode(ui);
                }
                AppMode::Gallery => {
                    self.render_gallery_mode(ui);
                }
            }
            
            // Show save message overlay
            if self.show_save_message {
                egui::Window::new("Save Complete")
                    .collapsible(false)
                    .resizable(false)
                    .anchor(egui::Align2::CENTER_CENTER, [0.0, 0.0])
                    .show(ctx, |ui| {
                        ui.label(&self.processing_message);
                        ui.add_space(10.0);
                        if ui.button("âœ– Close").clicked() {
                            self.show_save_message = false;
                        }
                    });
            }
        });
    }
    
    fn render_single_view_streamlined(&mut self, ui: &mut egui::Ui) {
        ui.columns(2, |columns| {
            // Left column - Video feed
            columns[0].group(|ui| {
                ui.horizontal(|ui| {
                    ui.heading("Camera Feed");
                    ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                        let toggle_text = if self.show_overlay { "Hide Overlay" } else { "Show Overlay" };
                        if ui.button(toggle_text).clicked() {
                            self.show_overlay = !self.show_overlay;
                        }
                    });
                });
                self.render_video_panel(ui, self.show_overlay);
            });

            // Right column - Gesture info only
            columns[1].vertical(|ui| {
                ui.group(|ui| {
                    ui.heading("Gesture Detection");
                    self.render_gesture_panel(ui);
                });
            });
        });
    }
    
    fn render_dual_view_streamlined(&mut self, ui: &mut egui::Ui) {
        // Top row: two video panels side-by-side
        ui.horizontal(|ui| {
            let avail_w = ui.available_width();
            let panel_w = (avail_w - 20.0) / 2.0;
            
            let aspect = 16.0 / 9.0;
            let video_display_h = (panel_w / aspect).clamp(180.0, 360.0);
            
            // Left panel - Raw Feed
            ui.vertical(|ui| {
                ui.set_width(panel_w);
                ui.group(|ui| {
                    ui.heading("Raw Feed");
                    ui.add_space(6.0);
                    self.render_video_panel_sized(ui, panel_w - 20.0, video_display_h, false);
                });
            });
            
            ui.add_space(20.0);
            
            // Right panel - Tracking Overlay
            ui.vertical(|ui| {
                ui.set_width(panel_w);
                ui.group(|ui| {
                    ui.heading("Tracking Overlay");
                    ui.add_space(6.0);
                    self.render_video_panel_sized(ui, panel_w - 20.0, video_display_h, true);
                });
            });
        });
        
        ui.add_space(10.0);
        ui.separator();
        ui.add_space(10.0);
        
        // Bottom: rotation info only
        ui.group(|ui| {
            ui.heading("Arm Rotation");
            ui.add_space(6.0);
            ui.vertical(|ui| {
                self.render_arm_rotation_panel_dynamic(ui, "left");
                ui.add_space(8.0);
                self.render_arm_rotation_panel_dynamic(ui, "right");
            });
        });
    }
    
    fn render_video_file_mode(&mut self, ui: &mut egui::Ui) {
        ui.vertical_centered(|ui| {
            ui.add_space(20.0);
            ui.heading("Video Upload & Processing");
            ui.add_space(20.0);
            
            if self.selected_video.is_none() {
                ui.group(|ui| {
                    ui.add_space(40.0);
                    ui.label("Upload a video to process with MediaPipe tracking");
                    ui.add_space(20.0);
                    
                    let button = egui::Button::new(
                        egui::RichText::new("ðŸ“ Select Video File")
                            .size(20.0)
                            .color(egui::Color32::WHITE)
                    )
                    .fill(egui::Color32::from_rgb(33, 150, 243));
                    
                    if ui.add_sized([200.0, 50.0], button).clicked() {
                        self.open_video_file();
                    }
                    
                    ui.add_space(20.0);
                    ui.label("Supported formats: MP4, AVI, MOV, MKV");
                    ui.add_space(40.0);
                });
            } else if let Some(path) = &self.selected_video {
                ui.label(format!("Processing: {}", path.file_name().unwrap().to_string_lossy()));
                ui.add_space(10.0);
                
                if self.is_processing && !self.processing_complete {
                    // Get loading info from video reader
                    let (load_progress, load_message) = self.get_video_loading_info();

                    ui.horizontal(|ui| {
                        ui.spinner();
                        if !load_message.is_empty() {
                            ui.label(&load_message);
                        } else {
                            ui.label(&self.processing_message);
                        }
                    });

                    ui.add_space(10.0);

                    // Show loading progress if available, otherwise show processing progress
                    let display_progress = if load_progress > 0.0 && load_progress < 1.0 {
                        load_progress
                    } else {
                        self.video_progress
                    };

                    let progress_bar = egui::ProgressBar::new(display_progress)
                        .show_percentage();
                    ui.add(progress_bar);
                    
                    // Display the video being processed
                    match self.view_mode {
                        ViewMode::SingleCamera => {
                            ui.group(|ui| {
                                ui.heading("Processing View");
                                self.render_video_panel(ui, true);
                            });
                        }
                        ViewMode::DualView => {
                            ui.horizontal(|ui| {
                                let avail_w = ui.available_width();
                                let panel_w = (avail_w - 20.0) / 2.0;
                                
                                ui.vertical(|ui| {
                                    ui.set_width(panel_w);
                                    ui.group(|ui| {
                                        ui.heading("Raw");
                                        self.render_video_panel(ui, false);
                                    });
                                });
                                
                                ui.vertical(|ui| {
                                    ui.set_width(panel_w);
                                    ui.group(|ui| {
                                        ui.heading("With Tracking");
                                        self.render_video_panel(ui, true);
                                    });
                                });
                            });
                        }
                    }
                } else if self.processing_complete {
                    ui.colored_label(egui::Color32::from_rgb(76, 175, 80), "âœ“ Processing Complete!");
                    ui.add_space(10.0);
                    ui.label(&self.processing_message);
                    ui.add_space(20.0);

                    // Video playback controls
                    self.render_video_playback_controls(ui);

                    ui.add_space(20.0);

                    if ui.button("â¬… Back to Gallery").clicked() {
                        self.mode = AppMode::Gallery;
                        self.selected_video = None;
                        self.processing_complete = false;
                        self.is_processing = false;
                    }

                    if ui.button("ðŸ“ Process Another Video").clicked() {
                        self.selected_video = None;
                        self.processing_complete = false;
                        self.is_processing = false;
                    }
                }
                
                ui.add_space(20.0);
                
                ui.horizontal(|ui| {
                    if !self.processing_complete {
                        if ui.button("Cancel").clicked() {
                            self.selected_video = None;
                            self.is_processing = false;
                            self.video_source = None;
                        }
                    }
                });
            }
        });
    }
    
    fn render_gallery_mode(&mut self, ui: &mut egui::Ui) {
    ui.heading("Video Gallery");
    ui.add_space(10.0);
    
    if ui.button("ðŸ”„ Refresh").clicked() {
        let _ = self.video_gallery.scan_videos();
    }
    
    ui.separator();
    ui.add_space(10.0);
    
    // Clone the videos to avoid borrow issue
    let videos = self.video_gallery.get_videos().to_vec();
    
    if videos.is_empty() {
        ui.vertical_centered(|ui| {
            ui.add_space(50.0);
            ui.label("No recorded videos yet");
            ui.add_space(20.0);
            ui.label("Videos will appear here after recording");
        });
    } else {
        // Display videos in a grid
        egui::ScrollArea::vertical().show(ui, |ui| {
            let columns = 4;
            let mut current_row = Vec::new();
            
            for (i, video) in videos.iter().enumerate() {
                current_row.push(video.clone());
                
                if current_row.len() == columns || i == videos.len() - 1 {
                    ui.horizontal(|ui| {
                        for video_entry in &current_row {
                            self.render_video_thumbnail(ui, video_entry);
                            ui.add_space(10.0);
                        }
                    });
                    ui.add_space(10.0);
                    current_row.clear();
                }
            }
        });
    }
}
    
    fn render_video_thumbnail(&mut self, ui: &mut egui::Ui, video: &VideoEntry) {
        ui.vertical(|ui| {
            ui.set_width(200.0);

            ui.group(|ui| {
                // Display thumbnail
                let (rect, response) = ui.allocate_exact_size(egui::vec2(200.0, 150.0), egui::Sense::click());

                if let Some(thumbnail) = &video.thumbnail {
                    // Convert thumbnail to texture if needed
                    let size = [thumbnail.width() as usize, thumbnail.height() as usize];
                    let rgba = thumbnail.to_rgba8();
                    let pixels = rgba.as_flat_samples();

                    let color_image = egui::ColorImage::from_rgba_unmultiplied(
                        size,
                        pixels.as_slice(),
                    );

                    let texture = ui.ctx().load_texture(
                        format!("thumb_{}", video.name),
                        color_image,
                        Default::default(),
                    );

                    ui.painter().image(
                        texture.id(),
                        rect,
                        egui::Rect::from_min_max(egui::pos2(0.0, 0.0), egui::pos2(1.0, 1.0)),
                        egui::Color32::WHITE,
                    );
                } else {
                    // Placeholder
                    ui.painter().rect_filled(rect, egui::Rounding::same(4.0), egui::Color32::from_rgb(50, 50, 55));
                    ui.painter().text(
                        rect.center(),
                        egui::Align2::CENTER_CENTER,
                        "ðŸ“¹",
                        egui::FontId::proportional(40.0),
                        egui::Color32::WHITE,
                    );
                }

                if response.clicked() {
                    self.selected_gallery_video = Some(video.clone());
                    self.selected_video = Some(video.path.clone());
                    // Switch to VideoFile mode to view the processed video
                    self.mode = AppMode::VideoFile;
                    self.load_selected_video();
                }
                
                ui.label(&video.name);
                ui.label(
                    egui::RichText::new(video.date.format("%Y-%m-%d %H:%M").to_string())
                        .size(11.0)
                        .color(egui::Color32::GRAY)
                );
                
                ui.horizontal(|ui| {
                    if video.has_overlay {
                        ui.colored_label(egui::Color32::GREEN, "âœ“ Overlay");
                    }
                    if video.has_csv {
                        ui.colored_label(egui::Color32::GREEN, "âœ“ CSV");
                    }
                });
            });
        });
    }
    
    fn render_video_panel(&mut self, ui: &mut egui::Ui, with_overlay: bool) {
        let max_w = ui.available_width();
        let aspect = 16.0 / 9.0;
        let display_w = (max_w - 20.0).max(240.0);
        let display_h = (display_w / aspect).clamp(160.0, 420.0);
        
        let (rect, _resp) = ui.allocate_exact_size(egui::vec2(display_w, display_h), egui::Sense::hover());
        
        ui.painter().rect_filled(rect, egui::Rounding::same(8.0), egui::Color32::from_rgb(28, 28, 34));
        
        if let Some(texture_id) = self.get_current_frame_texture() {
            ui.painter().image(
                texture_id,
                rect,
                egui::Rect::from_min_max(egui::pos2(0.0, 0.0), egui::pos2(1.0, 1.0)),
                egui::Color32::WHITE,
            );
            
            if with_overlay && !self.current_result.tracking_lost {
                self.draw_tracking_overlay(ui, rect);
            }
        } else {
            ui.painter().rect_stroke(rect, egui::Rounding::same(8.0), egui::Stroke::new(1.0, egui::Color32::from_gray(100)));
            ui.painter().text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                "No video feed",
                egui::FontId::proportional(16.0),
                egui::Color32::from_gray(180),
            );
        }
    }
    
    fn render_video_panel_sized(&mut self, ui: &mut egui::Ui, width: f32, height: f32, with_overlay: bool) {
        let (rect, _) = ui.allocate_exact_size(egui::vec2(width, height), egui::Sense::hover());
        
        ui.painter().rect_filled(rect, egui::Rounding::same(8.0), egui::Color32::from_rgb(28, 28, 34));
        
        if let Some(texture_id) = self.get_current_frame_texture() {
            ui.painter().image(
                texture_id,
                rect,
                egui::Rect::from_min_max(egui::pos2(0.0, 0.0), egui::pos2(1.0, 1.0)),
                egui::Color32::WHITE,
            );
            
            if with_overlay && !self.current_result.tracking_lost {
                self.draw_tracking_overlay(ui, rect);
            }
        } else {
            ui.painter().text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                "No video feed",
                egui::FontId::proportional(16.0),
                egui::Color32::from_gray(180),
            );
        }
    }
    
    fn render_gesture_panel(&mut self, ui: &mut egui::Ui) {
        ui.horizontal(|ui| {
            // Left arm gesture
            ui.vertical(|ui| {
                ui.label("Left Arm:");
                let gesture = self.current_result.left_gesture.as_ref()
                    .or(self.last_valid_result.as_ref().and_then(|r| r.left_gesture.as_ref()));
                
                if let Some(gesture) = gesture {
                    let color = match gesture.gesture_type {
                        GestureType::Supination => egui::Color32::from_rgb(76, 175, 80),
                        GestureType::Pronation => egui::Color32::from_rgb(255, 152, 0),
                        GestureType::None => egui::Color32::GRAY,
                    };
                    
                    ui.colored_label(color, format!("{:?}", gesture.gesture_type));
                    ui.label(format!("Confidence: {:.1}%", gesture.confidence * 100.0));
                    ui.label(format!("Angle: {:.1}Â°", gesture.angle.to_degrees()));
                } else {
                    ui.colored_label(egui::Color32::GRAY, "No detection");
                }
            });
            
            ui.separator();
            
            // Right arm gesture
            ui.vertical(|ui| {
                ui.label("Right Arm:");
                let gesture = self.current_result.right_gesture.as_ref()
                    .or(self.last_valid_result.as_ref().and_then(|r| r.right_gesture.as_ref()));
                
                if let Some(gesture) = gesture {
                    let color = match gesture.gesture_type {
                        GestureType::Supination => egui::Color32::from_rgb(76, 175, 80),
                        GestureType::Pronation => egui::Color32::from_rgb(255, 152, 0),
                        GestureType::None => egui::Color32::GRAY,
                    };
                    
                    ui.colored_label(color, format!("{:?}", gesture.gesture_type));
                    ui.label(format!("Confidence: {:.1}%", gesture.confidence * 100.0));
                    ui.label(format!("Angle: {:.1}Â°", gesture.angle.to_degrees()));
                } else {
                    ui.colored_label(egui::Color32::GRAY, "No detection");
                }
            });
        });
    }
    
    fn render_arm_rotation_panel_dynamic(&mut self, ui: &mut egui::Ui, side: &str) {
        let gesture = if side == "left" {
            self.current_result.left_gesture.as_ref()
                .or(self.last_valid_result.as_ref().and_then(|r| r.left_gesture.as_ref()))
        } else {
            self.current_result.right_gesture.as_ref()
                .or(self.last_valid_result.as_ref().and_then(|r| r.right_gesture.as_ref()))
        };
        
        ui.group(|ui| {
            ui.horizontal(|ui| {
                ui.label(
                    egui::RichText::new(format!("{} Arm", if side == "left" { "Left" } else { "Right" }))
                        .size(18.0)
                        .strong()
                );
                
                ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                    if let Some(gesture) = gesture {
                        let (bg_color, text_color, text) = match gesture.gesture_type {
                            GestureType::Supination => (
                                egui::Color32::from_rgb(76, 175, 80), 
                                egui::Color32::WHITE,
                                "Supination"
                            ),
                            GestureType::Pronation => (
                                egui::Color32::from_rgb(255, 152, 0), 
                                egui::Color32::BLACK,
                                "Pronation"
                            ),
                            GestureType::None => (
                                egui::Color32::from_rgb(100, 100, 100), 
                                egui::Color32::WHITE,
                                "None"
                            ),
                        };
                        
                        let badge = egui::Button::new(
                            egui::RichText::new(text)
                                .color(text_color)
                                .size(16.0)
                                .strong()
                        )
                        .fill(bg_color)
                        .sense(egui::Sense::hover());
                        
                        ui.add(badge);
                    }
                });
            });
            
            ui.add_space(8.0);
            
            if let Some(gesture) = gesture {
                ui.horizontal(|ui| {
                    ui.vertical(|ui| {
                        ui.label(
                            egui::RichText::new(format!("Confidence: {:.1}%", gesture.confidence * 100.0))
                                .size(15.0)
                        );
                        ui.label(
                            egui::RichText::new(format!("Rotation Angle: {:.1}Â°", gesture.angle.to_degrees()))
                                .size(15.0)
                        );
                    });
                    
                    ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                        let confidence_color = if gesture.confidence > 0.8 {
                            egui::Color32::from_rgb(76, 175, 80)
                        } else if gesture.confidence > 0.5 {
                            egui::Color32::from_rgb(255, 193, 7)
                        } else {
                            egui::Color32::from_rgb(244, 67, 54)
                        };
                        
                        let bar_width = 100.0;
                        let bar_height = 20.0;
                        let (rect, _response) = ui.allocate_exact_size(
                            egui::vec2(bar_width, bar_height),
                            egui::Sense::hover()
                        );
                        
                        ui.painter().rect_filled(
                            rect,
                            egui::Rounding::same(4.0),
                            egui::Color32::from_gray(60)
                        );
                        
                        let filled_width = bar_width * (gesture.confidence as f32);
                        let filled_rect = egui::Rect::from_min_size(
                            rect.min,
                            egui::vec2(filled_width, bar_height)
                        );
                        ui.painter().rect_filled(
                            filled_rect,
                            egui::Rounding::same(4.0),
                            confidence_color
                        );
                    });
                });
            } else {
                ui.horizontal(|ui| {
                    ui.label(
                        egui::RichText::new("No rotation detected")
                            .size(15.0)
                            .color(egui::Color32::GRAY)
                    );
                });
            }
            
            ui.add_space(4.0);
        });
    }
    
    fn draw_tracking_overlay(&self, ui: &mut egui::Ui, rect: egui::Rect) {
        let painter = ui.painter();
        
        // Draw skeleton connections
        let connections = vec![
            ("left_shoulder", "left_elbow"),
            ("left_elbow", "left_wrist"),
            ("right_shoulder", "right_elbow"),
            ("right_elbow", "right_wrist"),
            ("left_shoulder", "right_shoulder"),
        ];
        
        for (from, to) in connections {
            if let (Some(from_joint), Some(to_joint)) = (
                self.current_result.joints.get(from),
                self.current_result.joints.get(to),
            ) {
                let from_pos = egui::pos2(
                    rect.left() + from_joint.position.x as f32 * rect.width(),
                    rect.top() + from_joint.position.y as f32 * rect.height(),
                );
                let to_pos = egui::pos2(
                    rect.left() + to_joint.position.x as f32 * rect.width(),
                    rect.top() + to_joint.position.y as f32 * rect.height(),
                );
                
                painter.line_segment(
                    [from_pos, to_pos],
                    egui::Stroke::new(3.0, egui::Color32::from_rgb(0, 255, 0)),
                );
            }
        }
        
        // Draw joints
        for (name, joint) in &self.current_result.joints {
            let pos = egui::pos2(
                rect.left() + joint.position.x as f32 * rect.width(),
                rect.top() + joint.position.y as f32 * rect.height(),
            );
            
            let color = if name.contains("left") {
                egui::Color32::from_rgb(255, 0, 0)
            } else {
                egui::Color32::from_rgb(0, 0, 255)
            };
            
            painter.circle_filled(pos, 8.0, color);
            painter.circle_stroke(pos, 10.0, egui::Stroke::new(2.0, egui::Color32::WHITE));
        }
        
        // Draw hand landmarks and connections
        for (side, hand) in &self.current_result.hands {
            if !hand.is_tracked {
                continue;
            }
            
            let hand_color = if side == "left" {
                egui::Color32::from_rgb(255, 100, 100)
            } else {
                egui::Color32::from_rgb(100, 100, 255)
            };
            
            // Draw hand landmarks
            for (i, landmark) in hand.landmarks.iter().enumerate() {
                let pos = egui::pos2(
                    rect.left() + landmark.x as f32 * rect.width(),
                    rect.top() + landmark.y as f32 * rect.height(),
                );
                
                let radius = if i == 0 { 6.0 } else { 3.0 };
                painter.circle_filled(pos, radius, hand_color);
            }
            
            // Draw connections between finger joints
            let finger_connections = [
                // Thumb
                (0, 1), (1, 2), (2, 3), (3, 4),
                // Index
                (0, 5), (5, 6), (6, 7), (7, 8),
                // Middle
                (0, 9), (9, 10), (10, 11), (11, 12),
                // Ring
                (0, 13), (13, 14), (14, 15), (15, 16),
                // Pinky
                (0, 17), (17, 18), (18, 19), (19, 20),
            ];
            
            for (from, to) in finger_connections.iter() {
                if *from < hand.landmarks.len() && *to < hand.landmarks.len() {
                    let from_pos = egui::pos2(
                        rect.left() + hand.landmarks[*from].x as f32 * rect.width(),
                        rect.top() + hand.landmarks[*from].y as f32 * rect.height(),
                    );
                    let to_pos = egui::pos2(
                        rect.left() + hand.landmarks[*to].x as f32 * rect.width(),
                        rect.top() + hand.landmarks[*to].y as f32 * rect.height(),
                    );
                    
                    painter.line_segment(
                        [from_pos, to_pos],
                        egui::Stroke::new(2.0, hand_color.linear_multiply(0.7)),
                    );
                }
            }
        }
    }
    
    fn render_control_panel(&mut self, ctx: &egui::Context) {
        egui::TopBottomPanel::bottom("controls").show(ctx, |ui| {
            ui.add_space(10.0);
            ui.horizontal(|ui| {
                // Camera controls (Live mode only)
                if self.mode == AppMode::Live {
                    if self.video_source.is_some() {
                        let stop_cam = egui::Button::new(
                            egui::RichText::new("â¹ Stop Camera").color(egui::Color32::WHITE)
                        ).fill(egui::Color32::from_rgb(244, 67, 54));
                        if ui.add_sized([140.0, 40.0], stop_cam).clicked() {
                            self.stop_camera();
                        }
                        
                        ui.separator();
                        self.render_tracking_status(ui);
                    } else {
                        let start_cam = egui::Button::new(
                            egui::RichText::new("ðŸ“· Start Camera").color(egui::Color32::WHITE)
                        ).fill(egui::Color32::from_rgb(33, 150, 243));
                        if ui.add_sized([140.0, 40.0], start_cam).clicked() {
                            self.start_camera();
                        }
                    }
                    ui.separator();
                }
                
                // Record controls (only in Live mode)
                if self.mode == AppMode::Live {
                    if self.is_recording {
                        let stop_rec = egui::Button::new(
                            egui::RichText::new("â¹ Stop Recording").color(egui::Color32::WHITE)
                        ).fill(egui::Color32::from_rgb(244, 67, 54));
                        if ui.add_sized([160.0, 40.0], stop_rec).clicked() {
                            self.toggle_recording();
                        }
                    } else {
                        let start_rec = egui::Button::new(
                            egui::RichText::new("âº Record").color(egui::Color32::WHITE)
                        ).fill(egui::Color32::from_rgb(76, 175, 80));
                        if ui.add_sized([140.0, 40.0], start_rec).clicked() {
                            self.toggle_recording();
                        }
                    }
                    ui.separator();
                }
                
                ui.with_layout(egui::Layout::right_to_left(egui::Align::Center), |ui| {
                    if self.is_recording {
                        let duration = self.recording_duration;
                        let minutes = duration.as_secs() / 60;
                        let seconds = duration.as_secs() % 60;
                        ui.label(
                            egui::RichText::new(format!("Recording: {:02}:{:02}", minutes, seconds))
                                .color(egui::Color32::from_rgb(244, 67, 54)),
                        );
                    }
                });
            });
            ui.add_space(10.0);
        });
    }
    
    fn get_current_frame_texture(&self) -> Option<egui::TextureId> {
        self.current_frame_texture.as_ref().map(|t| t.id())
    }
    
    fn render_settings_window(&mut self, ctx: &egui::Context) {
        egui::Window::new("Settings")
            .open(&mut self.show_settings)
            .resizable(true)
            .default_size([500.0, 300.0])
            .show(ctx, |ui| {
                ui.heading("Directory Settings");
                ui.add_space(10.0);
                
                ui.group(|ui| {
                    ui.label("Working Directory (for processing videos):");
                    ui.horizontal(|ui| {
                        ui.label(self.settings.working_directory.display().to_string());
                        if ui.button("Browse...").clicked() {
                            if let Some(path) = FileDialog::new().pick_folder() {
                                self.settings.working_directory = path;
                                let _ = std::fs::create_dir_all(&self.settings.working_directory);
                            }
                        }
                    });
                });
                
                ui.add_space(10.0);
                
                ui.group(|ui| {
                    ui.label("Output Directory (for saving recordings):");
                    ui.horizontal(|ui| {
                        ui.label(self.settings.output_directory.display().to_string());
                        if ui.button("Browse...").clicked() {
                            if let Some(path) = FileDialog::new().pick_folder() {
                                self.settings.output_directory = path.clone();
                                let _ = std::fs::create_dir_all(&self.settings.output_directory);
                                // Update gallery to scan from new directory
                                self.video_gallery = VideoGallery::new(&self.settings.output_directory);
                                let _ = self.video_gallery.scan_videos();
                            }
                        }
                    });
                });

                ui.add_space(10.0);

                if ui.button("Save Settings").clicked() {
                    // Settings are already applied immediately, just show confirmation
                    ui.label("Settings saved!");
                }
            });
    }
    
    fn render_about_window(&mut self, ctx: &egui::Context) {
        egui::Window::new("About")
            .open(&mut self.show_about)
            .resizable(false)
            .default_size([420.0, 320.0])
            .show(ctx, |ui| {
                ui.vertical_centered(|ui| {
                    ui.heading("SuPro");
                    ui.label("Version 1.0.0");
                    ui.add_space(12.0);
                    ui.label("A sophisticated motion tracking application");
                    ui.label("for analyzing arm rotation patterns.");
                    ui.add_space(16.0);
                    ui.hyperlink("https://github.com/Juliorodrigo23/Supro");
                });
            });
    }

    fn render_video_playback_controls(&mut self, ui: &mut egui::Ui) {
        ui.group(|ui| {
            ui.heading("Video Playback");
            ui.add_space(10.0);

            // Get total frames if available
            let total_frames = if let Some(VideoSource::File(reader)) = &self.video_source {
                reader.get_total_frames()
            } else {
                0
            };

            if total_frames > 0 {
                // Frame scrubber
                ui.horizontal(|ui| {
                    ui.label("Frame:");

                    let mut frame_f32 = self.current_video_frame as f32;
                    let slider = egui::Slider::new(&mut frame_f32, 0.0..=(total_frames - 1) as f32)
                        .show_value(false);

                    if ui.add(slider).changed() {
                        self.current_video_frame = frame_f32 as usize;
                        // Seek to the new frame
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(self.current_video_frame);
                        }
                    }

                    ui.label(format!("{} / {}", self.current_video_frame + 1, total_frames));
                });

                ui.add_space(10.0);

                // Playback controls
                ui.horizontal(|ui| {
                    // Previous frame
                    if ui.button("â® Prev").clicked() && self.current_video_frame > 0 {
                        self.current_video_frame -= 1;
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(self.current_video_frame);
                        }
                    }

                    // Play/Pause
                    let play_pause_text = if self.is_playing { "â¸ Pause" } else { "â–¶ Play" };
                    if ui.button(play_pause_text).clicked() {
                        self.is_playing = !self.is_playing;
                    }

                    // Next frame
                    if ui.button("Next â­").clicked() && self.current_video_frame < total_frames - 1 {
                        self.current_video_frame += 1;
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(self.current_video_frame);
                        }
                    }

                    ui.separator();

                    // Speed control
                    ui.label("Speed:");
                    ui.add(egui::Slider::new(&mut self.video_playback_speed, 0.25..=2.0)
                        .text("x")
                        .suffix("x"));
                });

                ui.add_space(5.0);

                // Quick seek buttons
                ui.horizontal(|ui| {
                    ui.label("Quick Seek:");

                    if ui.button("-10 frames").clicked() {
                        self.current_video_frame = self.current_video_frame.saturating_sub(10);
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(self.current_video_frame);
                        }
                    }

                    if ui.button("+10 frames").clicked() {
                        self.current_video_frame = (self.current_video_frame + 10).min(total_frames - 1);
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(self.current_video_frame);
                        }
                    }

                    if ui.button("Reset to Start").clicked() {
                        self.current_video_frame = 0;
                        if let Some(VideoSource::File(reader)) = &mut self.video_source {
                            reader.seek(0);
                        }
                    }
                });
            } else {
                ui.label("No video loaded");
            }
        });
    }
}

impl eframe::App for ArmTrackerApp {
    fn update(&mut self, ctx: &egui::Context, _frame: &mut eframe::Frame) {
        #[cfg(target_os = "macos")]
        if !self.macos_icon_set {
            crate::set_macos_dock_icon_from_bundle();
            self.macos_icon_set = true;
        }
        
        // Handle save message timer
        if self.show_save_message {
            self.save_message_timer -= ctx.input(|i| i.unstable_dt);
            if self.save_message_timer <= 0.0 {
                self.show_save_message = false;
            }
        }
        
        // Update MediaPipe status
        self.update_mediapipe_status();
        
        // Update recording duration if recording
        if self.is_recording {
            if let Some(start) = self.recording_start {
                self.recording_duration = Local::now()
                    .signed_duration_since(start)
                    .to_std()
                    .unwrap_or_default();
            }
        }
        
        // Process video frames
        if let Some(video_source) = self.video_source.as_mut() {
            match video_source.read_frame() {
                Ok(frame) => {
                    // Create overlay frame with tracking
                    let overlay_frame = frame.clone();
                    
                    // Process with tracker
                    if let Ok(mut tracker) = self.tracker.lock() {
                        match tracker.process_frame(&frame) {
                            Ok(tracking_result) => {
                                self.current_result = tracking_result.clone();
                                
                                if tracking_result.left_gesture.is_some() || tracking_result.right_gesture.is_some() {
                                    self.last_valid_result = Some(tracking_result.clone());
                                }
                                
                                self.tracking_history.push(tracking_result.clone());
                                
                                if self.tracking_history.len() > 1000 {
                                    self.tracking_history.remove(0);
                                }
                                
                                // Update progress for video files
                                if self.mode == AppMode::VideoFile {
                                    self.video_progress = video_source.get_progress();
                                }
                                
                                // Add to data exporter if recording
                                if self.is_recording {
                                    if let Some(exporter) = &mut self.data_exporter {
                                        exporter.add_frame(tracking_result.clone(), self.sim_time);
                                    }
                                    
                                    if let Some(recorder) = &mut self.recorder {
                                        recorder.add_frame(&frame, Some(&overlay_frame));
                                    }
                                }
                            }
                            Err(e) => {
                                eprintln!("Tracking error: {}", e);
                            }
                        }
                    }
                    
                    // Update texture
                    let size = [frame.width() as usize, frame.height() as usize];
                    let rgba = frame.to_rgba8();
                    let pixels = rgba.as_flat_samples();
                    
                    let color_image = egui::ColorImage::from_rgba_unmultiplied(
                        size,
                        pixels.as_slice(),
                    );
                    
                    if let Some(texture) = &mut self.current_frame_texture {
                        texture.set(color_image, Default::default());
                    } else {
                        self.current_frame_texture = Some(ctx.load_texture(
                            "video_frame",
                            color_image,
                            Default::default(),
                        ));
                    }
                    
                    // Check if video processing is complete
                    if self.mode == AppMode::VideoFile && self.video_progress >= 0.99 {
                        self.processing_complete = true;
                        self.is_processing = false;
                        if self.recorder.is_some() {
                            self.save_processed_video();
                        }
                    }
                }
                Err(_) => {
                    // End of video or error
                    if self.mode == AppMode::VideoFile {
                        self.processing_complete = true;
                        self.is_processing = false;
                        if self.recorder.is_some() {
                            self.save_processed_video();
                        }
                    }
                }
            }
        }
        
        // Update time
        self.sim_time += ctx.input(|i| i.unstable_dt) as f64;
        
        // Render UI components
        self.render_header(ctx);
        self.render_control_panel(ctx);
        
        if self.show_settings {
            self.render_settings_window(ctx);
        }
        
        if self.show_about {
            self.render_about_window(ctx);
        }
        
        self.render_main_content(ctx);
        
        ctx.request_repaint();
    }
}

================================================================================
FILE: data.rs
================================================================================

// src/data.rs
use crate::tracking::{TrackingResult, GestureType};
use csv::Writer;
use std::path::{Path, PathBuf};
use std::fs::File;
use anyhow::Result;
use chrono::Local;
use serde::Serialize;
use nalgebra::Vector3;

#[derive(Debug, Serialize)]
struct TrackingRecord {
    timestamp: f64,
    frame: i32,
    tracking_lost: bool,
    
    // Joint positions
    left_shoulder_x: Option<f64>,
    left_shoulder_y: Option<f64>,
    left_shoulder_z: Option<f64>,
    left_shoulder_confidence: Option<f64>,
    
    right_shoulder_x: Option<f64>,
    right_shoulder_y: Option<f64>,
    right_shoulder_z: Option<f64>,
    right_shoulder_confidence: Option<f64>,
    
    left_elbow_x: Option<f64>,
    left_elbow_y: Option<f64>,
    left_elbow_z: Option<f64>,
    left_elbow_confidence: Option<f64>,
    
    right_elbow_x: Option<f64>,
    right_elbow_y: Option<f64>,
    right_elbow_z: Option<f64>,
    right_elbow_confidence: Option<f64>,
    
    left_wrist_x: Option<f64>,
    left_wrist_y: Option<f64>,
    left_wrist_z: Option<f64>,
    left_wrist_confidence: Option<f64>,
    
    right_wrist_x: Option<f64>,
    right_wrist_y: Option<f64>,
    right_wrist_z: Option<f64>,
    right_wrist_confidence: Option<f64>,
    
    // Gestures
    left_gesture: Option<String>,
    left_gesture_confidence: Option<f64>,
    left_gesture_angle: Option<f64>,
    
    right_gesture: Option<String>,
    right_gesture_confidence: Option<f64>,
    right_gesture_angle: Option<f64>,

    // Hand landmarks - Finger angles
    // Left hand
    left_thumb_angle: Option<f64>,
    left_index_angle: Option<f64>,
    left_middle_angle: Option<f64>,
    left_ring_angle: Option<f64>,
    left_pinky_angle: Option<f64>,
    left_wrist_flexion: Option<f64>,

    // Right hand
    right_thumb_angle: Option<f64>,
    right_index_angle: Option<f64>,
    right_middle_angle: Option<f64>,
    right_ring_angle: Option<f64>,
    right_pinky_angle: Option<f64>,
    right_wrist_flexion: Option<f64>,
}

pub struct DataExporter {
    output_dir: PathBuf,
    session_name: String,
    tracking_data: Vec<TrackingResult>,
    timestamps: Vec<f64>,
}

impl DataExporter {
    pub fn new(output_dir: impl AsRef<Path>, session_name: Option<String>) -> Self {
        let session_name = session_name.unwrap_or_else(|| {
            format!("session_{}", Local::now().format("%Y%m%d_%H%M%S"))
        });
        
        Self {
            output_dir: output_dir.as_ref().to_path_buf(),
            session_name,
            tracking_data: Vec::new(),
            timestamps: Vec::new(),
        }
    }
    
    pub fn add_frame(&mut self, result: TrackingResult, timestamp: f64) {
        self.tracking_data.push(result);
        self.timestamps.push(timestamp);
    }
    
    pub fn export_csv(&self) -> Result<PathBuf> {
        let csv_path = self.output_dir
            .join(&self.session_name)
            .join("tracking_data.csv");
        
        // Create directory if it doesn't exist
        if let Some(parent) = csv_path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let file = File::create(&csv_path)?;
        let mut writer = Writer::from_writer(file);
        
        for (i, (result, timestamp)) in self.tracking_data.iter()
            .zip(self.timestamps.iter())
            .enumerate() 
        {
            let record = self.create_record(i as i32, *timestamp, result);
            writer.serialize(record)?;
        }
        
        writer.flush()?;
        Ok(csv_path)
    }
    
    fn create_record(&self, frame: i32, timestamp: f64, result: &TrackingResult) -> TrackingRecord {
        let mut record = TrackingRecord {
            timestamp,
            frame,
            tracking_lost: result.tracking_lost,
            left_shoulder_x: None,
            left_shoulder_y: None,
            left_shoulder_z: None,
            left_shoulder_confidence: None,
            right_shoulder_x: None,
            right_shoulder_y: None,
            right_shoulder_z: None,
            right_shoulder_confidence: None,
            left_elbow_x: None,
            left_elbow_y: None,
            left_elbow_z: None,
            left_elbow_confidence: None,
            right_elbow_x: None,
            right_elbow_y: None,
            right_elbow_z: None,
            right_elbow_confidence: None,
            left_wrist_x: None,
            left_wrist_y: None,
            left_wrist_z: None,
            left_wrist_confidence: None,
            right_wrist_x: None,
            right_wrist_y: None,
            right_wrist_z: None,
            right_wrist_confidence: None,
            left_gesture: None,
            left_gesture_confidence: None,
            left_gesture_angle: None,
            right_gesture: None,
            right_gesture_confidence: None,
            right_gesture_angle: None,
            left_thumb_angle: None,
            left_index_angle: None,
            left_middle_angle: None,
            left_ring_angle: None,
            left_pinky_angle: None,
            left_wrist_flexion: None,
            right_thumb_angle: None,
            right_index_angle: None,
            right_middle_angle: None,
            right_ring_angle: None,
            right_pinky_angle: None,
            right_wrist_flexion: None,
        };
        
        // Fill in joint data
        for (name, joint) in &result.joints {
            match name.as_str() {
                "left_shoulder" => {
                    record.left_shoulder_x = Some(joint.position.x);
                    record.left_shoulder_y = Some(joint.position.y);
                    record.left_shoulder_z = Some(joint.position.z);
                    record.left_shoulder_confidence = Some(joint.confidence);
                }
                "right_shoulder" => {
                    record.right_shoulder_x = Some(joint.position.x);
                    record.right_shoulder_y = Some(joint.position.y);
                    record.right_shoulder_z = Some(joint.position.z);
                    record.right_shoulder_confidence = Some(joint.confidence);
                }
                "left_elbow" => {
                    record.left_elbow_x = Some(joint.position.x);
                    record.left_elbow_y = Some(joint.position.y);
                    record.left_elbow_z = Some(joint.position.z);
                    record.left_elbow_confidence = Some(joint.confidence);
                }
                "right_elbow" => {
                    record.right_elbow_x = Some(joint.position.x);
                    record.right_elbow_y = Some(joint.position.y);
                    record.right_elbow_z = Some(joint.position.z);
                    record.right_elbow_confidence = Some(joint.confidence);
                }
                "left_wrist" => {
                    record.left_wrist_x = Some(joint.position.x);
                    record.left_wrist_y = Some(joint.position.y);
                    record.left_wrist_z = Some(joint.position.z);
                    record.left_wrist_confidence = Some(joint.confidence);
                }
                "right_wrist" => {
                    record.right_wrist_x = Some(joint.position.x);
                    record.right_wrist_y = Some(joint.position.y);
                    record.right_wrist_z = Some(joint.position.z);
                    record.right_wrist_confidence = Some(joint.confidence);
                }
                _ => {}
            }
        }
        
        // Fill in gesture data
        if let Some(left_gesture) = &result.left_gesture {
            record.left_gesture = Some(format!("{:?}", left_gesture.gesture_type));
            record.left_gesture_confidence = Some(left_gesture.confidence);
            record.left_gesture_angle = Some(left_gesture.angle);
        }
        
        if let Some(right_gesture) = &result.right_gesture {
            record.right_gesture = Some(format!("{:?}", right_gesture.gesture_type));
            record.right_gesture_confidence = Some(right_gesture.confidence);
            record.right_gesture_angle = Some(right_gesture.angle);
        }

        // Calculate finger angles for left hand
        if let Some(left_hand) = result.hands.get("left") {
            if left_hand.is_tracked && left_hand.landmarks.len() >= 21 {
                record.left_thumb_angle = Some(Self::calculate_finger_angle(&left_hand.landmarks, 1, 2, 3, 4));
                record.left_index_angle = Some(Self::calculate_finger_angle(&left_hand.landmarks, 5, 6, 7, 8));
                record.left_middle_angle = Some(Self::calculate_finger_angle(&left_hand.landmarks, 9, 10, 11, 12));
                record.left_ring_angle = Some(Self::calculate_finger_angle(&left_hand.landmarks, 13, 14, 15, 16));
                record.left_pinky_angle = Some(Self::calculate_finger_angle(&left_hand.landmarks, 17, 18, 19, 20));
                record.left_wrist_flexion = Some(Self::calculate_wrist_angle(&left_hand.landmarks));
            }
        }

        // Calculate finger angles for right hand
        if let Some(right_hand) = result.hands.get("right") {
            if right_hand.is_tracked && right_hand.landmarks.len() >= 21 {
                record.right_thumb_angle = Some(Self::calculate_finger_angle(&right_hand.landmarks, 1, 2, 3, 4));
                record.right_index_angle = Some(Self::calculate_finger_angle(&right_hand.landmarks, 5, 6, 7, 8));
                record.right_middle_angle = Some(Self::calculate_finger_angle(&right_hand.landmarks, 9, 10, 11, 12));
                record.right_ring_angle = Some(Self::calculate_finger_angle(&right_hand.landmarks, 13, 14, 15, 16));
                record.right_pinky_angle = Some(Self::calculate_finger_angle(&right_hand.landmarks, 17, 18, 19, 20));
                record.right_wrist_flexion = Some(Self::calculate_wrist_angle(&right_hand.landmarks));
            }
        }

        record
    }

    // Calculate finger angle based on landmarks (MCP, PIP, DIP, TIP)
    fn calculate_finger_angle(landmarks: &[Vector3<f64>], mcp: usize, pip: usize, dip: usize, tip: usize) -> f64 {
        if landmarks.len() <= tip {
            return 0.0;
        }

        // Calculate vectors
        let v1 = landmarks[pip] - landmarks[mcp];
        let v2 = landmarks[dip] - landmarks[pip];
        let v3 = landmarks[tip] - landmarks[dip];

        // Calculate angles between consecutive segments
        let angle1 = Self::angle_between_vectors(&v1, &v2);
        let angle2 = Self::angle_between_vectors(&v2, &v3);

        // Return average angle (in degrees)
        ((angle1 + angle2) / 2.0).to_degrees()
    }

    // Calculate wrist flexion angle
    fn calculate_wrist_angle(landmarks: &[Vector3<f64>]) -> f64 {
        if landmarks.len() < 21 {
            return 0.0;
        }

        // Use wrist (0), middle finger MCP (9), and middle finger tip (12)
        let wrist = landmarks[0];
        let mcp = landmarks[9];
        let tip = landmarks[12];

        let v1 = mcp - wrist;
        let v2 = tip - mcp;

        Self::angle_between_vectors(&v1, &v2).to_degrees()
    }

    // Helper function to calculate angle between two vectors
    fn angle_between_vectors(v1: &Vector3<f64>, v2: &Vector3<f64>) -> f64 {
        let dot = v1.dot(v2);
        let mag1 = v1.norm();
        let mag2 = v2.norm();

        if mag1 == 0.0 || mag2 == 0.0 {
            return 0.0;
        }

        let cos_angle = (dot / (mag1 * mag2)).clamp(-1.0, 1.0);
        cos_angle.acos()
    }
    
    pub fn generate_report(&self) -> Result<PathBuf> {
        let report_path = self.output_dir
            .join(&self.session_name)
            .join("report.html");
        
        // Create directory if it doesn't exist
        if let Some(parent) = report_path.parent() {
            std::fs::create_dir_all(parent)?;
        }
        
        let html_content = self.create_html_report()?;
        std::fs::write(&report_path, html_content)?;
        
        Ok(report_path)
    }
    
    fn create_html_report(&self) -> Result<String> {
        let total_frames = self.tracking_data.len();
        let tracking_lost_count = self.tracking_data.iter()
            .filter(|r| r.tracking_lost)
            .count();
        
        let left_supination_count = self.tracking_data.iter()
            .filter(|r| r.left_gesture.as_ref()
                .map(|g| g.gesture_type == GestureType::Supination)
                .unwrap_or(false))
            .count();
        
        let left_pronation_count = self.tracking_data.iter()
            .filter(|r| r.left_gesture.as_ref()
                .map(|g| g.gesture_type == GestureType::Pronation)
                .unwrap_or(false))
            .count();
        
        let html = format!(r#"
<!DOCTYPE html>
<html>
<head>
    <title>Arm Tracking Report - {}</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background: #f5f5f5; }}
        h1 {{ color: #333; }}
        .stats {{ background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}
        .stat-item {{ margin: 10px 0; }}
        .stat-label {{ font-weight: bold; color: #666; }}
        .stat-value {{ color: #4682EA; font-size: 1.2em; }}
    </style>
</head>
<body>
    <h1>Arm Tracking Session Report</h1>
    <div class="stats">
        <h2>Session: {}</h2>
        <div class="stat-item">
            <span class="stat-label">Total Frames:</span>
            <span class="stat-value">{}</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Tracking Success Rate:</span>
            <span class="stat-value">{:.1}%</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Left Arm Supination:</span>
            <span class="stat-value">{} frames</span>
        </div>
        <div class="stat-item">
            <span class="stat-label">Left Arm Pronation:</span>
            <span class="stat-value">{} frames</span>
        </div>
    </div>
</body>
</html>
        "#,
            self.session_name,
            self.session_name,
            total_frames,
            (1.0 - tracking_lost_count as f64 / total_frames as f64) * 100.0,
            left_supination_count,
            left_pronation_count
        );
        
        Ok(html)
    }
}

================================================================================
FILE: main.rs
================================================================================

// src/main.rs
mod app;
mod tracking;
mod ui;
mod video;
mod data;
mod mediapipe_bridge;

use eframe::egui;
use usvg::TreeParsing;

#[cfg(target_os = "macos")]
pub(crate) fn set_macos_dock_icon_from_bundle() {
    use cocoa::{
        appkit::{NSApp, NSImage},
        base::{id, nil},
        foundation::NSString,
    };
    use objc::{class, msg_send, sel, sel_impl};

    unsafe {
        // NSBundle *bundle = [NSBundle mainBundle];
        let bundle: id = msg_send![class!(NSBundle), mainBundle];

        // NSString *name = @"AppIcon"; NSString *typ = @"icns";
        let name = NSString::alloc(nil).init_str("AppIcon");
        let typ = NSString::alloc(nil).init_str("icns");

        // NSString *path = [bundle pathForResource:name ofType:typ];
        let path: id = msg_send![bundle, pathForResource: name ofType: typ];
        if path != nil {
            // NSImage *img = [[NSImage alloc] initWithContentsOfFile:path];
            let img: id = msg_send![NSImage::alloc(nil), initWithContentsOfFile: path];
            if img != nil {
                // [NSApp setApplicationIconImage:img];
                let app = NSApp();
                let _: () = msg_send![app, setApplicationIconImage: img];
            }
        }
    }
}

#[cfg(not(target_os = "macos"))]
fn set_macos_dock_icon_from_bundle() {
    // no-op on non-macOS
}

fn main() {
    // Initialize logging
    tracing_subscriber::fmt::init();
    set_macos_dock_icon_from_bundle();

    if let Ok(p) = std::env::current_exe() {
        eprintln!("Running from: {}", p.display());
    }

    // DEBUG: List available cameras
    println!("=== Camera Detection Debug ===");
    match nokhwa::query(nokhwa::utils::ApiBackend::Auto) {
        Ok(cameras) => {
            println!("Found {} camera(s):", cameras.len());
            for (i, camera) in cameras.iter().enumerate() {
                println!("  [{}] {}", i, camera.human_name());
            }
        }
        Err(e) => {
            println!("Failed to query cameras: {}", e);
        }
    }
    println!("============================\n");

    // Set up GUI options
    let options = eframe::NativeOptions {
        viewport: egui::ViewportBuilder::default()
            .with_inner_size([1400.0, 900.0])
            .with_min_inner_size([1200.0, 800.0]),
        centered: true,
        ..Default::default()
    };

    // Run the application
    let result = eframe::run_native(
        "SuPro",
        options,
        Box::new(|cc| {
            // Configure fonts and visuals
            configure_fonts(&cc.egui_ctx);
            cc.egui_ctx.set_visuals(create_visuals());

            Box::new(app::ArmTrackerApp::new(cc))
        }),
    );

    if let Err(e) = result {
        eprintln!("Error running application: {:?}", e);
    }
}



fn load_svg_as_rgba(path: &str, size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    let svg_data = std::fs::read_to_string(path)?;
    let opt = usvg::Options::default();
    let tree = usvg::Tree::from_str(&svg_data, &opt)?;
    
    // Use resvg's re-exported tiny_skia types
    let pixmap_size = tree.size.to_int_size();
    let mut pixmap = resvg::tiny_skia::Pixmap::new(size, size).unwrap();
    
    let scale = size as f32 / pixmap_size.width().max(pixmap_size.height()) as f32;
    let transform = resvg::tiny_skia::Transform::from_scale(scale, scale);
    
    // Use the Tree's render method directly with consistent types
    resvg::Tree::from_usvg(&tree).render(transform, &mut pixmap.as_mut());
    
    Ok(pixmap.data().to_vec())
}

fn configure_fonts(ctx: &egui::Context) {
    let mut fonts = egui::FontDefinitions::default();
    
    // Load Montserrat font
    let font_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/fonts/Montserrat-VariableFont_wght.ttf";
    if let Ok(font_data) = std::fs::read(font_path) {
        fonts.font_data.insert(
            "Montserrat".to_owned(),
            egui::FontData::from_owned(font_data),
        );
        
        // Set Montserrat as the primary font
        fonts.families.entry(egui::FontFamily::Proportional)
            .or_default()
            .insert(0, "Montserrat".to_owned());
            
        fonts.families.entry(egui::FontFamily::Monospace)
            .or_default()
            .push("Montserrat".to_owned());
    }
    
    ctx.set_fonts(fonts);
}

fn create_visuals() -> egui::Visuals {
    let mut visuals = egui::Visuals::dark();
    
    // Customize colors for a modern, professional look
    visuals.widgets.noninteractive.bg_fill = egui::Color32::from_rgb(30, 30, 35);
    visuals.widgets.inactive.bg_fill = egui::Color32::from_rgb(45, 45, 52);
    visuals.widgets.hovered.bg_fill = egui::Color32::from_rgb(55, 55, 65);
    visuals.widgets.active.bg_fill = egui::Color32::from_rgb(70, 130, 240);
    
    // Adjust rounding for modern appearance
    visuals.widgets.noninteractive.rounding = egui::Rounding::same(8.0);
    visuals.widgets.inactive.rounding = egui::Rounding::same(8.0);
    visuals.widgets.hovered.rounding = egui::Rounding::same(8.0);
    visuals.widgets.active.rounding = egui::Rounding::same(8.0);
    
    visuals.window_rounding = egui::Rounding::same(12.0);
    visuals.menu_rounding = egui::Rounding::same(8.0);
    
    visuals
}

================================================================================
FILE: mediapipe_bridge.rs
================================================================================

// src/mediapipe_bridge.rs
use anyhow::{Result, Context};
use nalgebra::Vector3;
use std::process::{Command, Stdio, Child};
use std::io::{Write, BufRead, BufReader};
use serde::{Deserialize, Serialize};
use image::DynamicImage;
use std::time::{Duration, Instant};

#[derive(Debug, Serialize, Deserialize)]
struct MediaPipeFrame {
    width: u32,
    height: u32,
    data: Vec<u8>,
}

#[derive(Debug, Deserialize)]
pub struct MediaPipeResult {
    pub pose_landmarks: Vec<[f64; 3]>,    // Make public
    pub hand_landmarks: Vec<Vec<[f64; 3]>>, // Make public
}

pub struct MediaPipeWrapper {
    python_process: Child,
    stdin: std::process::ChildStdin,
    stdout: BufReader<std::process::ChildStdout>,
}

impl MediaPipeWrapper {
    pub fn new() -> Result<Self> {
        eprintln!("=== MediaPipe Initialization ===");
        
        // Try multiple paths to find the Python script
        let possible_paths = vec![
            // For bundled app
            std::env::current_exe()
                .ok()
                .and_then(|exe| exe.parent().map(|p| p.join("../Resources/python/mediapipe_service.py"))),
            // For development
            std::env::current_dir().ok().map(|d| d.join("python/mediapipe_service.py")),
        ];
        
        let script_path = possible_paths
            .into_iter()
            .flatten()
            .find(|p| p.exists())
            .ok_or_else(|| anyhow::anyhow!("Could not find mediapipe_service.py"))?;
        
        eprintln!("Found Python script at: {}", script_path.display());
        
        let mut child = Command::new("python3")
            .arg(&script_path)
            .stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::inherit())
            .spawn()
            .context("Failed to spawn Python MediaPipe process - is Python3 installed?")?;
        
        eprintln!("Python process spawned with PID: {:?}", child.id());
        
        let stdin = child.stdin.take()
            .ok_or_else(|| anyhow::anyhow!("Failed to get stdin"))?;
        let mut stdout = BufReader::new(
            child.stdout.take()
                .ok_or_else(|| anyhow::anyhow!("Failed to get stdout"))?
        );
        
        // Wait for ready signal with timeout
        eprintln!("Waiting for MediaPipe service to be ready...");
        let mut ready_line = String::new();
        let start = Instant::now();
        let timeout = Duration::from_secs(15); // Increased timeout
        
        loop {
            if start.elapsed() > timeout {
                eprintln!("Timeout after {:?}", start.elapsed());
                return Err(anyhow::anyhow!("Timeout waiting for MediaPipe service"));
            }
            
            match stdout.read_line(&mut ready_line) {
                Ok(0) => {
                    eprintln!("Python process closed unexpectedly");
                    return Err(anyhow::anyhow!("Python process terminated"));
                }
                Ok(_) => {
                    eprintln!("Received from Python: {}", ready_line.trim());
                    if ready_line.trim() == "READY" {
                        eprintln!("âœ“ MediaPipe service is ready!");
                        break;
                    }
                }
                Err(e) => {
                    eprintln!("Error reading from Python: {}", e);
                    return Err(anyhow::anyhow!("Failed to read from Python process: {}", e));
                }
            }
        }
        
        eprintln!("=== MediaPipe Initialized Successfully ===");
        
        Ok(Self {
            python_process: child,
            stdin,
            stdout,
        })
    }
    
    pub fn process_image(&mut self, image: &DynamicImage) -> Result<MediaPipeResult> {
        // Convert image to RGB bytes
        let rgb = image.to_rgb8();
        let frame_data = MediaPipeFrame {
            width: rgb.width(),
            height: rgb.height(),
            data: rgb.into_raw(),
        };
        
        eprintln!("Sending frame: {}x{} ({} bytes)", 
                 frame_data.width, frame_data.height, frame_data.data.len());
        
        // Send frame to Python
        let json_data = serde_json::to_string(&frame_data)?;
        writeln!(self.stdin, "{}", json_data)?;
        self.stdin.flush()?;
        
        // Read response
        let mut response = String::new();
        self.stdout.read_line(&mut response)
            .context("Failed to read response from MediaPipe")?;
        
        if response.trim().is_empty() {
            return Err(anyhow::anyhow!("Empty response from MediaPipe"));
        }
        
        // Parse result
        let result: MediaPipeResult = serde_json::from_str(&response)
            .context("Failed to parse MediaPipe response")?;
        
        if !result.pose_landmarks.is_empty() {
            eprintln!("âœ“ Received {} pose landmarks", result.pose_landmarks.len());
        } else {
            eprintln!("âœ— No pose landmarks detected");
        }
        
        Ok(result)
    }
    
    pub fn get_pose_landmarks(&mut self, image: &DynamicImage) -> Result<Vec<Vector3<f64>>> {
        let result = self.process_image(image)?;
        Ok(result.pose_landmarks.into_iter()
            .map(|[x, y, z]| Vector3::new(x, y, z))
            .collect())
    }
    
    pub fn get_hand_landmarks(&mut self, image: &DynamicImage) -> Result<Vec<Vec<Vector3<f64>>>> {
        let result = self.process_image(image)?;
        Ok(result.hand_landmarks.into_iter()
            .map(|hand| hand.into_iter()
                .map(|[x, y, z]| Vector3::new(x, y, z))
                .collect())
            .collect())
    }
}

impl Drop for MediaPipeWrapper {
    fn drop(&mut self) {
        eprintln!("Shutting down MediaPipe service");
        let _ = self.python_process.kill();
    }
}

================================================================================
FILE: tracking.rs
================================================================================

// src/tracking.rs - Fixed version with lazy MediaPipe initialization
use nalgebra::{Vector3, Vector6, Matrix3, Matrix6, Matrix3x6};
use std::collections::{HashMap, VecDeque};
use anyhow::Result;
use image::DynamicImage;
use crate::mediapipe_bridge::MediaPipeWrapper;
use std::time::Instant;

#[derive(Clone)]
pub struct PerformanceMetrics {
    pub avg_fps: f32,
    pub avg_processing_time: f32,
    pub tracking_confidence: f32,
    frame_times: VecDeque<f32>,
}

pub struct KalmanFilter {
    state: Vector6<f64>,  // [x, y, z, vx, vy, vz]
    covariance: Matrix6<f64>,
    process_noise: Matrix6<f64>,
    measurement_noise: Matrix3<f64>,    
    dt: f64,
}

#[derive(Debug, Clone, Copy, PartialEq)]
pub enum GestureType {
    Pronation,
    Supination,
    None,
}

#[derive(Debug, Clone)]
pub struct GestureState {
    pub gesture_type: GestureType,
    pub confidence: f64,
    pub angle: f64,
}

#[derive(Debug, Clone)]
pub struct JointState {
    pub position: Vector3<f64>,
    pub velocity: Vector3<f64>,
    pub confidence: f64,
    pub pixel_pos: (i32, i32),
}

#[derive(Debug, Clone)]
pub struct HandState {
    pub landmarks: Vec<Vector3<f64>>,
    pub confidences: Vec<f64>,
    pub is_tracked: bool,
}

#[derive(Debug, Clone, Default)]
pub struct TrackingResult {
    pub tracking_lost: bool,
    pub joints: HashMap<String, JointState>,
    pub hands: HashMap<String, HandState>,
    pub left_gesture: Option<GestureState>,
    pub right_gesture: Option<GestureState>,
    pub timestamp: f64,
}

pub struct ArmTracker {
    active_arms: HashMap<String, bool>,
    active_fingers: HashMap<String, bool>,
    palm_history: HashMap<String, VecDeque<Vector3<f64>>>,
    rotation_history: HashMap<String, VecDeque<f64>>,
    last_valid_gestures: HashMap<String, GestureState>,
    config: TrackerConfig,
    // Simulation state for demo
    sim_time: f64,
    mediapipe: Option<MediaPipeWrapper>,
    mediapipe_initialized: bool,
    init_attempts: u32,
    metrics: PerformanceMetrics,
    frame_counter: u32,
    adaptive_skip_rate: usize,
    last_confidence: f64,
    joint_filters: HashMap<String, KalmanFilter>,
    hand_state_cache: HashMap<String, (HandState, u32)>,
    hand_filters: HashMap<String, Vec<KalmanFilter>>,
}

#[derive(Debug, Clone)]
pub struct TrackerConfig {
    pub history_size: usize,
    pub confidence_threshold: f64,
    pub gesture_angle_threshold: f64,
    pub min_rotation_threshold: f64,
    pub rotation_smoothing_factor: f64,
    pub min_stable_frames: usize,
    pub enable_kalman: bool,          // Add this
    pub downsample_width: u32,        // Add this
    pub adaptive_frame_skip: bool,    // Add this
    pub max_frame_skip: usize,        // Add this
}

impl Default for TrackerConfig {
    fn default() -> Self {
        Self {
            history_size: 10,
            confidence_threshold: 0.6,
            gesture_angle_threshold: 0.05,  // Lowered from 0.1
            min_rotation_threshold: 0.03,   // Lowered from 0.05
            rotation_smoothing_factor: 0.5,  // Lowered from 0.6 for faster response
            min_stable_frames: 2,
            enable_kalman: true,
            downsample_width: 640,
            adaptive_frame_skip: false,  // Disable adaptive skipping
            max_frame_skip: 1,
        }
    }
}
impl PerformanceMetrics {
    pub fn new() -> Self {
        Self {
            avg_fps: 0.0,
            avg_processing_time: 0.0,
            tracking_confidence: 0.0,
            frame_times: VecDeque::with_capacity(30),
        }
    }
}


impl KalmanFilter {
    pub fn new() -> Self {
        let mut process_noise = Matrix6::identity() * 0.1;
        process_noise.fixed_view_mut::<3, 3>(3, 3).fill_diagonal(0.2);
        
        Self {
            state: Vector6::zeros(),
            covariance: Matrix6::identity(),
            process_noise,
            measurement_noise: Matrix3::identity() * 0.1,
            dt: 1.0 / 30.0,
        }
    }
    
    pub fn predict(&mut self) {
        let mut f = Matrix6::identity();
        f.fixed_view_mut::<3, 3>(0, 3).fill_diagonal(self.dt);
        
        self.state = f * self.state;
        self.covariance = f * self.covariance * f.transpose() + self.process_noise;
    }
    
    pub fn update(&mut self, measurement: Vector3<f64>) {
        // H is 3x6 matrix (observes position, not velocity)
        let mut h = Matrix3x6::<f64>::zeros();
        h[(0, 0)] = 1.0;
        h[(1, 1)] = 1.0;
        h[(2, 2)] = 1.0;
        
        // Innovation
        let y = measurement - (h * self.state);
        
        // Innovation covariance
        let s = h * self.covariance * h.transpose() + self.measurement_noise;
        
        // Kalman gain
        let k = self.covariance * h.transpose() * s.try_inverse().unwrap();
        
        // Update state and covariance
        self.state = self.state + k * y;
        let i = Matrix6::identity();
        self.covariance = (i - k * h) * self.covariance;
    }
    
    pub fn position(&self) -> Vector3<f64> {
        Vector3::new(self.state[0], self.state[1], self.state[2])
    }
}

impl ArmTracker {
    pub fn new() -> Result<Self> {
        let mut tracker = Self {
            active_arms: HashMap::new(),
            active_fingers: HashMap::new(),
            palm_history: HashMap::new(),
            rotation_history: HashMap::new(),
            last_valid_gestures: HashMap::new(),
            config: TrackerConfig::default(),
            sim_time: 0.0,
            mediapipe: None,
            mediapipe_initialized: false,
            init_attempts: 0,
            metrics: PerformanceMetrics::new(),
            frame_counter: 0,
            adaptive_skip_rate: 1,
            last_confidence: 0.0,
            joint_filters: HashMap::new(),
            hand_state_cache: HashMap::new(),
            hand_filters: HashMap::new(),
        };
        
        // Initialize tracking flags
        tracker.active_arms.insert("left".to_string(), true);
        tracker.active_arms.insert("right".to_string(), true);
        tracker.active_fingers.insert("left".to_string(), true);
        tracker.active_fingers.insert("right".to_string(), true);
        
        // Initialize history buffers
        for side in &["left", "right"] {
            tracker.palm_history.insert(
                side.to_string(),
                VecDeque::with_capacity(tracker.config.history_size)
            );
            tracker.rotation_history.insert(
                side.to_string(),
                VecDeque::with_capacity(tracker.config.history_size)
            );
            tracker.last_valid_gestures.insert(
                side.to_string(),
                GestureState {
                    gesture_type: GestureType::None,
                    confidence: 0.0,
                    angle: 0.0,
                }
            );
        }
        
        Ok(tracker)
    }

     pub fn initialize_mediapipe(&mut self) {
        if self.mediapipe_initialized {
            eprintln!("MediaPipe already initialized");
            return;
        }
        
        eprintln!("Initializing MediaPipe for camera tracking...");
        std::thread::sleep(std::time::Duration::from_millis(500));
        
        match MediaPipeWrapper::new() {
            Ok(mp) => {
                eprintln!("âœ“ MediaPipe initialized successfully");
                self.mediapipe = Some(mp);
                self.mediapipe_initialized = true;
                self.init_attempts = 0;
            }
            Err(e) => {
                eprintln!("âœ— MediaPipe initialization failed: {}", e);
                eprintln!("  Will use simulation mode for tracking");
            }
        }
    }
    
    pub fn shutdown_mediapipe(&mut self) {
        if self.mediapipe.is_some() {
            eprintln!("Shutting down MediaPipe...");
            self.mediapipe = None;
            self.mediapipe_initialized = false;
            self.init_attempts = 0;
            eprintln!("âœ“ MediaPipe shutdown complete");
        }
    }
    
    fn generate_simulation_data(&mut self, result: &mut TrackingResult) {
        let t = self.sim_time;
        
        if *self.active_arms.get("left").unwrap_or(&false) {
            result.joints.insert("left_shoulder".to_string(), JointState {
                position: Vector3::new(0.3, 0.4, 0.0),
                velocity: Vector3::zeros(),
                confidence: 0.95,
                pixel_pos: (300, 200),
            });
            
            result.joints.insert("left_elbow".to_string(), JointState {
                position: Vector3::new(0.35, 0.5 + 0.05 * t.sin(), 0.0),
                velocity: Vector3::new(0.0, 0.05 * t.cos(), 0.0),
                confidence: 0.9,
                pixel_pos: (350, 300),
            });
            
            result.joints.insert("left_wrist".to_string(), JointState {
                position: Vector3::new(0.4 + 0.1 * (t * 0.5).cos(), 0.6 + 0.1 * t.sin(), 0.0),
                velocity: Vector3::new(-0.05 * (t * 0.5).sin(), 0.1 * t.cos(), 0.0),
                confidence: 0.85,
                pixel_pos: (400, 400),
            });
            
            let gesture_type = if (t * 0.3).sin() > 0.3 {
                GestureType::Supination
            } else if (t * 0.3).sin() < -0.3 {
                GestureType::Pronation
            } else {
                GestureType::None
            };
            
            if gesture_type != GestureType::None {
                result.left_gesture = Some(GestureState {
                    gesture_type,
                    confidence: 0.7 + 0.2 * (t * 2.0).sin().abs(),
                    angle: 45.0_f64.to_radians() * (t * 0.3).sin(),
                });
            }
        }
        
        if *self.active_arms.get("right").unwrap_or(&false) {
            result.joints.insert("right_shoulder".to_string(), JointState {
                position: Vector3::new(0.7, 0.4, 0.0),
                velocity: Vector3::zeros(),
                confidence: 0.95,
                pixel_pos: (700, 200),
            });
            
            result.joints.insert("right_elbow".to_string(), JointState {
                position: Vector3::new(0.65, 0.5 + 0.05 * (t + 1.5).sin(), 0.0),
                velocity: Vector3::new(0.0, 0.05 * (t + 1.5).cos(), 0.0),
                confidence: 0.9,
                pixel_pos: (650, 300),
            });
            
            result.joints.insert("right_wrist".to_string(), JointState {
                position: Vector3::new(0.6 - 0.1 * (t * 0.5 + 1.0).cos(), 0.6 + 0.1 * (t + 1.5).sin(), 0.0),
                velocity: Vector3::new(0.05 * (t * 0.5 + 1.0).sin(), 0.1 * (t + 1.5).cos(), 0.0),
                confidence: 0.85,
                pixel_pos: (600, 400),
            });
            
            let gesture_type = if (t * 0.25 + 1.0).sin() > 0.3 {
                GestureType::Pronation
            } else if (t * 0.25 + 1.0).sin() < -0.3 {
                GestureType::Supination
            } else {
                GestureType::None
            };
            
            if gesture_type != GestureType::None {
                result.right_gesture = Some(GestureState {
                    gesture_type,
                    confidence: 0.65 + 0.25 * (t * 1.5).cos().abs(),
                    angle: 50.0_f64.to_radians() * (t * 0.25 + 1.0).sin(),
                });
            }
        }
    }
    
    pub fn toggle_arm(&mut self, side: &str) {
        if let Some(active) = self.active_arms.get_mut(side) {
            *active = !*active;
        }
    }
    
    pub fn toggle_fingers(&mut self, side: &str) {
        if let Some(active) = self.active_fingers.get_mut(side) {
            *active = !*active;
        }
    }
    
    pub fn is_using_mediapipe(&self) -> bool {
        self.mediapipe.is_some() && self.mediapipe_initialized
    }
    
    pub fn is_initializing(&self) -> bool {
        false
    }
    
    pub fn reset_mediapipe(&mut self) {
        self.shutdown_mediapipe();
        eprintln!("MediaPipe reset - call initialize_mediapipe() to retry");
    }

    // Add the missing process_hand_landmarks method
    // Add the missing process_hand_landmarks method
fn process_hand_landmarks(&mut self, hand_landmarks: &[[f64; 3]], hand_index: usize, result: &mut TrackingResult) {
    if hand_landmarks.len() < 21 {
        return;
    }
    
    let landmarks: Vec<Vector3<f64>> = hand_landmarks.iter()
        .map(|lm| Vector3::new(lm[0], lm[1], lm[2]))
        .collect();
    
    let wrist_pos = landmarks[0];
    
    // Try to match to wrist joints first (strictest)
    let side = if result.joints.contains_key("left_wrist") && 
                result.joints.contains_key("right_wrist") {
        let left_wrist = &result.joints["left_wrist"].position;
        let right_wrist = &result.joints["right_wrist"].position;
        
        let dist_left = (wrist_pos - left_wrist).norm();
        let dist_right = (wrist_pos - right_wrist).norm();
        
        eprintln!("Hand {} distances - left: {:.3}, right: {:.3}", hand_index, dist_left, dist_right);
        
        // Increased threshold - MediaPipe coordinates are 0-1 range
        const MAX_HAND_ARM_DISTANCE: f64 = 0.3; // DOUBLED from 0.15
        
        if dist_left.min(dist_right) > MAX_HAND_ARM_DISTANCE {
            eprintln!("Hand {} too far from wrists (min dist: {:.3}), trying position fallback", 
                     hand_index, dist_left.min(dist_right));
            
            // FALLBACK: Use x-position if distances too large
            if wrist_pos.x > 0.5 { "right" } else { "left" }
        } else {
            if dist_left < dist_right { "left" } else { "right" }
        }
    } else {
        eprintln!("Missing wrist joints - left: {}, right: {}", 
                 result.joints.contains_key("left_wrist"),
                 result.joints.contains_key("right_wrist"));
        
        // FALLBACK: Use x-position
        if wrist_pos.x > 0.5 { "right" } else { "left" }
    };
    
    eprintln!("Hand {} assigned to {} side", hand_index, side);
    
    // Rest of your code unchanged...
    let filters = self.get_or_create_hand_filters(side);
    let mut smoothed_landmarks = Vec::new();
    
    for (i, lm) in hand_landmarks.iter().enumerate() {
        let measurement = Vector3::new(lm[0], lm[1], lm[2]);
        filters[i].predict();
        filters[i].update(measurement);
        smoothed_landmarks.push(filters[i].position());
    }

    let hand_state = HandState {
        landmarks: smoothed_landmarks.clone(),
        confidences: vec![1.0; smoothed_landmarks.len()],
        is_tracked: true,
    };
    
    self.hand_state_cache.insert(side.to_string(), (hand_state.clone(), 0));
    result.hands.insert(side.to_string(), hand_state);
    
    // Calculate gesture if we have arm joints
    if result.joints.contains_key(&format!("{}_shoulder", side)) &&
       result.joints.contains_key(&format!("{}_elbow", side)) &&
       result.joints.contains_key(&format!("{}_wrist", side)) {
        
        let shoulder = &result.joints[&format!("{}_shoulder", side)].position;
        let elbow = &result.joints[&format!("{}_elbow", side)].position;
        let wrist = &result.joints[&format!("{}_wrist", side)].position;
        
        if let Some(gesture) = self.calculate_arm_rotation_enhanced(
            side,
            shoulder,
            elbow,
            wrist,
            Some(&smoothed_landmarks)
        ) {
            if side == "left" {
                result.left_gesture = Some(gesture);
            } else {
                result.right_gesture = Some(gesture);
            }
        }
    }
}

    pub fn process_frame_with_metrics(&mut self, frame: &DynamicImage) -> Result<(TrackingResult, PerformanceMetrics)> {
        let start = Instant::now();
        let result = self.process_frame(frame)?;
        let elapsed = start.elapsed().as_secs_f32();
        
        self.metrics.frame_times.push_front(elapsed);
        if self.metrics.frame_times.len() > 30 {
            self.metrics.frame_times.pop_back();
        }
        
        self.metrics.avg_processing_time = self.metrics.frame_times.iter().sum::<f32>() 
            / self.metrics.frame_times.len() as f32;
        self.metrics.avg_fps = 1.0 / self.metrics.avg_processing_time;
        
        // Fix: Convert f64 to f32
        self.metrics.tracking_confidence = if result.joints.is_empty() {
            0.0
        } else {
            (result.joints.values()
                .map(|j| j.confidence)
                .sum::<f64>() / result.joints.len() as f64) as f32
        };
        
        Ok((result, self.metrics.clone()))
    }

    fn get_or_create_hand_filters(&mut self, side: &str) -> &mut Vec<KalmanFilter> {
        self.hand_filters.entry(side.to_string())
            .or_insert_with(|| {
                (0..21).map(|_| KalmanFilter::new()).collect()
            })
    }

    fn calculate_arm_rotation_enhanced(
        &mut self, 
        side: &str,
        shoulder: &Vector3<f64>, 
        elbow: &Vector3<f64>, 
        wrist: &Vector3<f64>,
        hand_landmarks: Option<&Vec<Vector3<f64>>>
    ) -> Option<GestureState> {
        // Calculate forearm vector
        let forearm = (wrist - elbow).normalize();
        
        // Get palm normal if hand landmarks available
        let palm_normal = hand_landmarks.and_then(|landmarks| {
            if landmarks.len() >= 21 {
                Some(self.calculate_palm_normal(landmarks))
            } else {
                None
            }
        })?;  // Early return if no palm normal
        
        // Calculate rotation axis and angle relative to anatomical reference
        let rotation_axis = palm_normal.cross(&forearm);
        let _rotation_angle = palm_normal.dot(&forearm).clamp(-1.0, 1.0).acos();
        
        // Update palm history with anatomically aware normal
        let history = self.palm_history.get_mut(side).unwrap();
        history.push_front(palm_normal);
        if history.len() > self.config.history_size {
            history.pop_back();
        }
        
        // Need at least MIN_STABLE_FRAMES for stable detection
        if history.len() < self.config.min_stable_frames {
            return None;
        }

        // Calculate smoothed rotation angle from palm history - MATCHING C++ LOGIC
        let mut cumulative_angle = 0.0;
        let mut cumulative_axis = Vector3::zeros();
        let mut valid_samples = 0;

        for i in 1..history.len() {
            let curr_normal = history[i-1];
            let prev_normal = history[i];
            
            // Calculate rotation angle between consecutive frames
            let angle = curr_normal.dot(&prev_normal).clamp(-1.0, 1.0).acos();
            
            // Only count significant rotations - MATCHING C++
            if angle > self.config.min_rotation_threshold {
                cumulative_angle += angle;
                cumulative_axis += curr_normal.cross(&prev_normal);
                valid_samples += 1;
            }
        }

        // If we don't have enough valid samples, no significant rotation
        if valid_samples < (self.config.min_stable_frames - 1) {
            return None;
        }

        let avg_angle = cumulative_angle / valid_samples as f64;
        let _avg_axis = cumulative_axis.normalize();

        // Apply exponential smoothing to rotation history
        let rotation_history = self.rotation_history.get_mut(side).unwrap();
        rotation_history.push_front(avg_angle);
        if rotation_history.len() > self.config.history_size {
            rotation_history.pop_back();
        }

        // Calculate smoothed rotation with exponential moving average - MATCHING C++
        let mut smoothed_rotation = 0.0;
        let mut weight_sum = 0.0;
        let mut weight = 1.0;

        for rot in rotation_history.iter() {
            smoothed_rotation += rot * weight;
            weight_sum += weight;
            weight *= self.config.rotation_smoothing_factor;
        }
        smoothed_rotation /= weight_sum;

        // Only detect rotation if it's significant
        if smoothed_rotation > self.config.gesture_angle_threshold {
            // Determine rotation direction - MATCHING C++ LOGIC
            let is_supination = if side == "left" {
                // For left arm, positive rotation around forearm axis is supination
                rotation_axis.dot(&Vector3::y()) < 0.0
            } else {
                // For right arm, negative rotation around forearm axis is supination
                rotation_axis.dot(&Vector3::y()) < 0.0
            };
            
            Some(GestureState {
                gesture_type: if is_supination { 
                    GestureType::Supination 
                } else { 
                    GestureType::Pronation 
                },
                confidence: (smoothed_rotation / (self.config.gesture_angle_threshold * 2.0)).min(1.0),
                angle: smoothed_rotation,
            })
        } else {
            None
        }
    }

    fn calculate_palm_normal(&self, landmarks: &[Vector3<f64>]) -> Vector3<f64> {
        // MediaPipe hand landmark indices - matching C++ exactly
        const WRIST: usize = 0;
        const THUMB_CMC: usize = 1;
        const INDEX_MCP: usize = 5;
        const MIDDLE_MCP: usize = 9;
        const RING_MCP: usize = 13;
        const PINKY_MCP: usize = 17;
        const MIDDLE_PIP: usize = 10;
        const MIDDLE_TIP: usize = 12;

        // Get key points
        let wrist = landmarks[WRIST];
        let thumb_cmc = landmarks[THUMB_CMC];
        let index_mcp = landmarks[INDEX_MCP];
        let middle_mcp = landmarks[MIDDLE_MCP];
        let ring_mcp = landmarks[RING_MCP];
        let pinky_mcp = landmarks[PINKY_MCP];
        let middle_pip = landmarks[MIDDLE_PIP];
        let middle_tip = landmarks[MIDDLE_TIP];

        // Calculate robust palm direction vectors
        let palm_center = (index_mcp + middle_mcp + ring_mcp + pinky_mcp) / 4.0;
        let palm_direction = (palm_center - wrist).normalize();
        
        // Calculate palm width vector (perpendicular to thumb-pinky line)
        let thumb_pinky = (pinky_mcp - thumb_cmc).normalize();
        
        // Calculate finger direction (using middle finger as reference)
        let finger_direction = (middle_tip - middle_mcp).normalize();
        
        // Calculate palm normal using multiple reference vectors - THIS IS THE KEY DIFFERENCE
        let normal1 = thumb_pinky.cross(&palm_direction);
        let normal2 = thumb_pinky.cross(&finger_direction);
        
        // Combine normals with weights (equal weighting like C++)
        let weighted_normal = (normal1 + normal2).normalize();
        
        weighted_normal
    }


// In tracking.rs, update the process_frame method around line 500:
pub fn process_frame(&mut self, frame: &DynamicImage) -> Result<TrackingResult> {
    let mut result = TrackingResult::default();
    result.timestamp = self.sim_time;
    self.sim_time += 0.033;
    self.frame_counter += 1;
    
    if let Some(ref mut mp) = self.mediapipe {
        match mp.process_image(frame) {
            Ok(mp_result) => {
                if mp_result.pose_landmarks.len() > 16 {
                    self.process_pose_with_kalman(&mp_result.pose_landmarks, &mut result);
                    
                    for (i, hand_lms) in mp_result.hand_landmarks.iter().enumerate() {
                        self.process_hand_landmarks(hand_lms, i, &mut result);
                    }
                    
                    // Keep gestures from last_valid_gestures if not detected this frame
                    if result.left_gesture.is_none() {
                        if let Some(last_gesture) = self.last_valid_gestures.get("left") {
                            if last_gesture.gesture_type != GestureType::None {
                                result.left_gesture = Some(last_gesture.clone());
                            }
                        }
                    } else if let Some(gesture) = &result.left_gesture {
                        self.last_valid_gestures.insert("left".to_string(), gesture.clone());
                    }
                    
                    if result.right_gesture.is_none() {
                        if let Some(last_gesture) = self.last_valid_gestures.get("right") {
                            if last_gesture.gesture_type != GestureType::None {
                                result.right_gesture = Some(last_gesture.clone());
                            }
                        }
                    } else if let Some(gesture) = &result.right_gesture {
                        self.last_valid_gestures.insert("right".to_string(), gesture.clone());
                    }
                    
                    result.tracking_lost = false;
                }
            }
            Err(e) => {
                eprintln!("MediaPipe error: {}", e);
                result.tracking_lost = true;
            }
        }
    } else {
        self.generate_simulation_data(&mut result);
    }
    
    Ok(result)
}

    fn process_pose_with_kalman(&mut self, landmarks: &[[f64; 3]], result: &mut TrackingResult) {
        const LEFT_SHOULDER: usize = 11;
        const RIGHT_SHOULDER: usize = 12;
        const LEFT_ELBOW: usize = 13;
        const RIGHT_ELBOW: usize = 14;
        const LEFT_WRIST: usize = 15;
        const RIGHT_WRIST: usize = 16;
        
        let joint_indices = [
            ("left_shoulder", LEFT_SHOULDER),
            ("right_shoulder", RIGHT_SHOULDER),
            ("left_elbow", LEFT_ELBOW),
            ("right_elbow", RIGHT_ELBOW),
            ("left_wrist", LEFT_WRIST),
            ("right_wrist", RIGHT_WRIST),
        ];
        
        for (name, idx) in joint_indices.iter() {
            if *idx < landmarks.len() {
                let measurement = Vector3::new(
                    landmarks[*idx][0],
                    landmarks[*idx][1],
                    landmarks[*idx][2],
                );
                
                // Use or create Kalman filter for this joint
                let kalman = self.joint_filters
                    .entry(name.to_string())
                    .or_insert_with(KalmanFilter::new);
                
                kalman.predict();
                kalman.update(measurement);
                
                let smoothed_pos = kalman.position();
                
                result.joints.insert(name.to_string(), JointState {
                    position: smoothed_pos,
                    velocity: Vector3::zeros(), // Could calculate from Kalman state
                    confidence: 0.9,
                    pixel_pos: (
                        (smoothed_pos.x * 640.0) as i32,
                        (smoothed_pos.y * 480.0) as i32
                    ),
                });
            }
        }
    }



}


================================================================================
FILE: ui.rs
================================================================================

// src/ui.rs - Fixed to use resvg's re-exported tiny_skia
use eframe::egui::{self, Color32, Pos2, Rect, Stroke, Vec2};
use image::DynamicImage;
use usvg::TreeParsing;

#[derive(Debug, Clone)]
pub struct Theme {
    pub primary: Color32,
    pub secondary: Color32,
    pub background: Color32,
    pub surface: Color32,
    pub error: Color32,
    pub warning: Color32,
    pub success: Color32,
    pub text_primary: Color32,
    pub text_secondary: Color32,
}

impl Default for Theme {
    fn default() -> Self {
        Self {
            primary: Color32::from_rgb(70, 130, 240),
            secondary: Color32::from_rgb(255, 152, 0),
            background: Color32::from_rgb(20, 20, 25),
            surface: Color32::from_rgb(30, 30, 35),
            error: Color32::from_rgb(244, 67, 54),
            warning: Color32::from_rgb(255, 152, 0),
            success: Color32::from_rgb(76, 175, 80),
            text_primary: Color32::WHITE,
            text_secondary: Color32::from_rgb(200, 200, 200),
        }
    }
}

pub struct UIComponents {
    pub logo_texture: Option<egui::TextureHandle>,
    pub theme: Theme,
    animations: AnimationState,
}

#[derive(Default)]
struct AnimationState {
    record_pulse: f32,
    gesture_transitions: std::collections::HashMap<String, f32>,
}

impl UIComponents {
    pub fn new(ctx: &egui::Context) -> Self {
        let mut components = Self {
            logo_texture: None,
            theme: Theme::default(),
            animations: AnimationState::default(),
        };
        
        // Try to load SVG logo
        let logo_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/assets/supro.svg";
        if let Ok(logo_rgba) = load_svg_as_rgba(logo_path, 256) {
            let size = [256, 256];
            let color_image = egui::ColorImage::from_rgba_unmultiplied(
                size,
                &logo_rgba,
            );
            
            components.logo_texture = Some(ctx.load_texture(
                "logo",
                color_image,
                Default::default(),
            ));
        }
        
        components
    }
    
    pub fn draw_gesture_indicator(
        &mut self,
        ui: &mut egui::Ui,
        gesture_type: &str,
        confidence: f32,
        angle: f32,
    ) {
        let available_size = ui.available_size();
        let center = Pos2::new(available_size.x / 2.0, available_size.y / 2.0);
        let radius = available_size.x.min(available_size.y) * 0.4;
        
        // Background circle
        let painter = ui.painter();
        painter.circle_filled(center, radius, self.theme.surface);
        
        // Confidence arc
        let color = match gesture_type {
            "supination" => self.theme.success,
            "pronation" => self.theme.warning,
            _ => self.theme.text_secondary,
        };
        
        let arc_angle = confidence * std::f32::consts::PI * 2.0;
        draw_arc(painter, center, radius * 0.9, 0.0, arc_angle, color, 5.0);
        
        // Center text
        painter.text(
            center,
            egui::Align2::CENTER_CENTER,
            gesture_type.to_uppercase(),
            egui::FontId::proportional(24.0),
            self.theme.text_primary,
        );
        
        // Angle indicator
        let angle_text = format!("{:.1}Â°", angle.to_degrees());
        painter.text(
            Pos2::new(center.x, center.y + radius * 0.5),
            egui::Align2::CENTER_CENTER,
            angle_text,
            egui::FontId::proportional(16.0),
            self.theme.text_secondary,
        );
    }
    
    pub fn draw_joint_skeleton(
        &mut self,
        ui: &mut egui::Ui,
        joints: &[(String, (f32, f32))],
    ) {
        let painter = ui.painter();
        let rect = ui.available_rect_before_wrap();
        
        // Define skeleton connections
        let connections = vec![
            ("left_shoulder", "left_elbow"),
            ("left_elbow", "left_wrist"),
            ("right_shoulder", "right_elbow"),
            ("right_elbow", "right_wrist"),
            ("left_shoulder", "right_shoulder"),
        ];
        
        // Draw connections
        for (from, to) in connections {
            if let (Some(from_joint), Some(to_joint)) = (
                joints.iter().find(|(name, _)| name == from),
                joints.iter().find(|(name, _)| name == to),
            ) {
                let from_pos = Pos2::new(
                    rect.left() + from_joint.1.0 * rect.width(),
                    rect.top() + from_joint.1.1 * rect.height(),
                );
                let to_pos = Pos2::new(
                    rect.left() + to_joint.1.0 * rect.width(),
                    rect.top() + to_joint.1.1 * rect.height(),
                );
                
                painter.line_segment(
                    [from_pos, to_pos],
                    Stroke::new(2.0, self.theme.primary),
                );
            }
        }
        
        // Draw joints
        for (name, (x, y)) in joints {
            let pos = Pos2::new(
                rect.left() + x * rect.width(),
                rect.top() + y * rect.height(),
            );
            
            let color = if name.contains("left") {
                self.theme.primary
            } else {
                self.theme.secondary
            };
            
            painter.circle_filled(pos, 5.0, color);
            painter.circle_stroke(pos, 7.0, Stroke::new(2.0, self.theme.text_primary));
        }
    }
    
    pub fn draw_recording_indicator(&mut self, ui: &mut egui::Ui, is_recording: bool) {
        if !is_recording {
            return;
        }
        
        // Animate pulse effect
        self.animations.record_pulse += ui.input(|i| i.unstable_dt) * 2.0;
        let pulse = (self.animations.record_pulse.sin() + 1.0) * 0.5;
        
        let size = 20.0 + pulse * 5.0;
        let color = Color32::from_rgb(
            244,
            (67.0 + pulse * 30.0) as u8,
            54,
        );
        
        let painter = ui.painter();
        let pos = Pos2::new(ui.available_width() - 30.0, 30.0);
        
        painter.circle_filled(pos, size, color);
        painter.text(
            Pos2::new(pos.x - 50.0, pos.y),
            egui::Align2::RIGHT_CENTER,
            "REC",
            egui::FontId::proportional(14.0),
            color,
        );
    }
    
    pub fn draw_confidence_bar(
        &self,
        ui: &mut egui::Ui,
        label: &str,
        value: f32,
    ) {
        ui.horizontal(|ui| {
            ui.label(label);
            
            let bar_width = 200.0;
            let bar_height = 20.0;
            let rect = ui.allocate_space(Vec2::new(bar_width, bar_height)).1;
            
            let painter = ui.painter();
            
            // Background
            painter.rect_filled(
                rect,
                egui::Rounding::same(4.0),
                self.theme.surface,
            );
            
            // Fill
            let fill_width = bar_width * value;
            let fill_rect = Rect::from_min_size(
                rect.min,
                Vec2::new(fill_width, bar_height),
            );
            
            let color = if value > 0.7 {
                self.theme.success
            } else if value > 0.4 {
                self.theme.warning
            } else {
                self.theme.error
            };
            
            painter.rect_filled(
                fill_rect,
                egui::Rounding::same(4.0),
                color,
            );
            
            // Text
            painter.text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                format!("{:.0}%", value * 100.0),
                egui::FontId::proportional(12.0),
                self.theme.text_primary,
            );
        });
    }
}

fn draw_arc(
    painter: &egui::Painter,
    center: Pos2,
    radius: f32,
    start_angle: f32,
    end_angle: f32,
    color: Color32,
    thickness: f32,
) {
    let points_count = ((end_angle - start_angle).abs() * 50.0) as usize;
    let mut points = Vec::with_capacity(points_count);
    
    for i in 0..=points_count {
        let t = i as f32 / points_count as f32;
        let angle = start_angle + (end_angle - start_angle) * t;
        let x = center.x + radius * angle.cos();
        let y = center.y + radius * angle.sin();
        points.push(Pos2::new(x, y));
    }
    
    for i in 1..points.len() {
        painter.line_segment(
            [points[i - 1], points[i]],
            Stroke::new(thickness, color),
        );
    }
}

fn load_svg_as_rgba(path: &str, size: u32) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    let svg_data = std::fs::read_to_string(path)?;
    let opt = usvg::Options::default();
    let tree = usvg::Tree::from_str(&svg_data, &opt)?;
    
    // Use resvg's re-exported tiny_skia types
    let pixmap_size = tree.size.to_int_size();
    let mut pixmap = resvg::tiny_skia::Pixmap::new(size, size).unwrap();
    
    let scale = size as f32 / pixmap_size.width().max(pixmap_size.height()) as f32;
    let transform = resvg::tiny_skia::Transform::from_scale(scale, scale);
    
    // Use the Tree's render method directly with consistent types
    resvg::Tree::from_usvg(&tree).render(transform, &mut pixmap.as_mut());
    
    Ok(pixmap.data().to_vec())
}

fn load_logo_image() -> Result<DynamicImage, image::ImageError> {
    // This is now a fallback
    Ok(DynamicImage::new_rgba8(128, 128))
}

// Custom widget for video display
pub struct VideoWidget {
    texture_id: Option<egui::TextureId>,
    aspect_ratio: f32,
}

impl VideoWidget {
    pub fn new() -> Self {
        Self {
            texture_id: None,
            aspect_ratio: 16.0 / 9.0,
        }
    }
    
    pub fn update_frame(&mut self, ctx: &egui::Context, frame: &DynamicImage) {
        // Convert image to egui texture
        let size = [frame.width() as _, frame.height() as _];
        let rgba = frame.to_rgba8();
        let pixels = rgba.as_flat_samples();
        
        let color_image = egui::ColorImage::from_rgba_unmultiplied(
            size,
            pixels.as_slice(),
        );
        
        self.texture_id = Some(ctx.load_texture(
            "video_frame",
            color_image,
            Default::default(),
        ).id());
    }
    
    pub fn show(&self, ui: &mut egui::Ui) {
        let available_size = ui.available_size();
        let widget_width = available_size.x;
        let widget_height = widget_width / self.aspect_ratio;
        
        let size = Vec2::new(widget_width, widget_height);
        let (rect, _response) = ui.allocate_exact_size(size, egui::Sense::hover());
        
        if let Some(texture_id) = self.texture_id {
            ui.painter().image(
                texture_id,
                rect,
                Rect::from_min_max(Pos2::ZERO, Pos2::new(1.0, 1.0)),
                Color32::WHITE,
            );
        } else {
            ui.painter().rect_filled(
                rect,
                egui::Rounding::same(4.0),
                Color32::from_rgb(50, 50, 55),
            );
            ui.painter().text(
                rect.center(),
                egui::Align2::CENTER_CENTER,
                "No Video Signal",
                egui::FontId::proportional(16.0),
                Color32::from_rgb(150, 150, 155),
            );
        }
    }
}

================================================================================
FILE: video.rs
================================================================================

// src/video.rs - Enhanced with video file processing and overlay capabilities
use std::path::{Path, PathBuf};
use anyhow::{Result, Context};
use image::{DynamicImage, ImageBuffer};
use nokhwa::pixel_format::RgbFormat;
use nokhwa::utils::{CameraIndex, RequestedFormat, RequestedFormatType};
use nokhwa::Camera;
use std::sync::{Arc, Mutex};
use std::process::Command;
use std::fs;

pub enum VideoSource {
    Camera(Arc<Mutex<Camera>>),
    File(VideoFileReader),
}

pub struct VideoFileReader {
    path: PathBuf,
    current_frame: usize,
    total_frames: usize,
    width: u32,
    height: u32,
    fps: f32,
    frames_cache: Vec<DynamicImage>,
    is_loaded: bool,
    loading_progress: f32,
    loading_message: String,
}

impl VideoFileReader {
    pub fn new(path: impl AsRef<Path>) -> Result<Self> {
        let path = path.as_ref().to_path_buf();

        // Check if file exists
        if !path.exists() {
            return Err(anyhow::anyhow!("Video file does not exist: {}", path.display()));
        }

        // Check if we have read permissions
        if let Err(e) = std::fs::File::open(&path) {
            return Err(anyhow::anyhow!("Cannot read video file (permission denied): {}", e));
        }

        // Check if ffprobe is available
        if Command::new("ffprobe").arg("-version").output().is_err() {
            return Err(anyhow::anyhow!("FFmpeg is not installed or not in PATH. Please install FFmpeg to process videos."));
        }

        // Get video info using ffprobe
        let output = Command::new("ffprobe")
            .args(&[
                "-v", "error",
                "-select_streams", "v:0",
                "-count_frames",
                "-show_entries", "stream=width,height,r_frame_rate,nb_frames",
                "-of", "csv=p=0",
                path.to_str().unwrap(),
            ])
            .output()
            .context("Failed to run ffprobe")?;
        
        let info = String::from_utf8_lossy(&output.stdout);
        let parts: Vec<&str> = info.trim().split(',').collect();

        if parts.len() < 4 {
            return Err(anyhow::anyhow!("Invalid video format or corrupted file"));
        }

        let width = parts[0].parse()
            .map_err(|_| anyhow::anyhow!("Invalid video width"))?;
        let height = parts[1].parse()
            .map_err(|_| anyhow::anyhow!("Invalid video height"))?;
        let fps_str = parts[2];
        let fps = if fps_str.contains('/') {
            let fps_parts: Vec<&str> = fps_str.split('/').collect();
            if fps_parts.len() != 2 {
                return Err(anyhow::anyhow!("Invalid frame rate format"));
            }
            fps_parts[0].parse::<f32>().unwrap_or(30.0) / fps_parts[1].parse::<f32>().unwrap_or(1.0)
        } else {
            fps_str.parse().unwrap_or(30.0)
        };
        let total_frames: usize = parts[3].parse()
            .map_err(|_| anyhow::anyhow!("Invalid frame count"))?;

        if total_frames == 0 {
            return Err(anyhow::anyhow!("Video has no frames"));
        }

        Ok(Self {
            path,
            current_frame: 0,
            total_frames,
            width,
            height,
            fps,
            frames_cache: Vec::new(),
            is_loaded: false,
            loading_progress: 0.0,
            loading_message: String::from("Initializing..."),
        })
    }

    pub fn get_loading_progress(&self) -> f32 {
        self.loading_progress
    }

    pub fn get_loading_message(&self) -> &str {
        &self.loading_message
    }

    pub fn get_total_frames(&self) -> usize {
        self.total_frames
    }
    
    pub fn load_all_frames(&mut self) -> Result<()> {
        if self.is_loaded {
            return Ok(());
        }

        eprintln!("Loading video frames from: {}", self.path.display());
        self.loading_message = "Extracting frames...".to_string();
        self.loading_progress = 0.0;

        // Check if ffmpeg is available
        if Command::new("ffmpeg").arg("-version").output().is_err() {
            return Err(anyhow::anyhow!("FFmpeg is not installed. Please install FFmpeg to process videos."));
        }

        // Check available disk space
        let temp_dir = std::env::temp_dir().join(format!("supro_{}", uuid::Uuid::new_v4()));

        // Estimate required space (rough estimate: frames * 0.5MB per frame)
        let estimated_space_mb = (self.total_frames as f64 * 0.5) as u64;
        eprintln!("Estimated disk space needed: {} MB", estimated_space_mb);

        if let Err(e) = fs::create_dir_all(&temp_dir) {
            return Err(anyhow::anyhow!("Cannot create temporary directory: {}", e));
        }

        self.loading_progress = 0.1;
        self.loading_message = format!("Extracting {} frames...", self.total_frames);

        // Extract frames as images
        let status = Command::new("ffmpeg")
            .args(&[
                "-i", self.path.to_str().unwrap(),
                "-vf", "scale=640:480",
                &format!("{}/frame_%04d.png", temp_dir.display()),
            ])
            .status()
            .context("Failed to extract frames with ffmpeg")?;

        if !status.success() {
            let _ = fs::remove_dir_all(&temp_dir);
            return Err(anyhow::anyhow!("FFmpeg frame extraction failed. The video format may be unsupported."));
        }

        self.loading_progress = 0.5;
        self.loading_message = "Loading frames into memory...".to_string();

        // Load extracted frames
        self.frames_cache.clear();
        for i in 1..=self.total_frames {
            let frame_path = temp_dir.join(format!("frame_{:04}.png", i));
            if frame_path.exists() {
                match image::open(&frame_path) {
                    Ok(img) => {
                        self.frames_cache.push(img);
                        self.loading_progress = 0.5 + (0.5 * (i as f32 / self.total_frames as f32));
                        self.loading_message = format!("Loading frame {}/{}", i, self.total_frames);
                    }
                    Err(e) => {
                        eprintln!("Warning: Failed to load frame {}: {}", i, e);
                    }
                }
            }
        }

        // Clean up temp files
        let _ = fs::remove_dir_all(&temp_dir);

        if self.frames_cache.is_empty() {
            return Err(anyhow::anyhow!("No frames could be loaded from the video"));
        }

        self.is_loaded = true;
        self.loading_progress = 1.0;
        self.loading_message = format!("Loaded {} frames successfully", self.frames_cache.len());
        eprintln!("Loaded {} frames", self.frames_cache.len());
        Ok(())
    }
    
    pub fn get_frame(&mut self, index: usize) -> Option<DynamicImage> {
        if !self.is_loaded {
            let _ = self.load_all_frames();
        }
        self.frames_cache.get(index).cloned()
    }
    
    pub fn next_frame(&mut self) -> Option<DynamicImage> {
        let frame = self.get_frame(self.current_frame);
        if frame.is_some() {
            self.current_frame = (self.current_frame + 1) % self.total_frames;
        }
        frame
    }
    
    pub fn seek(&mut self, frame_index: usize) {
        self.current_frame = frame_index.min(self.total_frames - 1);
    }
    
    pub fn get_progress(&self) -> f32 {
        if self.total_frames == 0 {
            0.0
        } else {
            self.current_frame as f32 / self.total_frames as f32
        }
    }
}

#[derive(Debug, Clone)]
pub struct VideoInfo {
    pub path: PathBuf,
    pub fps: f64,
    pub frame_count: i32,
    pub width: i32,
    pub height: i32,
    pub current_frame: i32,
}

impl VideoSource {
    pub fn new_camera(index: i32) -> Result<Self> {
        eprintln!("DEBUG: Attempting to open camera index {}", index);
        
        let camera_index = CameraIndex::Index(index as u32);
        
        use nokhwa::utils::{CameraFormat, FrameFormat, Resolution};
        
        let format = CameraFormat::new(
            Resolution::new(640, 480),
            FrameFormat::MJPEG,
            30,
        );
        
        let requested = RequestedFormat::new::<RgbFormat>(RequestedFormatType::Exact(format));
        
        eprintln!("DEBUG: Creating camera object...");
        let camera = Camera::new(camera_index, requested)
            .map_err(|e| {
                eprintln!("DEBUG: Failed to create camera: {}", e);
                anyhow::anyhow!("Failed to open camera: {}", e)
            })?;
        
        eprintln!("DEBUG: Camera created successfully");
        Ok(VideoSource::Camera(Arc::new(Mutex::new(camera))))
    }
    
    pub fn new_file(path: impl AsRef<Path>) -> Result<Self> {
        let reader = VideoFileReader::new(path)?;
        Ok(VideoSource::File(reader))
    }
    
    pub fn read_frame(&mut self) -> Result<DynamicImage> {
        match self {
            VideoSource::Camera(camera) => {
                let mut cam = camera.lock().unwrap();
                
                if !cam.is_stream_open() {
                    cam.open_stream()
                        .map_err(|e| anyhow::anyhow!("Failed to open camera stream: {}", e))?;
                }
                
                let frame = cam.frame()
                    .map_err(|e| anyhow::anyhow!("Failed to capture frame: {}", e))?;
                
                let decoded = frame.decode_image::<RgbFormat>()
                    .map_err(|e| anyhow::anyhow!("Failed to decode frame: {}", e))?;
                
                let width = decoded.width();
                let height = decoded.height();
                let rgb_data = decoded.into_vec();
                
                let mut rgba_data = Vec::with_capacity((width * height * 4) as usize);
                for chunk in rgb_data.chunks(3) {
                    rgba_data.push(chunk[0]);
                    rgba_data.push(chunk[1]);
                    rgba_data.push(chunk[2]);
                    rgba_data.push(255);
                }
                
                let img = ImageBuffer::from_raw(width, height, rgba_data)
                    .ok_or_else(|| anyhow::anyhow!("Failed to create image buffer"))?;
                
                let flipped = image::imageops::flip_horizontal(&img);
                Ok(DynamicImage::ImageRgba8(flipped))
            }
            VideoSource::File(reader) => {
                reader.next_frame()
                    .ok_or_else(|| anyhow::anyhow!("No more frames in video"))
            }
        }
    }
    
    pub fn get_info(&self) -> Option<VideoInfo> {
        match self {
            VideoSource::Camera(camera) => {
                let cam = camera.lock().unwrap();
                let resolution = cam.resolution();
                Some(VideoInfo {
                    path: PathBuf::from("camera://0"),
                    fps: cam.frame_rate() as f64,
                    frame_count: -1,
                    width: resolution.width() as i32,
                    height: resolution.height() as i32,
                    current_frame: 0,
                })
            }
            VideoSource::File(reader) => Some(VideoInfo {
                path: reader.path.clone(),
                fps: reader.fps as f64,
                frame_count: reader.total_frames as i32,
                width: reader.width as i32,
                height: reader.height as i32,
                current_frame: reader.current_frame as i32,
            }),
        }
    }
    
    pub fn seek(&mut self, frame_number: i32) -> Result<()> {
        if let VideoSource::File(reader) = self {
            reader.seek(frame_number as usize);
        }
        Ok(())
    }
    
    pub fn get_progress(&self) -> f32 {
        match self {
            VideoSource::Camera(_) => 0.0,
            VideoSource::File(reader) => reader.get_progress(),
        }
    }
}

impl Drop for VideoSource {
    fn drop(&mut self) {
        if let VideoSource::Camera(camera) = self {
            if let Ok(mut cam) = camera.lock() {
                let _ = cam.stop_stream();
            }
        }
    }
}

pub struct VideoRecorder {
    output_dir: PathBuf,
    session_id: String,
    fps: f64,
    frame_count: i32,
    frames: Vec<DynamicImage>,
    overlay_frames: Vec<DynamicImage>,
    width: u32,
    height: u32,
}

impl VideoRecorder {
    pub fn new(
        output_dir: impl AsRef<Path>,
        width: u32,
        height: u32,
        fps: f64,
    ) -> Result<Self> {
        let session_id = format!("recording_{}", chrono::Local::now().format("%Y%m%d_%H%M%S"));
        let output_dir = output_dir.as_ref().join(&session_id);
        
        // Create output directory
        std::fs::create_dir_all(&output_dir)?;
        
        Ok(Self {
            output_dir,
            session_id,
            fps,
            frame_count: 0,
            frames: Vec::new(),
            overlay_frames: Vec::new(),
            width,
            height,
        })
    }
    
    pub fn add_frame(&mut self, frame: &DynamicImage, overlay_frame: Option<&DynamicImage>) {
        self.frames.push(frame.clone());
        if let Some(overlay) = overlay_frame {
            self.overlay_frames.push(overlay.clone());
        } else {
            self.overlay_frames.push(frame.clone());
        }
        self.frame_count += 1;
    }
    
    pub fn save_videos(&self) -> Result<(PathBuf, PathBuf)> {
        let raw_video_path = self.output_dir.join("raw_video.mp4");
        let overlay_video_path = self.output_dir.join("overlay_video.mp4");
        
        // Save raw video
        self.save_video_from_frames(&self.frames, &raw_video_path)?;
        
        // Save overlay video
        self.save_video_from_frames(&self.overlay_frames, &overlay_video_path)?;
        
        Ok((raw_video_path, overlay_video_path))
    }
    
    fn save_video_from_frames(&self, frames: &[DynamicImage], output_path: &Path) -> Result<()> {
        // Create temp directory for frames
        let temp_dir = self.output_dir.join("temp_frames");
        std::fs::create_dir_all(&temp_dir)?;
        
        // Save frames as images
        for (i, frame) in frames.iter().enumerate() {
            let frame_path = temp_dir.join(format!("frame_{:05}.png", i));
            frame.save(&frame_path)?;
        }
        
        // Use ffmpeg to create video
        let status = Command::new("ffmpeg")
            .args(&[
                "-y",
                "-r", &self.fps.to_string(),
                "-i", &format!("{}/frame_%05d.png", temp_dir.display()),
                "-c:v", "libx264",
                "-preset", "medium",
                "-crf", "23",
                "-pix_fmt", "yuv420p",
                output_path.to_str().unwrap(),
            ])
            .status()
            .context("Failed to run ffmpeg")?;
        
        // Clean up temp frames
        let _ = std::fs::remove_dir_all(&temp_dir);
        
        if !status.success() {
            return Err(anyhow::anyhow!("FFmpeg video encoding failed"));
        }
        
        Ok(())
    }
    
    pub fn get_output_dir(&self) -> &Path {
        &self.output_dir
    }
}

// Video gallery management
pub struct VideoGallery {
    videos_dir: PathBuf,
    videos: Vec<VideoEntry>,
}

#[derive(Clone)]
pub struct VideoEntry {
    pub path: PathBuf,
    pub thumbnail: Option<DynamicImage>,
    pub name: String,
    pub date: chrono::DateTime<chrono::Local>,
    pub has_overlay: bool,
    pub has_csv: bool,
}

impl VideoGallery {
    pub fn new(videos_dir: impl AsRef<Path>) -> Self {
        Self {
            videos_dir: videos_dir.as_ref().to_path_buf(),
            videos: Vec::new(),
        }
    }
    
    pub fn scan_videos(&mut self) -> Result<()> {
        self.videos.clear();
        
        if !self.videos_dir.exists() {
            std::fs::create_dir_all(&self.videos_dir)?;
        }
        
        // Scan for video directories
        for entry in std::fs::read_dir(&self.videos_dir)? {
            let entry = entry?;
            let path = entry.path();
            
            if path.is_dir() {
                // Check for raw video
                let raw_video = path.join("raw_video.mp4");
                if raw_video.exists() {
                    let overlay_exists = path.join("overlay_video.mp4").exists();
                    let csv_exists = path.join("tracking_data.csv").exists();
                    
                    // Generate thumbnail from first frame
                    let thumbnail = self.extract_thumbnail(&raw_video).ok();
                    
                    let metadata = std::fs::metadata(&raw_video)?;
                    let modified = metadata.modified()?;
                    let datetime = chrono::DateTime::<chrono::Local>::from(modified);
                    
                    self.videos.push(VideoEntry {
                        path: raw_video,
                        thumbnail,
                        name: path.file_name().unwrap().to_string_lossy().to_string(),
                        date: datetime,
                        has_overlay: overlay_exists,
                        has_csv: csv_exists,
                    });
                }
            }
        }
        
        // Sort by date (newest first)
        self.videos.sort_by(|a, b| b.date.cmp(&a.date));
        
        Ok(())
    }
    
    fn extract_thumbnail(&self, video_path: &Path) -> Result<DynamicImage> {
        // Extract first frame as thumbnail
        let temp_thumb = std::env::temp_dir().join("thumb.png");
        
        let status = Command::new("ffmpeg")
            .args(&[
                "-i", video_path.to_str().unwrap(),
                "-vf", "scale=320:240",
                "-vframes", "1",
                "-y",
                temp_thumb.to_str().unwrap(),
            ])
            .status()?;
        
        if !status.success() {
            return Err(anyhow::anyhow!("Failed to extract thumbnail"));
        }
        
        let thumb = image::open(&temp_thumb)?;
        let _ = std::fs::remove_file(&temp_thumb);
        
        Ok(thumb)
    }
    
    pub fn get_videos(&self) -> &[VideoEntry] {
        &self.videos
    }
}

################################################################################
# EXTRA FILES (appended after source tree)
################################################################################


================================================================================
EXTRA FILE: /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/Cargo.toml
================================================================================

[package]
name = "supro"
version = "0.1.0"
edition = "2021"

[package.metadata.bundle]
name = "SuPro"
identifier = "com.supro.app"
icon = ["assets/AppIcon.icns"]
version = "1.0.0"
copyright = "Copyright (c) 2025"
category = "DeveloperTool" 
short_description = "Arm rotation tracking system"
long_description = "A sophisticated motion tracking application for analyzing arm rotation patterns"
osx_minimum_system_version = "10.13"

[package.metadata.bundle.osx]
info_plist_path = "/Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/Info.plist"

[dependencies]
# GUI Framework
eframe = "0.24"
egui = "0.24"
egui_extras = "0.24"
image = { version = "0.24", features = ["jpeg", "png"] }
cocoa = "0.25"
objc = "0.2"

# File Dialog
rfd = "0.12"

# Camera support
nokhwa = { version = "0.10", features = ["input-native", "output-threaded"] }

# SVG support for logo
resvg = "0.35"
usvg = "0.35"

# Math and Linear Algebra
nalgebra = "0.32"
nalgebra-glm = "0.18"

# Async Runtime
tokio = { version = "1.35", features = ["full"] }

# Serialization
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
csv = "1.3"

# Time and Date
chrono = "0.4"

# File System
directories = "5.0"

# UUID for temp file naming
uuid = { version = "1.6", features = ["v4", "fast-rng"] }

# Error Handling
anyhow = "1.0"
thiserror = "1.0"

# Logging
tracing = "0.1"
tracing-subscriber = "0.3"

# Basic utilities
once_cell = "1.19"

[[bin]]
name = "supro"
path = "src/main.rs"

================================================================================
EXTRA FILE: /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/build.sh
================================================================================

#!/bin/bash
set -euo pipefail

echo "Building bundle..."
cargo bundle --release

APP="target/release/bundle/osx/Arm Tracker.app"
PLIST="$APP/Contents/Info.plist"
RES="$APP/Contents/Resources"

# Your .icns built from PNG beforehand and tracked by Cargo.toml:
# [package.metadata.bundle]
# icon = ["assets/AppIcon.icns"]
ASSETS_ICNS="assets/AppIcon.icns"

echo "Ensuring AppIcon.icns exists inside the bundle..."
if [[ ! -f "$RES/AppIcon.icns" ]]; then
  echo "  - AppIcon.icns not found in bundle Resources. Attempting to copy from assets/..."
  mkdir -p "$RES"
  if [[ -f "$ASSETS_ICNS" ]]; then
    cp "$ASSETS_ICNS" "$RES/AppIcon.icns"
    echo "  - Copied: $ASSETS_ICNS -> $RES/AppIcon.icns"
  else
    echo "ERROR: $ASSETS_ICNS not found. Build AppIcon.icns from your 1024x1024 PNG first."
    echo "Hint:"
    echo "  mkdir -p assets/AppIcon.iconset"
    echo "  sips -z 16 16     appicon_1024.png --out assets/AppIcon.iconset/icon_16x16.png"
    echo "  sips -z 32 32     appicon_1024.png --out assets/AppIcon.iconset/icon_16x16@2x.png"
    echo "  sips -z 32 32     appicon_1024.png --out assets/AppIcon.iconset/icon_32x32.png"
    echo "  sips -z 64 64     appicon_1024.png --out assets/AppIcon.iconset/icon_32x32@2x.png"
    echo "  sips -z 128 128   appicon_1024.png --out assets/AppIcon.iconset/icon_128x128.png"
    echo "  sips -z 256 256   appicon_1024.png --out assets/AppIcon.iconset/icon_128x128@2x.png"
    echo "  sips -z 256 256   appicon_1024.png --out assets/AppIcon.iconset/icon_256x256.png"
    echo "  sips -z 512 512   appicon_1024.png --out assets/AppIcon.iconset/icon_256x256@2x.png"
    echo "  sips -z 512 512   appicon_1024.png --out assets/AppIcon.iconset/icon_512x512.png"
    echo "  cp appicon_1024.png assets/AppIcon.iconset/icon_512x512@2x.png"
    echo "  iconutil -c icns assets/AppIcon.iconset -o assets/AppIcon.icns"
    exit 1
  fi
fi

echo "Setting icon and permissions keys in Info.plist..."

# Ensure CFBundleIconName = AppIcon (no extension)
# If the key exists, set; otherwise, add.
if /usr/libexec/PlistBuddy -c 'Print :CFBundleIconName' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :CFBundleIconName AppIcon' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :CFBundleIconName string AppIcon' "$PLIST"
fi

# If legacy CFBundleIconFile exists and has an extension, fix it (macOS prefers name w/o extension)
if /usr/libexec/PlistBuddy -c 'Print :CFBundleIconFile' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :CFBundleIconFile AppIcon' "$PLIST" || true
fi

# (Optional) Set App Store category string to Developer Tools
if /usr/libexec/PlistBuddy -c 'Print :LSApplicationCategoryType' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :LSApplicationCategoryType public.app-category.developer-tools' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :LSApplicationCategoryType string public.app-category.developer-tools' "$PLIST"
fi

# Add/Update privacy usage descriptions
if /usr/libexec/PlistBuddy -c 'Print :NSCameraUsageDescription' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSCameraUsageDescription This app requires camera access to track arm rotation movements for analysis.' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSCameraUsageDescription string This app requires camera access to track arm rotation movements for analysis.' "$PLIST"
fi

if /usr/libexec/PlistBuddy -c 'Print :NSMicrophoneUsageDescription' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSMicrophoneUsageDescription Arm Tracker may record audio while capturing video.' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSMicrophoneUsageDescription string Arm Tracker may record audio while capturing video.' "$PLIST"
fi

# (Nice to have) Mark high-DPI capable
if /usr/libexec/PlistBuddy -c 'Print :NSHighResolutionCapable' "$PLIST" >/dev/null 2>&1; then
  /usr/libexec/PlistBuddy -c 'Set :NSHighResolutionCapable true' "$PLIST"
else
  /usr/libexec/PlistBuddy -c 'Add :NSHighResolutionCapable bool true' "$PLIST"
fi

# (Strongly recommended) bump CFBundleVersion to avoid icon caching issues
# If CFBundleVersion is numeric, increment; else set a fresh numeric build
if /usr/libexec/PlistBuddy -c 'Print :CFBundleVersion' "$PLIST" >/dev/null 2>&1; then
  CUR_VER="$(/usr/libexec/PlistBuddy -c 'Print :CFBundleVersion' "$PLIST" || echo 0)"
  if [[ "$CUR_VER" =~ ^[0-9]+$ ]]; then
    NEW_VER=$((CUR_VER + 1))
  else
    NEW_VER="$(date +%s)"
  fi
  /usr/libexec/PlistBuddy -c "Set :CFBundleVersion $NEW_VER" "$PLIST"
else
  /usr/libexec/PlistBuddy -c "Add :CFBundleVersion string $(date +%s)" "$PLIST"
fi

echo ""
echo "âœ“ Bundle created successfully"
echo "Verifying keys..."
/usr/libexec/PlistBuddy -c 'Print :CFBundleIdentifier' "$PLIST" || echo "CFBundleIdentifier missing (check Cargo.toml identifier)"
/usr/libexec/PlistBuddy -c 'Print :CFBundleIconName' "$PLIST" || echo "CFBundleIconName missing"
ls -l "$RES/AppIcon.icns" || true
echo ""

# After creating the bundle, copy Python files
PYTHON_DIR="$APP/Contents/Resources/python"
mkdir -p "$PYTHON_DIR"
cp -r python/*.py "$PYTHON_DIR/"
echo "Copied Python scripts to bundle"

# Replace old app in /Applications to avoid duplicate cache entries
DEST="/Applications/Arm Tracker.app"
if [[ -d "$DEST" ]]; then
  echo "Removing old /Applications bundle..."
  rm -rf "$DEST"
fi
echo "Copying new bundle to /Applications..."
cp -R "$APP" "$DEST"

echo "Refreshing Dock/Finder to clear icon caches..."
killall Dock || true
killall Finder || true

echo "Opening app..."
open "$DEST"

echo "Done."


================================================================================
EXTRA FILE: /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/python/mediapipe_service.py
================================================================================

#!/usr/bin/env python3
import sys
import json
import numpy as np
import traceback

try:
    import mediapipe as mp
    import cv2
except ImportError as e:
    print(f"Error: Missing required packages: {e}", file=sys.stderr)
    print("Install with: pip3 install mediapipe opencv-python numpy", file=sys.stderr)
    sys.exit(1)

class MediaPipeService:
    def __init__(self):
        # Initialize pose tracking
        self.mp_pose = mp.solutions.pose
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=1,  # Changed from 0 to 1 for better accuracy
            smooth_landmarks=True,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5  # Increased from 0.3
        )
        
        # Initialize hand tracking - ENABLED
        self.mp_hands = mp.solutions.hands
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,  # Changed from 0 to 1
            min_detection_confidence=0.4,  # Lowered from 0.5
            min_tracking_confidence=0.4
        )
        
        print("MediaPipe service initialized with hands enabled", file=sys.stderr)
    
    def process_frame(self, frame_data):
        try:
            width = frame_data['width']
            height = frame_data['height']
            data = np.array(frame_data['data'], dtype=np.uint8)
            
            # Reshape frame
            frame = data.reshape((height, width, 3))
            
            # Process with MediaPipe
            pose_results = self.pose.process(frame)
            hands_results = self.hands.process(frame)
            
            result = {
                'pose_landmarks': [],
                'hand_landmarks': []
            }
            
            if pose_results.pose_landmarks:
                result['pose_landmarks'] = [
                    [lm.x, lm.y, lm.z] 
                    for lm in pose_results.pose_landmarks.landmark
                ]
            
            # PROCESS HANDS - ENABLED
            if hands_results.multi_hand_landmarks:
                for hand_landmarks in hands_results.multi_hand_landmarks:
                    hand_data = [
                        [lm.x, lm.y, lm.z] 
                        for lm in hand_landmarks.landmark
                    ]
                    result['hand_landmarks'].append(hand_data)
            
            return result
            
        except Exception as e:
            print(f"Error processing frame: {e}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            return {'pose_landmarks': [], 'hand_landmarks': []}
    
    def run(self):
        print("READY", file=sys.stdout)
        sys.stdout.flush()
        print("MediaPipe service ready with hands tracking", file=sys.stderr)
        
        while True:
            try:
                line = sys.stdin.readline()
                if not line:
                    print("End of input stream", file=sys.stderr)
                    break
                
                frame_data = json.loads(line)
                result = self.process_frame(frame_data)
                print(json.dumps(result))
                sys.stdout.flush()
                
            except json.JSONDecodeError as e:
                print(f"JSON decode error: {e}", file=sys.stderr)
                print(json.dumps({'pose_landmarks': [], 'hand_landmarks': []}))
                sys.stdout.flush()
            except Exception as e:
                print(f"Service error: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                print(json.dumps({'pose_landmarks': [], 'hand_landmarks': []}))
                sys.stdout.flush()

if __name__ == '__main__':
    try:
        service = MediaPipeService()
        service.run()
    except Exception as e:
        print(f"Failed to start service: {e}", file=sys.stderr)
        sys.exit(1)

================================================================================
EXTRA FILE: /Users/JulioContreras/Desktop/School/Research/Baseball SuPro /SuPro Rewritten/python/mediapipe_processor.py
================================================================================

#mediapipe_processor.py
import mediapipe as mp
import numpy as np
import json
import cv2

class MediaPipeProcessor:
    def __init__(self):
        self.mp_pose = mp.solutions.pose
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        
        self.pose = self.mp_pose.Pose(
            static_image_mode=False,
            model_complexity=2,
            smooth_landmarks=True,
            enable_segmentation=False,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        
        self.hands = self.mp_hands.Hands(
            static_image_mode=False,
            max_num_hands=2,
            model_complexity=1,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
    
    def process_frame(self, frame_data):
        """Process a frame and return landmarks"""
        # Convert frame data to numpy array
        frame = np.frombuffer(frame_data, dtype=np.uint8)
        frame = frame.reshape((720, 1280, 3))  # Adjust dimensions as needed
        
        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # Process with MediaPipe
        pose_results = self.pose.process(rgb_frame)
        hands_results = self.hands.process(rgb_frame)
        
        result = {
            'pose_landmarks': [],
            'hand_landmarks': []
        }
        
        if pose_results.pose_landmarks:
            result['pose_landmarks'] = [
                [lm.x, lm.y, lm.z] 
                for lm in pose_results.pose_landmarks.landmark
            ]
        
        if hands_results.multi_hand_landmarks:
            for hand_landmarks in hands_results.multi_hand_landmarks:
                hand_data = [
                    [lm.x, lm.y, lm.z] 
                    for lm in hand_landmarks.landmark
                ]
                result['hand_landmarks'].append(hand_data)
        
        return json.dumps(result)
    
    def cleanup(self):
        self.pose.close()
        self.hands.close()
